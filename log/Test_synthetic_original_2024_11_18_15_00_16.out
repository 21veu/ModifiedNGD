/nishome/yui/ModifiedNGD/utils/readData.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)
Train info: 
 train data shape: torch.Size([512, 1]), 
 train lable shape: torch.Size([512, 1]), 
 positive / negative: 0.021206805482506752 / 0.978793203830719, 
 discrepancy norm error: 5.045955617788422e-07
Test info: 
 test data shape: torch.Size([128, 1]), 
 test lable shape: torch.Size([128, 1]), , 
 positive / negative: -0.030759211629629135 / 1.0307592153549194, 
 discrepancy norm error: 2.750667249529215e-07
Valid info: 
 valid data shape: torch.Size([128, 1]), valid lable shape: torch.Size([128, 1]), 
 positive / negative: 0.015859205275774002 / 0.9841408133506775, 
 discrepancy norm error: 2.3576444618811365e-07
torch.Size([512, 1]) torch.Size([512])
torch.Size([512, 1]) torch.Size([512])
trainloader randomness check:  tensor([[-2.0661e+00],
        [ 4.9868e+00],
        [ 1.6473e+00],
        [-1.0572e+00],
        [-2.0705e+00],
        [ 5.2220e+00],
        [ 5.7635e-01],
        [-7.4949e-01],
        [-2.8597e+00],
        [ 4.1086e-01],
        [ 2.4208e+00],
        [ 1.5584e-02],
        [-2.3569e+00],
        [ 3.4857e+00],
        [-8.9004e-01],
        [-1.5039e-01],
        [-3.6009e+00],
        [ 5.5010e-02],
        [-1.3479e-01],
        [ 3.4546e+00],
        [-2.8288e+00],
        [-1.6784e+00],
        [-2.8528e+00],
        [-4.6615e+00],
        [-3.6975e+00],
        [ 1.4116e+00],
        [-1.8620e-01],
        [-4.0001e+00],
        [-9.0446e-01],
        [-1.6941e+00],
        [-4.1348e+00],
        [-3.3701e+00],
        [-1.8217e+00],
        [-3.9196e+00],
        [-1.9001e+00],
        [-1.5728e+00],
        [ 3.3766e+00],
        [ 4.5247e-01],
        [-2.6408e+00],
        [ 4.2572e+00],
        [ 4.7278e+00],
        [-9.5600e-01],
        [ 3.3600e-01],
        [ 7.8375e-01],
        [-3.9015e+00],
        [ 9.6463e-01],
        [ 6.8382e-01],
        [ 1.8879e+00],
        [ 3.2714e+00],
        [-3.9916e+00],
        [-3.7008e+00],
        [-1.4699e+00],
        [-1.5172e+00],
        [-4.8404e-01],
        [ 2.4407e+00],
        [ 5.3943e+00],
        [ 1.1009e+00],
        [ 6.1987e-01],
        [-6.3895e-01],
        [-4.0812e+00],
        [ 1.1443e+00],
        [ 4.1372e+00],
        [ 5.0460e+00],
        [-1.0237e-01],
        [ 2.4607e+00],
        [ 9.2865e-02],
        [ 8.9651e-01],
        [ 4.3292e+00],
        [ 5.4784e-01],
        [-1.4242e+00],
        [-3.6161e+00],
        [ 5.7641e+00],
        [ 5.3798e+00],
        [ 1.1960e+00],
        [-2.8115e-01],
        [-1.5462e+00],
        [-8.6707e-01],
        [-7.7019e-01],
        [-3.5081e+00],
        [ 2.5633e+00],
        [-1.8156e+00],
        [ 4.2639e+00],
        [-4.8344e+00],
        [-6.0569e+00],
        [ 2.1416e-01],
        [-1.3108e-01],
        [-1.7529e+00],
        [-1.0103e+00],
        [ 4.4569e-01],
        [ 2.0852e+00],
        [ 2.3408e+00],
        [ 1.8078e-03],
        [ 8.9037e-01],
        [-1.2641e+00],
        [-1.9627e+00],
        [ 2.9629e-01],
        [ 6.4428e-01],
        [-5.1767e+00],
        [-2.9872e+00],
        [ 2.5289e-01],
        [ 7.5688e-01],
        [-1.7701e+00],
        [ 2.8481e+00],
        [ 1.2998e+00],
        [-3.1008e+00],
        [-3.4797e-01],
        [ 9.1288e-01],
        [ 1.4025e+00],
        [-1.6062e+00],
        [-2.5004e+00],
        [ 4.2015e-01],
        [-6.9504e-01],
        [-2.3774e+00],
        [-2.5448e+00],
        [ 2.1260e+00],
        [ 2.5016e-01],
        [ 1.2991e+00],
        [ 1.6473e+00],
        [ 3.4249e+00],
        [ 5.0475e+00],
        [ 2.8650e-01],
        [-1.2376e+00],
        [-6.5356e-01],
        [-2.9964e+00],
        [-1.4180e-01],
        [-1.0932e+00],
        [-9.3564e-01],
        [ 3.9079e+00],
        [ 8.3187e-01],
        [-4.1127e+00],
        [ 4.6245e+00],
        [ 3.9568e+00],
        [ 1.6120e+00],
        [-3.9353e-01],
        [ 5.9222e-01],
        [ 4.0792e-01],
        [ 1.3265e+00],
        [ 3.1897e+00],
        [ 7.1148e-01],
        [ 1.1281e-01],
        [-4.9600e+00],
        [ 4.8488e+00],
        [-2.5664e+00],
        [ 2.7144e+00],
        [ 3.5035e+00],
        [-6.4374e-01],
        [-8.0777e-01],
        [ 2.7987e+00],
        [ 2.8931e+00],
        [-3.4484e-01],
        [ 3.9041e+00],
        [ 5.5468e+00],
        [ 2.3013e+00],
        [-3.2733e+00],
        [-8.6509e-01],
        [ 3.3026e-01],
        [ 6.1869e-02],
        [-1.0034e+00],
        [ 2.4034e+00],
        [ 1.3264e+00],
        [ 2.5845e+00],
        [ 4.8073e+00],
        [-1.8816e+00],
        [ 2.8372e+00],
        [ 4.3028e-01],
        [ 3.8842e+00],
        [ 4.5624e+00],
        [-1.2354e+00],
        [ 4.2558e-01],
        [ 1.0200e+00],
        [ 8.1605e-01],
        [-4.4072e+00],
        [ 4.1986e-01],
        [-1.6567e+00],
        [-1.3474e+00],
        [ 1.5478e+00],
        [ 4.5212e-01],
        [-2.0117e+00],
        [ 3.8695e-01],
        [ 2.8623e+00],
        [-2.5215e+00],
        [ 3.1062e+00],
        [-1.4797e+00],
        [-4.7606e-01],
        [-2.1827e+00],
        [ 1.5762e+00],
        [ 1.1017e+00],
        [-9.1903e-01],
        [ 5.9488e+00],
        [-2.3121e+00],
        [-2.6681e+00],
        [ 4.9689e-01],
        [ 3.5055e+00],
        [-4.5440e+00],
        [-2.0451e+00],
        [ 2.6116e+00],
        [-1.0253e+00],
        [-2.5616e+00],
        [-4.4903e+00],
        [ 7.8220e-01],
        [-1.2715e+00],
        [-1.1035e+00],
        [-2.6456e+00],
        [ 3.8309e+00],
        [-3.9588e+00],
        [-3.7223e+00],
        [-5.0823e-01],
        [-1.7203e+00],
        [-1.5247e+00],
        [-2.4336e+00],
        [-1.1305e+00],
        [-4.7830e+00],
        [ 6.5622e-01],
        [ 7.4616e-01],
        [ 2.3337e+00],
        [-6.6577e-01],
        [-1.8317e+00],
        [-4.1354e+00],
        [-2.5827e+00],
        [-1.3814e+00],
        [ 1.6980e+00],
        [ 2.5341e+00],
        [ 3.7894e+00],
        [-1.4832e+00],
        [-3.2583e+00],
        [ 2.6289e+00],
        [-3.0406e-01],
        [-4.1477e+00],
        [ 4.9432e+00],
        [-3.8281e+00],
        [-6.2109e-01],
        [-4.9607e+00],
        [-2.4696e+00],
        [ 1.3399e+00],
        [-5.5921e-01],
        [-6.0731e-01],
        [ 2.3810e+00],
        [-2.9083e+00],
        [ 3.8119e+00],
        [-2.8716e+00],
        [ 2.4712e+00],
        [-3.3032e+00],
        [-2.4516e+00],
        [ 2.1824e-01],
        [ 1.6382e+00],
        [-1.0978e-01],
        [ 3.9854e+00],
        [-1.5620e+00],
        [-4.2206e+00],
        [-5.0318e+00],
        [ 2.4276e-01],
        [ 3.4579e+00],
        [ 5.4542e-01],
        [ 1.4084e+00],
        [-2.3975e+00],
        [-5.0428e+00],
        [-2.6106e+00],
        [ 1.7768e+00],
        [-5.4749e+00],
        [ 1.8253e+00],
        [-6.2522e-01],
        [-1.0462e+00],
        [ 3.4764e+00],
        [-3.0228e+00],
        [-5.5140e-02],
        [ 2.2094e+00],
        [ 6.1219e-01],
        [ 3.5497e-01],
        [ 8.1164e-01],
        [-1.1220e+00],
        [-2.8159e+00],
        [ 3.8120e+00],
        [-6.2310e-01],
        [-3.6719e+00],
        [ 1.6004e+00],
        [ 3.8239e-01],
        [-2.8307e-01],
        [ 3.5442e+00],
        [-7.0733e-01],
        [ 3.4300e+00],
        [ 3.5633e+00],
        [ 1.3154e-01],
        [ 3.0735e+00],
        [ 8.9640e-01],
        [ 4.7800e+00],
        [-1.7469e+00],
        [ 2.6309e+00],
        [-4.8001e+00],
        [ 2.2696e+00],
        [ 1.1458e+00],
        [ 9.0260e-01],
        [ 2.1917e+00],
        [-1.6092e+00],
        [-1.4506e+00],
        [-4.3430e+00],
        [ 3.5802e+00],
        [-6.5472e-01],
        [-9.2368e-01],
        [ 2.2996e+00],
        [ 2.7762e+00],
        [-2.6543e-01],
        [ 3.9144e+00],
        [-2.0908e+00],
        [ 3.3007e+00],
        [-5.0034e+00],
        [ 2.0347e+00],
        [ 1.7518e+00],
        [ 1.7851e+00],
        [ 2.3255e+00],
        [ 1.7028e+00],
        [ 1.2338e+00],
        [-4.4186e-02],
        [ 1.2535e+00],
        [-1.7239e+00],
        [-2.5389e+00],
        [ 6.1604e-01],
        [-2.5097e+00],
        [-3.3172e+00],
        [ 3.5887e+00],
        [ 1.7486e+00],
        [-2.9886e+00],
        [-1.9980e+00],
        [-9.8085e-01],
        [-5.8127e+00],
        [ 3.4094e+00],
        [ 4.6880e-01],
        [-5.7641e-01],
        [ 4.5888e+00],
        [-3.7800e+00],
        [-1.0957e-01],
        [-1.0821e+00],
        [ 2.7569e+00],
        [ 2.4997e-01],
        [ 5.5635e-01],
        [-2.7305e+00],
        [ 3.1547e-01],
        [ 3.4826e+00],
        [ 4.2067e+00],
        [ 2.9098e+00],
        [ 1.3350e+00],
        [-4.8851e+00],
        [ 1.8493e+00],
        [ 3.9765e+00],
        [ 1.5142e+00],
        [ 3.1064e+00],
        [ 3.4607e+00],
        [ 1.5995e+00],
        [-6.7757e-01],
        [-3.2825e+00],
        [ 5.2036e+00],
        [ 2.3893e+00],
        [-1.7402e+00],
        [ 3.4586e-01],
        [ 1.6039e-01],
        [-5.8604e-01],
        [ 4.3820e+00],
        [-1.0302e+00],
        [ 2.9792e+00],
        [-2.1798e+00],
        [-5.8852e+00],
        [-2.2980e+00],
        [-5.1240e+00],
        [-3.0360e+00],
        [-4.6777e+00],
        [-9.3615e-01],
        [ 3.2293e+00],
        [-2.4199e+00],
        [-3.8991e+00],
        [ 2.5074e+00],
        [ 1.3515e-01],
        [-5.0449e+00],
        [ 1.2318e+00],
        [ 3.1820e+00],
        [ 1.4147e+00],
        [ 2.9143e-01],
        [ 3.9090e+00],
        [-3.3157e+00],
        [-6.9114e-01],
        [ 6.6466e-01],
        [-3.5231e+00],
        [-8.1987e-02],
        [-3.1159e+00],
        [-2.8655e+00],
        [ 2.1420e+00],
        [ 2.6103e+00],
        [-3.9669e+00],
        [ 1.6670e+00],
        [-1.1794e+00],
        [-1.1313e+00],
        [ 2.5311e+00],
        [-7.9759e-01],
        [ 2.2980e+00],
        [-2.9593e-01],
        [ 7.3822e-01],
        [-5.8808e-01],
        [ 4.6683e+00],
        [ 5.7818e-01],
        [ 2.7239e+00],
        [ 3.0366e+00],
        [-1.2076e-01],
        [ 3.9412e-01],
        [ 2.9465e+00],
        [ 1.8352e+00],
        [ 8.6541e-01],
        [ 4.3086e+00],
        [ 3.4143e+00],
        [-2.7322e+00],
        [-2.4313e+00],
        [-4.6418e+00],
        [ 2.8282e+00],
        [ 2.7789e+00],
        [ 3.5324e-01],
        [ 3.6150e+00],
        [-2.3940e+00],
        [ 5.4637e-01],
        [ 4.3390e+00],
        [-1.6834e-01],
        [ 1.1222e+00],
        [-4.8730e+00],
        [ 8.1195e-01],
        [-5.9318e+00],
        [ 4.1043e+00],
        [ 0.0000e+00],
        [-3.0723e+00],
        [ 2.0285e+00],
        [-2.2035e+00],
        [-1.2730e+00],
        [-4.0581e+00],
        [ 2.4273e+00],
        [-3.5440e+00],
        [-1.1227e+00],
        [-3.0487e+00],
        [-1.2847e+00],
        [ 2.9198e+00],
        [ 4.0630e+00],
        [ 5.7531e-01],
        [-4.9538e+00],
        [-4.2797e+00],
        [ 3.2270e+00],
        [-3.9214e-01],
        [-3.0291e-02],
        [-4.9876e-01],
        [ 2.4106e+00],
        [ 4.2888e+00],
        [-2.2537e+00],
        [ 4.7363e-01],
        [-1.2003e+00],
        [ 2.0900e+00],
        [-3.7613e+00],
        [ 3.5257e+00],
        [ 3.2946e+00],
        [ 8.7531e-01],
        [ 1.2134e+00],
        [ 3.3007e+00],
        [ 8.2542e-01],
        [-2.0113e-01],
        [ 0.0000e+00],
        [-1.4127e+00],
        [ 1.1690e+00],
        [ 4.2739e-01],
        [ 5.2085e-01],
        [-3.9923e+00],
        [-2.3361e+00],
        [-5.5717e-01],
        [ 9.3256e-01],
        [-1.9590e+00],
        [-1.2811e+00],
        [-4.2136e+00],
        [-6.3505e-01],
        [ 2.8781e-02],
        [-3.3944e+00],
        [-7.6979e-01],
        [-7.3744e-01],
        [ 2.7453e+00],
        [ 4.4086e+00],
        [-1.4777e+00],
        [ 5.5672e+00],
        [-2.2282e+00],
        [ 4.1669e+00],
        [-4.3939e+00],
        [-4.4811e-01],
        [ 5.0176e-01],
        [-3.2073e+00],
        [ 2.7264e+00],
        [-1.1619e+00],
        [-1.0704e+00],
        [ 1.3604e+00],
        [-6.6157e-01],
        [ 3.1134e+00],
        [-2.1423e+00],
        [ 5.3654e+00],
        [ 2.0604e+00],
        [-9.0129e-01],
        [ 2.2847e+00],
        [-1.9153e+00],
        [-2.7279e-01],
        [-1.4359e+00],
        [-4.7126e+00],
        [-4.9197e+00],
        [ 1.8517e+00],
        [-2.2191e+00],
        [-2.0470e+00],
        [-3.2615e+00],
        [-1.0289e+00],
        [-4.2627e-01],
        [ 3.9035e-01],
        [ 1.6960e+00],
        [-3.2005e+00],
        [ 3.2714e+00],
        [ 3.2519e+00],
        [-2.1353e+00],
        [-1.8688e+00]], device='cuda:0')
seed is  2191
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/1 [00:00<?, ?it/s]/nishome/yui/anaconda3/envs/ng/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971214/work/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch:   1
train data shape:  torch.Size([512, 1]) torch.Size([512, 1]) torch.Size([512])
LOSS BY ALPHA:  tensor(0.0702, device='cuda:0') 512
max of Lambda2 tensor(1000., device='cuda:0')
min of Lambda2 tensor(0.0100, device='cuda:0')
eigenvalues preserved:  512
Test train Loss:  0.05464249104261398
Test train Acc:  0.0
Test Loss:  0.0616748183965683
Test Acc:  0.0
Valid Loss:  0.05691187083721161
Valid Acc:  0.0
LOSS:  tensor(0.0546, device='cuda:0', grad_fn=<MulBackward0>) alpha^2/2N:  torch.Size([512]) torch.Size([512, 1]) tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0702, device='cuda:0') tensor([[-9.8547e-02],
        [-1.5495e-01],
        [-2.2396e-01],
        [ 1.0922e-01],
        [ 4.8215e-02],
        [-2.8265e-01],
        [ 5.2699e-02],
        [-3.9560e-02],
        [ 1.6843e-01],
        [-6.6058e-02],
        [ 8.4438e-02],
        [-1.0274e-01],
        [-1.4329e-02],
        [ 2.1230e-01],
        [ 1.3506e-01],
        [-1.4319e-02],
        [-2.3046e-01],
        [ 1.0954e-01],
        [ 1.9453e-01],
        [ 3.2196e-02],
        [-1.5811e-01],
        [-1.6750e-01],
        [ 1.2426e-01],
        [-7.1416e-02],
        [-1.6192e-02],
        [-2.3939e-01],
        [-1.0182e-02],
        [ 1.7215e-02],
        [ 1.0831e-01],
        [ 1.9836e-01],
        [-2.0159e-01],
        [ 1.5881e-01],
        [-1.0066e-01],
        [-5.7693e-02],
        [-1.7994e-02],
        [-6.5612e-02],
        [ 1.6054e-01],
        [-4.7818e-02],
        [ 2.0589e-01],
        [ 3.0469e-02],
        [ 2.7202e-02],
        [ 7.8995e-02],
        [ 4.7440e-03],
        [ 8.6353e-02],
        [-5.0531e-02],
        [-1.8116e-01],
        [-7.1657e-02],
        [-1.9311e-01],
        [ 1.1848e-01],
        [ 1.2123e-01],
        [-2.2786e-01],
        [-2.3554e-01],
        [-1.6624e-01],
        [-1.1796e-01],
        [-1.8340e-01],
        [ 4.8783e-02],
        [ 4.5240e-04],
        [ 1.9547e-01],
        [-2.9418e-01],
        [-4.8836e-02],
        [-4.6133e-02],
        [-1.4235e-02],
        [-2.4138e-01],
        [-2.0820e-01],
        [ 2.3605e-01],
        [ 1.5186e-03],
        [ 1.7080e-02],
        [ 2.9190e-03],
        [-3.3423e-02],
        [ 3.2143e-03],
        [-4.0717e-03],
        [-9.7715e-03],
        [ 4.5244e-02],
        [-1.1176e-01],
        [-5.1377e-02],
        [-1.1353e-01],
        [-4.9858e-02],
        [-4.5814e-03],
        [-3.2553e-03],
        [-2.7570e-01],
        [-8.1264e-02],
        [-2.6370e-02],
        [-7.3874e-02],
        [-7.3054e-02],
        [ 2.3297e-01],
        [-2.0680e-01],
        [-4.1899e-02],
        [-2.2586e-01],
        [ 1.7307e-01],
        [ 7.6790e-02],
        [ 6.3954e-02],
        [-9.4269e-02],
        [-3.3607e-02],
        [ 2.3896e-01],
        [ 1.9176e-01],
        [ 2.7423e-01],
        [ 2.5264e-01],
        [ 1.2180e-01],
        [-2.1221e-01],
        [ 1.5517e-01],
        [ 2.5869e-01],
        [ 2.7733e-01],
        [ 2.4435e-02],
        [-2.6102e-01],
        [ 1.2638e-01],
        [ 1.5523e-01],
        [-9.5767e-03],
        [ 1.0109e-03],
        [ 1.5517e-01],
        [ 2.3934e-01],
        [ 1.1547e-01],
        [ 2.2727e-02],
        [-5.4610e-02],
        [ 7.9171e-02],
        [-4.9132e-02],
        [-2.7847e-01],
        [ 1.5049e-01],
        [-1.1708e-02],
        [ 8.1609e-02],
        [ 1.1433e-01],
        [ 1.3054e-01],
        [-8.3588e-02],
        [-8.3333e-02],
        [-1.2266e-01],
        [-1.1233e-01],
        [ 8.2948e-02],
        [-1.8935e-01],
        [-2.8153e-01],
        [ 2.7469e-01],
        [-3.7057e-02],
        [-2.4184e-02],
        [ 9.1407e-02],
        [ 1.7510e-01],
        [ 2.8243e-01],
        [-1.8015e-01],
        [-4.4190e-02],
        [ 8.1325e-03],
        [-1.1761e-01],
        [ 1.8857e-01],
        [ 1.5483e-01],
        [-2.6880e-01],
        [ 8.7675e-02],
        [-7.0275e-02],
        [-3.2877e-02],
        [ 2.8682e-01],
        [-2.7083e-02],
        [ 2.2024e-01],
        [ 1.0903e-01],
        [-7.0722e-02],
        [ 2.1025e-01],
        [ 2.3171e-02],
        [-1.0904e-01],
        [-2.5440e-01],
        [-6.5285e-03],
        [-2.1109e-01],
        [-5.5677e-04],
        [ 1.3714e-01],
        [-2.8264e-02],
        [ 3.1501e-02],
        [-1.5483e-01],
        [-8.2610e-02],
        [ 1.0845e-01],
        [ 1.4478e-03],
        [-1.6376e-01],
        [-1.5521e-02],
        [-8.1623e-03],
        [ 1.7087e-01],
        [-8.3207e-02],
        [-6.2783e-02],
        [-2.0449e-01],
        [ 1.3626e-01],
        [ 1.0467e-02],
        [-2.2599e-01],
        [-8.8669e-02],
        [ 9.9014e-02],
        [ 6.2751e-03],
        [-1.0223e-01],
        [-1.2754e-01],
        [ 2.7628e-02],
        [-1.7432e-01],
        [ 6.8232e-02],
        [-1.0221e-01],
        [ 8.3715e-03],
        [ 2.3436e-01],
        [ 6.2860e-02],
        [ 1.8050e-01],
        [-2.0854e-01],
        [-1.6046e-01],
        [-5.3600e-02],
        [ 1.2902e-02],
        [ 7.7997e-02],
        [-1.0963e-01],
        [-2.3431e-01],
        [-1.4199e-01],
        [ 5.1482e-02],
        [-2.2947e-01],
        [-2.4800e-01],
        [ 1.5632e-01],
        [ 1.1991e-02],
        [ 5.0305e-03],
        [ 2.0770e-02],
        [ 1.9601e-02],
        [-4.5686e-02],
        [-9.1692e-02],
        [-2.4870e-01],
        [ 4.1260e-03],
        [ 1.3526e-01],
        [ 7.8101e-02],
        [-1.1996e-01],
        [-9.5239e-03],
        [-3.6481e-04],
        [ 1.2979e-01],
        [-2.3738e-02],
        [-1.2511e-01],
        [ 3.5982e-02],
        [ 3.6031e-02],
        [-8.2441e-03],
        [ 9.3314e-02],
        [ 1.0627e-01],
        [-1.2334e-01],
        [-2.8634e-02],
        [ 6.3687e-03],
        [-2.3271e-01],
        [-2.5239e-01],
        [-1.7566e-01],
        [ 2.6208e-01],
        [ 1.4050e-02],
        [ 7.1753e-03],
        [-5.1549e-02],
        [ 1.4499e-01],
        [-9.2653e-02],
        [-1.0586e-01],
        [ 1.3995e-01],
        [ 2.8672e-02],
        [-1.5053e-01],
        [-2.2221e-01],
        [ 2.2691e-02],
        [ 2.1384e-01],
        [-1.3879e-01],
        [ 1.6185e-02],
        [ 2.5313e-01],
        [ 3.6767e-02],
        [ 2.7227e-01],
        [ 1.5875e-02],
        [ 9.3182e-02],
        [-3.1677e-02],
        [-7.1539e-02],
        [ 8.6766e-02],
        [ 9.2092e-02],
        [-2.3349e-01],
        [ 9.8140e-02],
        [-2.4583e-01],
        [ 0.0000e+00],
        [ 8.0512e-02],
        [ 2.0133e-01],
        [ 8.0494e-02],
        [ 7.5638e-02],
        [-2.8689e-02],
        [-1.4269e-01],
        [-4.9443e-02],
        [ 8.2847e-02],
        [ 2.1652e-01],
        [-1.6473e-01],
        [-1.5513e-01],
        [ 6.1290e-03],
        [-2.7664e-01],
        [ 5.3055e-02],
        [ 2.1360e-03],
        [ 2.4987e-01],
        [-5.2545e-02],
        [-2.0058e-01],
        [-2.3630e-01],
        [-1.3790e-01],
        [ 1.7941e-01],
        [-1.9260e-01],
        [-2.5397e-01],
        [ 2.0409e-01],
        [ 1.1244e-01],
        [-2.3487e-01],
        [-1.9583e-02],
        [ 1.1350e-01],
        [-4.0448e-02],
        [ 4.2973e-03],
        [ 1.7843e-02],
        [-1.4157e-01],
        [-8.7973e-02],
        [ 1.9908e-01],
        [-6.3253e-02],
        [ 1.0792e-01],
        [-2.3144e-01],
        [-1.2919e-03],
        [ 4.1660e-02],
        [ 1.5430e-01],
        [ 5.0952e-02],
        [ 1.7542e-01],
        [-2.0897e-02],
        [-7.3385e-02],
        [-1.0798e-02],
        [ 1.8288e-02],
        [-2.6945e-02],
        [ 1.6610e-03],
        [-1.1076e-03],
        [-1.1324e-01],
        [-1.2780e-01],
        [-5.7391e-02],
        [ 1.7151e-01],
        [ 1.0252e-02],
        [ 6.6029e-02],
        [ 2.8051e-01],
        [ 4.3485e-03],
        [-1.0282e-02],
        [ 8.6976e-03],
        [-5.4800e-02],
        [-9.0169e-02],
        [-1.6431e-01],
        [ 1.6586e-01],
        [ 2.1416e-01],
        [ 2.1827e-01],
        [ 2.0055e-01],
        [ 2.0638e-01],
        [-4.3315e-02],
        [ 1.4722e-01],
        [-2.5828e-02],
        [ 1.2404e-01],
        [-1.7739e-01],
        [-1.5192e-01],
        [ 2.5704e-02],
        [-4.3143e-02],
        [-2.3581e-01],
        [ 7.3911e-02],
        [-2.0584e-01],
        [ 1.7139e-01],
        [-1.1298e-01],
        [ 1.9632e-01],
        [ 7.8448e-02],
        [ 2.6058e-01],
        [-1.3587e-01],
        [ 1.0749e-01],
        [ 2.5974e-02],
        [-2.1592e-02],
        [-1.8628e-01],
        [-1.3540e-01],
        [ 1.5748e-01],
        [ 7.9912e-02],
        [ 1.5183e-01],
        [-1.6635e-01],
        [-2.8557e-01],
        [ 2.7455e-02],
        [-9.5649e-02],
        [-9.6349e-02],
        [ 7.9806e-03],
        [-2.6264e-01],
        [ 2.5279e-01],
        [ 1.8254e-01],
        [ 2.1853e-01],
        [-2.5442e-01],
        [-8.6257e-02],
        [ 2.6549e-01],
        [ 2.6515e-01],
        [ 9.6293e-02],
        [ 5.5211e-02],
        [-2.2129e-01],
        [-5.3984e-02],
        [ 2.3089e-02],
        [-2.1744e-01],
        [ 4.8821e-02],
        [ 1.6290e-01],
        [ 1.3162e-01],
        [-3.1058e-03],
        [ 8.1649e-02],
        [ 1.8362e-02],
        [ 1.7592e-01],
        [ 8.4588e-02],
        [-4.2499e-02],
        [-7.8959e-03],
        [-2.5399e-02],
        [ 4.8998e-03],
        [ 1.3531e-01],
        [ 8.2431e-02],
        [ 2.4697e-01],
        [-1.3244e-01],
        [ 2.5666e-01],
        [-7.8476e-02],
        [-3.8365e-02],
        [-2.6829e-01],
        [-8.7670e-03],
        [ 2.0512e-01],
        [ 1.2879e-01],
        [-1.4761e-01],
        [ 2.3691e-01],
        [-4.8003e-02],
        [-2.3316e-01],
        [ 9.7141e-02],
        [-7.5823e-02],
        [-6.2622e-02],
        [ 2.5941e-01],
        [ 3.9201e-02],
        [-3.3601e-02],
        [ 1.6064e-01],
        [-6.0582e-02],
        [ 4.4400e-03],
        [ 1.7584e-01],
        [-6.8244e-03],
        [-1.3787e-01],
        [ 1.1561e-01],
        [ 1.1154e-01],
        [ 2.7968e-01],
        [-1.3480e-01],
        [ 1.3857e-01],
        [-3.6016e-02],
        [-2.7509e-02],
        [-9.3208e-02],
        [ 1.3300e-01],
        [ 2.4531e-03],
        [-9.9991e-02],
        [ 1.0147e-01],
        [-5.4453e-02],
        [ 2.8236e-01],
        [-1.0985e-01],
        [-6.7864e-02],
        [-1.7225e-01],
        [ 2.2750e-01],
        [-6.5884e-03],
        [ 4.8691e-02],
        [-1.8331e-01],
        [ 2.0753e-04],
        [ 7.1666e-02],
        [-1.7514e-01],
        [-1.8258e-01],
        [-2.4540e-01],
        [ 6.6715e-02],
        [-1.2591e-01],
        [ 3.0574e-02],
        [-1.5273e-01],
        [-1.0912e-01],
        [ 2.6641e-01],
        [-2.8203e-01],
        [ 4.2713e-03],
        [-6.2700e-02],
        [ 5.3705e-02],
        [ 1.2111e-01],
        [ 9.4633e-02],
        [ 2.4053e-01],
        [ 1.5012e-01],
        [-9.1834e-03],
        [-9.3533e-02],
        [ 2.8334e-02],
        [-1.2563e-01],
        [ 6.9178e-02],
        [ 6.4434e-02],
        [ 3.4624e-03],
        [ 2.4689e-01],
        [ 5.5635e-03],
        [-5.9980e-02],
        [ 8.3504e-02],
        [ 1.0892e-01],
        [-3.4532e-03],
        [ 1.3487e-01],
        [ 2.5907e-01],
        [ 2.6931e-01],
        [ 1.4310e-01],
        [ 1.2310e-01],
        [-1.6290e-03],
        [-1.4665e-01],
        [-2.3596e-01],
        [-1.3749e-02],
        [ 4.8266e-03],
        [-1.2748e-02],
        [-4.8328e-02],
        [-2.3703e-01],
        [-2.4418e-01],
        [-2.1650e-02],
        [ 3.2728e-02],
        [-2.2751e-02],
        [ 2.1606e-01],
        [ 1.8376e-01],
        [ 2.0694e-02],
        [-5.1907e-02],
        [ 6.5181e-02],
        [ 2.3394e-02],
        [ 1.2099e-01],
        [-2.5605e-01],
        [-1.8884e-01],
        [-7.3468e-03],
        [-1.3185e-01],
        [ 1.2943e-01],
        [ 1.7761e-02],
        [-5.3580e-02],
        [-1.3930e-02],
        [-1.1675e-01],
        [-2.5000e-01],
        [ 3.7577e-02],
        [-1.4312e-01],
        [ 7.0129e-02],
        [ 2.6625e-01],
        [ 1.4973e-01],
        [-3.3498e-02],
        [-1.1461e-01],
        [ 8.4707e-02],
        [ 3.0068e-01],
        [ 8.5623e-02],
        [-7.8030e-03],
        [ 5.4140e-02],
        [-1.5015e-01],
        [-2.9335e-01],
        [ 9.0831e-02],
        [-1.8657e-01],
        [ 1.0767e-01],
        [-6.7313e-02],
        [ 1.4511e-01],
        [-8.4021e-02],
        [-4.6503e-02]], device='cuda:0', grad_fn=<SubBackward0>)
max of grad d_p:  tensor(0.0106, device='cuda:0')
min of grad d_p:  tensor(-0.0819, device='cuda:0')
J_L:  tensor([[-4.8385e-05],
        [ 1.2540e-03],
        [ 3.3021e-05],
        ...,
        [ 2.5367e-03],
        [ 1.9587e-03],
        [-8.1886e-02]], device='cuda:0') 
Jta:  tensor([-8.0972e-06,  4.8762e-03, -1.2125e-05,  ...,  6.6531e-03,
         6.5654e-03, -8.1886e-02], device='cuda:0')
max|min: (J_L, Jta/N)  (0.01059434562921524, 0.05017261207103729, ratio: 4.7357916831970215)|(-0.08188621699810028, -0.08188624680042267)

 check Jacobi res:  torch.Size([532609]) max:  tensor(0.0594, device='cuda:0') mean:  tensor(-0.0002, device='cuda:0') min:  tensor(-0.0396, device='cuda:0') norm:  tensor(1.2835, device='cuda:0') MSE:  tensor(2.4099e-06, device='cuda:0')
BAD Jacobian OCCURS!

 check NTK dimension reduction res:  torch.Size([532609, 1]) max:  tensor(0.0003, device='cuda:0') mean:  tensor(3.6665e-06, device='cuda:0') min:  tensor(0., device='cuda:0') norm:  tensor(0.0061, device='cuda:0') MSE:  tensor(1.1442e-08, device='cuda:0')
Shape check:  torch.Size([532609, 1])
max of d_p_list:  tensor(0.0094, device='cuda:0')
min of d_p_list:  tensor(-0.0047, device='cuda:0')
Epoch:  1  
Training Loss: 0.054250793167739175
Test Loss:  0.06109918653964996
Test Acc:  0.0
Valid Loss:  0.05642188340425491
Valid Acc:  0.0
local minima detector shape:  (0,)
Preserved_eigens number check:  512
100%|██████████| 1/1 [00:02<00:00,  2.40s/it]100%|██████████| 1/1 [00:02<00:00,  2.40s/it]
