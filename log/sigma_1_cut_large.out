/home/yuyi/Documents/ModifiedNGD/utils/readData.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)
/home/yuyi/anaconda3/envs/ng/lib/python3.10/site-packages/torch/autograd/__init__.py:200: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343967769/work/torch/csrc/autograd/engine.cpp:1151.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
train data shape torch.Size([256, 2])
train label shape torch.Size([256, 1])
torch.Size([256, 3])
train_data shape torch.Size([3])
seed is  1
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:03<25:48,  3.10s/it]  1%|          | 3/500 [00:03<07:03,  1.17it/s]  1%|          | 5/500 [00:03<03:41,  2.24it/s]  1%|▏         | 7/500 [00:03<02:20,  3.52it/s]  2%|▏         | 9/500 [00:03<01:38,  4.98it/s]  2%|▏         | 11/500 [00:06<05:31,  1.48it/s]  3%|▎         | 13/500 [00:06<03:50,  2.11it/s]  3%|▎         | 15/500 [00:07<02:45,  2.92it/s]  3%|▎         | 17/500 [00:07<02:02,  3.93it/s]  4%|▍         | 19/500 [00:07<01:34,  5.11it/s]  4%|▍         | 21/500 [00:10<04:48,  1.66it/s]  5%|▍         | 23/500 [00:10<03:29,  2.28it/s]  5%|▌         | 25/500 [00:10<02:34,  3.08it/s]  5%|▌         | 27/500 [00:10<01:56,  4.07it/s]  6%|▌         | 29/500 [00:10<01:29,  5.24it/s]  6%|▌         | 31/500 [00:13<04:42,  1.66it/s]  7%|▋         | 33/500 [00:14<03:25,  2.27it/s]  7%|▋         | 35/500 [00:14<02:32,  3.05it/s]  7%|▋         | 37/500 [00:14<01:55,  4.02it/s]  8%|▊         | 39/500 [00:14<01:29,  5.17it/s]  8%|▊         | 41/500 [00:17<04:35,  1.66it/s]  9%|▊         | 43/500 [00:17<03:20,  2.27it/s]  9%|▉         | 45/500 [00:17<02:28,  3.06it/s]  9%|▉         | 47/500 [00:17<01:52,  4.04it/s] 10%|▉         | 49/500 [00:18<01:26,  5.19it/s] 10%|█         | 51/500 [00:21<04:24,  1.69it/s] 11%|█         | 53/500 [00:21<03:14,  2.30it/s] 11%|█         | 55/500 [00:21<02:23,  3.09it/s] 11%|█▏        | 57/500 [00:21<01:48,  4.07it/s] 12%|█▏        | 59/500 [00:21<01:24,  5.23it/s] 12%|█▏        | 61/500 [00:24<04:22,  1.67it/s] 13%|█▎        | 63/500 [00:24<03:11,  2.29it/s] 13%|█▎        | 65/500 [00:24<02:21,  3.08it/s] 13%|█▎        | 67/500 [00:25<01:46,  4.06it/s] 14%|█▍        | 69/500 [00:25<01:22,  5.22it/s] 14%|█▍        | 71/500 [00:28<04:14,  1.69it/s]Epoch:  1  	Training Loss: 0.013355491682887077
Test Loss:  2.531846046447754
Valid Loss:  2.5609800815582275
Epoch:  2  	Training Loss: 2.411831855773926
Test Loss:  0.055232804268598557
Valid Loss:  0.0442972406744957
Epoch:  3  	Training Loss: 0.03997649997472763
Test Loss:  0.04981371760368347
Valid Loss:  0.03969664126634598
Epoch:  4  	Training Loss: 0.0358748733997345
Test Loss:  0.04503393918275833
Valid Loss:  0.03566755726933479
Epoch:  5  	Training Loss: 0.03231150656938553
Test Loss:  0.040813907980918884
Valid Loss:  0.032136548310518265
Epoch:  6  	Training Loss: 0.029214706271886826
Test Loss:  0.037084270268678665
Valid Loss:  0.029039766639471054
Epoch:  7  	Training Loss: 0.0265223179012537
Test Loss:  0.03378451243042946
Valid Loss:  0.026321668177843094
Epoch:  8  	Training Loss: 0.024180468171834946
Test Loss:  0.030861852690577507
Valid Loss:  0.023933952674269676
Epoch:  9  	Training Loss: 0.022142477333545685
Test Loss:  0.02827455848455429
Valid Loss:  0.021841444075107574
Epoch:  10  	Training Loss: 0.020371612161397934
Test Loss:  0.026049038395285606
Valid Loss:  0.02009146474301815
Epoch:  11  	Training Loss: 0.01888248324394226
Test Loss:  0.0241683479398489
Valid Loss:  0.018601730465888977
Epoch:  12  	Training Loss: 0.01763761416077614
Test Loss:  0.022504597902297974
Valid Loss:  0.017286423593759537
Epoch:  13  	Training Loss: 0.01655951887369156
Test Loss:  0.021007712930440903
Valid Loss:  0.01611272804439068
Epoch:  14  	Training Loss: 0.015607405453920364
Test Loss:  0.019675282761454582
Valid Loss:  0.01509035937488079
Epoch:  15  	Training Loss: 0.014778586104512215
Test Loss:  0.01881229504942894
Valid Loss:  0.0146226417273283
Epoch:  16  	Training Loss: 0.014297558926045895
Test Loss:  0.018593203276395798
Valid Loss:  0.014535319060087204
Epoch:  17  	Training Loss: 0.014164199121296406
Test Loss:  0.01854410395026207
Valid Loss:  0.014515921473503113
Epoch:  18  	Training Loss: 0.014140387997031212
Test Loss:  0.018538396805524826
Valid Loss:  0.014510837383568287
Epoch:  19  	Training Loss: 0.014134077355265617
Test Loss:  0.018535012379288673
Valid Loss:  0.014507574960589409
Epoch:  20  	Training Loss: 0.014129014685750008
Test Loss:  0.018534306436777115
Valid Loss:  0.014505472034215927
Epoch:  21  	Training Loss: 0.014124365523457527
Test Loss:  0.018534205853939056
Valid Loss:  0.014503635466098785
Epoch:  22  	Training Loss: 0.014119759202003479
Test Loss:  0.018534060567617416
Valid Loss:  0.01450175791978836
Epoch:  23  	Training Loss: 0.014115121215581894
Test Loss:  0.018533753231167793
Valid Loss:  0.01449982076883316
Epoch:  24  	Training Loss: 0.014110530726611614
Test Loss:  0.018535282462835312
Valid Loss:  0.01449867058545351
Epoch:  25  	Training Loss: 0.01410597376525402
Test Loss:  0.018534790724515915
Valid Loss:  0.01449667476117611
Epoch:  26  	Training Loss: 0.014101400971412659
Test Loss:  0.018535196781158447
Valid Loss:  0.014495071023702621
Epoch:  27  	Training Loss: 0.014096880331635475
Test Loss:  0.018535520881414413
Valid Loss:  0.014493441209197044
Epoch:  28  	Training Loss: 0.014092381112277508
Test Loss:  0.018535759299993515
Valid Loss:  0.014491783455014229
Epoch:  29  	Training Loss: 0.014087908901274204
Test Loss:  0.018536873161792755
Valid Loss:  0.014490517787635326
Epoch:  30  	Training Loss: 0.014083483256399632
Test Loss:  0.01853596791625023
Valid Loss:  0.014488443732261658
Epoch:  31  	Training Loss: 0.014079047366976738
Test Loss:  0.018536938354372978
Valid Loss:  0.014487073756754398
Epoch:  32  	Training Loss: 0.014074627310037613
Test Loss:  0.01853778399527073
Valid Loss:  0.014485706575214863
Epoch:  33  	Training Loss: 0.014070243574678898
Test Loss:  0.018536601215600967
Valid Loss:  0.014483598992228508
Epoch:  34  	Training Loss: 0.014065863564610481
Test Loss:  0.018537316471338272
Valid Loss:  0.014482134953141212
Epoch:  35  	Training Loss: 0.014061497524380684
Test Loss:  0.018537938594818115
Valid Loss:  0.014480655081570148
Epoch:  36  	Training Loss: 0.014057159423828125
Test Loss:  0.018538469448685646
Valid Loss:  0.014479164965450764
Epoch:  37  	Training Loss: 0.01405284833163023
Test Loss:  0.018538914620876312
Valid Loss:  0.014477658085525036
Epoch:  38  	Training Loss: 0.014048563316464424
Test Loss:  0.018539272248744965
Valid Loss:  0.014476140029728413
Epoch:  39  	Training Loss: 0.014044299721717834
Test Loss:  0.01853954792022705
Valid Loss:  0.01447460986673832
Epoch:  40  	Training Loss: 0.01404005941003561
Test Loss:  0.018539749085903168
Valid Loss:  0.014473064802587032
Epoch:  41  	Training Loss: 0.014035839587450027
Test Loss:  0.018539870157837868
Valid Loss:  0.014471507631242275
Epoch:  42  	Training Loss: 0.014031639322638512
Test Loss:  0.018539946526288986
Valid Loss:  0.014469965361058712
Epoch:  43  	Training Loss: 0.014027477242052555
Test Loss:  0.018539952114224434
Valid Loss:  0.014468410983681679
Epoch:  44  	Training Loss: 0.014023332856595516
Test Loss:  0.01853989064693451
Valid Loss:  0.014466846361756325
Epoch:  45  	Training Loss: 0.014019204303622246
Test Loss:  0.01853976398706436
Valid Loss:  0.014465268701314926
Epoch:  46  	Training Loss: 0.014015092514455318
Test Loss:  0.01853957399725914
Valid Loss:  0.014463679865002632
Epoch:  47  	Training Loss: 0.01401099469512701
Test Loss:  0.018539324402809143
Valid Loss:  0.014462081715464592
Epoch:  48  	Training Loss: 0.014006920158863068
Test Loss:  0.018539883196353912
Valid Loss:  0.014460723847150803
Epoch:  49  	Training Loss: 0.014002871699631214
Test Loss:  0.018539585173130035
Valid Loss:  0.014459126628935337
Epoch:  50  	Training Loss: 0.013998848386108875
Test Loss:  0.01854010298848152
Valid Loss:  0.01445777714252472
Epoch:  51  	Training Loss: 0.013994839042425156
Test Loss:  0.018540430814027786
Valid Loss:  0.014456379227340221
Epoch:  52  	Training Loss: 0.013990853913128376
Test Loss:  0.018540751188993454
Valid Loss:  0.01445496454834938
Epoch:  53  	Training Loss: 0.01398685947060585
Test Loss:  0.018540993332862854
Valid Loss:  0.014453531242907047
Epoch:  54  	Training Loss: 0.013982885517179966
Test Loss:  0.018541157245635986
Valid Loss:  0.01445208489894867
Epoch:  55  	Training Loss: 0.013978929258883
Test Loss:  0.018541252240538597
Valid Loss:  0.014450622722506523
Epoch:  56  	Training Loss: 0.013974988833069801
Test Loss:  0.01854127272963524
Valid Loss:  0.014449145644903183
Epoch:  57  	Training Loss: 0.013971067033708096
Test Loss:  0.01854122430086136
Valid Loss:  0.014447657391428947
Epoch:  58  	Training Loss: 0.013967160135507584
Test Loss:  0.018541112542152405
Valid Loss:  0.014446154236793518
Epoch:  59  	Training Loss: 0.013963267207145691
Test Loss:  0.018540935590863228
Valid Loss:  0.014444636180996895
Epoch:  60  	Training Loss: 0.013959389179944992
Test Loss:  0.018540695309638977
Valid Loss:  0.014443106949329376
Epoch:  61  	Training Loss: 0.013955541886389256
Test Loss:  0.018541377037763596
Valid Loss:  0.014441872015595436
Epoch:  62  	Training Loss: 0.013951706700026989
Test Loss:  0.018541855737566948
Valid Loss:  0.014440574683248997
Epoch:  63  	Training Loss: 0.013947895728051662
Test Loss:  0.018541477620601654
Valid Loss:  0.014439016580581665
Epoch:  64  	Training Loss: 0.013944096863269806
Test Loss:  0.01854192279279232
Valid Loss:  0.014437727630138397
Epoch:  65  	Training Loss: 0.013940319418907166
Test Loss:  0.01854228973388672
Valid Loss:  0.014436421915888786
Epoch:  66  	Training Loss: 0.013936560600996017
Test Loss:  0.018542572855949402
Valid Loss:  0.01443509478121996
Epoch:  67  	Training Loss: 0.013932820409536362
Test Loss:  0.018542783334851265
Valid Loss:  0.014433752745389938
Epoch:  68  	Training Loss: 0.013929096981883049
Test Loss:  0.01854291744530201
Valid Loss:  0.014432394877076149
Epoch:  69  	Training Loss: 0.01392539031803608
Test Loss:  0.018542984500527382
Valid Loss:  0.014431020244956017
Epoch:  70  	Training Loss: 0.013921701349318027
Test Loss:  0.018542978912591934
Valid Loss:  0.01442963071167469
Epoch:  71  	Training Loss: 0.013918024487793446
Test Loss:  0.018542911857366562
Valid Loss:  0.014428223483264446
 15%|█▍        | 73/500 [00:28<03:05,  2.30it/s] 15%|█▌        | 75/500 [00:28<02:17,  3.09it/s] 15%|█▌        | 77/500 [00:28<01:44,  4.06it/s] 16%|█▌        | 79/500 [00:28<01:20,  5.22it/s] 16%|█▌        | 81/500 [00:31<04:08,  1.69it/s] 17%|█▋        | 83/500 [00:31<03:00,  2.31it/s] 17%|█▋        | 85/500 [00:32<02:13,  3.10it/s] 17%|█▋        | 87/500 [00:32<01:41,  4.08it/s] 18%|█▊        | 89/500 [00:32<01:18,  5.25it/s] 18%|█▊        | 91/500 [00:35<04:02,  1.69it/s] 19%|█▊        | 93/500 [00:35<02:57,  2.30it/s] 19%|█▉        | 95/500 [00:35<02:11,  3.08it/s] 19%|█▉        | 97/500 [00:35<01:39,  4.06it/s] 20%|█▉        | 99/500 [00:35<01:16,  5.22it/s] 20%|██        | 101/500 [00:38<03:54,  1.70it/s] 21%|██        | 103/500 [00:39<02:51,  2.32it/s] 21%|██        | 105/500 [00:39<02:06,  3.12it/s] 21%|██▏       | 107/500 [00:39<01:35,  4.10it/s] 22%|██▏       | 109/500 [00:39<01:14,  5.27it/s] 22%|██▏       | 111/500 [00:42<03:49,  1.70it/s] 23%|██▎       | 113/500 [00:42<02:47,  2.31it/s] 23%|██▎       | 115/500 [00:42<02:04,  3.10it/s] 23%|██▎       | 117/500 [00:42<01:33,  4.08it/s] 24%|██▍       | 119/500 [00:43<01:12,  5.24it/s] 24%|██▍       | 121/500 [00:46<03:41,  1.71it/s] 25%|██▍       | 123/500 [00:46<02:41,  2.33it/s] 25%|██▌       | 125/500 [00:46<01:59,  3.14it/s] 25%|██▌       | 127/500 [00:46<01:30,  4.12it/s] 26%|██▌       | 129/500 [00:46<01:10,  5.29it/s] 26%|██▌       | 131/500 [00:49<03:36,  1.71it/s] 27%|██▋       | 133/500 [00:49<02:37,  2.32it/s] 27%|██▋       | 135/500 [00:49<01:57,  3.12it/s] 27%|██▋       | 137/500 [00:49<01:28,  4.10it/s] 28%|██▊       | 139/500 [00:50<01:10,  5.16it/s] 28%|██▊       | 141/500 [00:53<03:32,  1.69it/s]Epoch:  72  	Training Loss: 0.013914361596107483
Test Loss:  0.018542880192399025
Valid Loss:  0.014426900073885918
Epoch:  73  	Training Loss: 0.01391078531742096
Test Loss:  0.018542787060141563
Valid Loss:  0.01442556083202362
Epoch:  74  	Training Loss: 0.013907221145927906
Test Loss:  0.018542632460594177
Valid Loss:  0.014424205757677555
Epoch:  75  	Training Loss: 0.013903666287660599
Test Loss:  0.018542420119047165
Valid Loss:  0.014422837644815445
Epoch:  76  	Training Loss: 0.013900140300393105
Test Loss:  0.018543042242527008
Valid Loss:  0.01442175917327404
Epoch:  77  	Training Loss: 0.013896634802222252
Test Loss:  0.01854368858039379
Valid Loss:  0.014420697465538979
Epoch:  78  	Training Loss: 0.013893154449760914
Test Loss:  0.01854335516691208
Valid Loss:  0.014419306069612503
Epoch:  79  	Training Loss: 0.013889679685235023
Test Loss:  0.01854386366903782
Valid Loss:  0.014418213628232479
Epoch:  80  	Training Loss: 0.013886228203773499
Test Loss:  0.018544290214776993
Valid Loss:  0.014417095109820366
Epoch:  81  	Training Loss: 0.013882791623473167
Test Loss:  0.018544642254710197
Valid Loss:  0.014415958896279335
Epoch:  82  	Training Loss: 0.01387937180697918
Test Loss:  0.018544871360063553
Valid Loss:  0.014414763078093529
Epoch:  83  	Training Loss: 0.013875933364033699
Test Loss:  0.018545031547546387
Valid Loss:  0.01441355049610138
Epoch:  84  	Training Loss: 0.013872511684894562
Test Loss:  0.01854512467980385
Valid Loss:  0.014412318356335163
Epoch:  85  	Training Loss: 0.013869103044271469
Test Loss:  0.018545150756835938
Valid Loss:  0.014411071315407753
Epoch:  86  	Training Loss: 0.013865708373486996
Test Loss:  0.018545113503932953
Valid Loss:  0.014409809373319149
Epoch:  87  	Training Loss: 0.013862330466508865
Test Loss:  0.01854589954018593
Valid Loss:  0.014408840797841549
Epoch:  88  	Training Loss: 0.0138589758425951
Test Loss:  0.018545717000961304
Valid Loss:  0.014407536946237087
Epoch:  89  	Training Loss: 0.013855627737939358
Test Loss:  0.018546463921666145
Valid Loss:  0.014406571164727211
Epoch:  90  	Training Loss: 0.013852303847670555
Test Loss:  0.01854613423347473
Valid Loss:  0.01440522726625204
Epoch:  91  	Training Loss: 0.013848993927240372
Test Loss:  0.01854674331843853
Valid Loss:  0.01440422609448433
Epoch:  92  	Training Loss: 0.013845697045326233
Test Loss:  0.018547259271144867
Valid Loss:  0.01440318301320076
Epoch:  93  	Training Loss: 0.013842402957379818
Test Loss:  0.018547693267464638
Valid Loss:  0.014402119442820549
Epoch:  94  	Training Loss: 0.01383912656456232
Test Loss:  0.01854805089533329
Valid Loss:  0.014401034452021122
Epoch:  95  	Training Loss: 0.013835866004228592
Test Loss:  0.018548335880041122
Valid Loss:  0.014399930834770203
Epoch:  96  	Training Loss: 0.013832622207701206
Test Loss:  0.018548548221588135
Valid Loss:  0.014398802071809769
Epoch:  97  	Training Loss: 0.013829391449689865
Test Loss:  0.018548693507909775
Valid Loss:  0.014397661201655865
Epoch:  98  	Training Loss: 0.013826176524162292
Test Loss:  0.01854877546429634
Valid Loss:  0.014396503567695618
Epoch:  99  	Training Loss: 0.013822974637150764
Test Loss:  0.018548792228102684
Valid Loss:  0.014395326375961304
Epoch:  100  	Training Loss: 0.013819783926010132
Test Loss:  0.018548745661973953
Valid Loss:  0.014394130557775497
Epoch:  101  	Training Loss: 0.013816606253385544
Test Loss:  0.018548637628555298
Valid Loss:  0.014392919838428497
Epoch:  102  	Training Loss: 0.013813438825309277
Test Loss:  0.018548496067523956
Valid Loss:  0.014391705393791199
Epoch:  103  	Training Loss: 0.013810292817652225
Test Loss:  0.01854829117655754
Valid Loss:  0.01439046859741211
Epoch:  104  	Training Loss: 0.013807153329253197
Test Loss:  0.018548037856817245
Valid Loss:  0.0143892215564847
Epoch:  105  	Training Loss: 0.013804025948047638
Test Loss:  0.01854773610830307
Valid Loss:  0.01438796054571867
Epoch:  106  	Training Loss: 0.013800919987261295
Test Loss:  0.01854826882481575
Valid Loss:  0.014387017115950584
Epoch:  107  	Training Loss: 0.013797832652926445
Test Loss:  0.018548836931586266
Valid Loss:  0.014386090449988842
Epoch:  108  	Training Loss: 0.013794763013720512
Test Loss:  0.018548432737588882
Valid Loss:  0.014384805224835873
Epoch:  109  	Training Loss: 0.013791707344353199
Test Loss:  0.01854887418448925
Valid Loss:  0.014383845031261444
Epoch:  110  	Training Loss: 0.013788663782179356
Test Loss:  0.01854924112558365
Valid Loss:  0.0143828634172678
Epoch:  111  	Training Loss: 0.013785634189844131
Test Loss:  0.018549539148807526
Valid Loss:  0.014381857588887215
Epoch:  112  	Training Loss: 0.013782619498670101
Test Loss:  0.018549777567386627
Valid Loss:  0.01438085176050663
Epoch:  113  	Training Loss: 0.013779629953205585
Test Loss:  0.018549950793385506
Valid Loss:  0.014379826374351978
Epoch:  114  	Training Loss: 0.013776654377579689
Test Loss:  0.01855006068944931
Valid Loss:  0.014378784224390984
Epoch:  115  	Training Loss: 0.013773689046502113
Test Loss:  0.01855010911822319
Valid Loss:  0.014377720654010773
Epoch:  116  	Training Loss: 0.013770735822618008
Test Loss:  0.018550096079707146
Valid Loss:  0.01437663845717907
Epoch:  117  	Training Loss: 0.013767793774604797
Test Loss:  0.018550027161836624
Valid Loss:  0.014375541359186172
Epoch:  118  	Training Loss: 0.013764861971139908
Test Loss:  0.018549900501966476
Valid Loss:  0.014374425634741783
Epoch:  119  	Training Loss: 0.013761939480900764
Test Loss:  0.01854972541332245
Valid Loss:  0.014373294077813625
Epoch:  120  	Training Loss: 0.013759026303887367
Test Loss:  0.018549494445323944
Valid Loss:  0.014372145757079124
Epoch:  121  	Training Loss: 0.013756124302744865
Test Loss:  0.01855010725557804
Valid Loss:  0.014371329918503761
Epoch:  122  	Training Loss: 0.013753248378634453
Test Loss:  0.018549714237451553
Valid Loss:  0.014370111748576164
Epoch:  123  	Training Loss: 0.013750359416007996
Test Loss:  0.01855028048157692
Valid Loss:  0.014369271695613861
Epoch:  124  	Training Loss: 0.013747484423220158
Test Loss:  0.01855076663196087
Valid Loss:  0.014368407428264618
Epoch:  125  	Training Loss: 0.013744624331593513
Test Loss:  0.01855117827653885
Valid Loss:  0.014367522671818733
Epoch:  126  	Training Loss: 0.013741780072450638
Test Loss:  0.018551521003246307
Valid Loss:  0.01436660997569561
Epoch:  127  	Training Loss: 0.013738946989178658
Test Loss:  0.018551794812083244
Valid Loss:  0.014365681447088718
Epoch:  128  	Training Loss: 0.01373613066971302
Test Loss:  0.01855200156569481
Valid Loss:  0.014364728704094887
Epoch:  129  	Training Loss: 0.01373332366347313
Test Loss:  0.01855214312672615
Valid Loss:  0.014363755472004414
Epoch:  130  	Training Loss: 0.013730529695749283
Test Loss:  0.018552221357822418
Valid Loss:  0.014362761750817299
Epoch:  131  	Training Loss: 0.013727745041251183
Test Loss:  0.01855223812162876
Valid Loss:  0.014361747540533543
Epoch:  132  	Training Loss: 0.013724969699978828
Test Loss:  0.018552200868725777
Valid Loss:  0.014360720291733742
Epoch:  133  	Training Loss: 0.013722206465899944
Test Loss:  0.01855214312672615
Valid Loss:  0.014359707944095135
Epoch:  134  	Training Loss: 0.013719454407691956
Test Loss:  0.01855207048356533
Valid Loss:  0.01435871422290802
Epoch:  135  	Training Loss: 0.013716709800064564
Test Loss:  0.018551956862211227
Valid Loss:  0.014357713982462883
Epoch:  136  	Training Loss: 0.013713981956243515
Test Loss:  0.018552659079432487
Valid Loss:  0.014356967061758041
Epoch:  137  	Training Loss: 0.013711269944906235
Test Loss:  0.018552429974079132
Valid Loss:  0.014355938881635666
Epoch:  138  	Training Loss: 0.013708571903407574
Test Loss:  0.018553022295236588
Valid Loss:  0.014355167746543884
Epoch:  139  	Training Loss: 0.01370587944984436
Test Loss:  0.01855354942381382
Valid Loss:  0.014354381710290909
Epoch:  140  	Training Loss: 0.013703200966119766
Test Loss:  0.018554015085101128
Valid Loss:  0.014353583566844463
Epoch:  141  	Training Loss: 0.01370054017752409
Test Loss:  0.01855355128645897
Valid Loss:  0.014352498576045036
Epoch:  142  	Training Loss: 0.013697889633476734
Test Loss:  0.018553927540779114
 29%|██▊       | 143/500 [00:53<02:34,  2.31it/s] 29%|██▉       | 145/500 [00:53<01:54,  3.10it/s] 29%|██▉       | 147/500 [00:53<01:26,  4.09it/s] 30%|██▉       | 149/500 [00:53<01:06,  5.26it/s] 30%|███       | 151/500 [00:56<03:26,  1.69it/s] 31%|███       | 153/500 [00:56<02:30,  2.30it/s] 31%|███       | 155/500 [00:56<01:51,  3.10it/s] 31%|███▏      | 157/500 [00:57<01:24,  4.07it/s] 32%|███▏      | 159/500 [00:57<01:05,  5.23it/s] 32%|███▏      | 161/500 [01:00<03:18,  1.71it/s] 33%|███▎      | 163/500 [01:00<02:24,  2.33it/s] 33%|███▎      | 165/500 [01:00<01:47,  3.13it/s] 33%|███▎      | 167/500 [01:00<01:20,  4.12it/s] 34%|███▍      | 169/500 [01:00<01:02,  5.28it/s] 34%|███▍      | 171/500 [01:03<03:14,  1.69it/s] 35%|███▍      | 173/500 [01:03<02:22,  2.30it/s] 35%|███▌      | 175/500 [01:04<01:44,  3.10it/s] 35%|███▌      | 177/500 [01:04<01:19,  4.06it/s] 36%|███▌      | 179/500 [01:04<01:01,  5.23it/s] 36%|███▌      | 181/500 [01:07<03:07,  1.70it/s] 37%|███▋      | 183/500 [01:07<02:16,  2.32it/s] 37%|███▋      | 185/500 [01:07<01:40,  3.12it/s] 37%|███▋      | 187/500 [01:07<01:16,  4.11it/s] 38%|███▊      | 189/500 [01:07<00:58,  5.29it/s] 38%|███▊      | 191/500 [01:11<03:07,  1.65it/s] 39%|███▊      | 193/500 [01:11<02:16,  2.24it/s] 39%|███▉      | 195/500 [01:11<01:41,  3.02it/s] 39%|███▉      | 197/500 [01:11<01:16,  3.97it/s] 40%|███▉      | 199/500 [01:11<00:58,  5.11it/s] 40%|████      | 201/500 [01:14<02:58,  1.68it/s] 41%|████      | 203/500 [01:14<02:09,  2.29it/s] 41%|████      | 205/500 [01:14<01:35,  3.08it/s] 41%|████▏     | 207/500 [01:15<01:12,  4.06it/s] 42%|████▏     | 209/500 [01:15<00:55,  5.23it/s] 42%|████▏     | 211/500 [01:18<02:49,  1.70it/s]Valid Loss:  0.014351685531437397
Epoch:  143  	Training Loss: 0.013695251196622849
Test Loss:  0.018554244190454483
Valid Loss:  0.014350855723023415
Epoch:  144  	Training Loss: 0.013692625798285007
Test Loss:  0.018554504960775375
Valid Loss:  0.01435001939535141
Epoch:  145  	Training Loss: 0.013690013438463211
Test Loss:  0.01855471543967724
Valid Loss:  0.014349168166518211
Epoch:  146  	Training Loss: 0.013687413185834885
Test Loss:  0.018554871901869774
Valid Loss:  0.014348305761814117
Epoch:  147  	Training Loss: 0.013684822246432304
Test Loss:  0.01855497807264328
Valid Loss:  0.01434742845594883
Epoch:  148  	Training Loss: 0.01368224062025547
Test Loss:  0.018555033951997757
Valid Loss:  0.014346541836857796
Epoch:  149  	Training Loss: 0.013679670169949532
Test Loss:  0.018555045127868652
Valid Loss:  0.014345644041895866
Epoch:  150  	Training Loss: 0.013677106238901615
Test Loss:  0.018555007874965668
Valid Loss:  0.014344734139740467
Epoch:  151  	Training Loss: 0.013674551621079445
Test Loss:  0.0185549259185791
Valid Loss:  0.014343814924359322
Epoch:  152  	Training Loss: 0.013672005385160446
Test Loss:  0.018554799258708954
Valid Loss:  0.014342878945171833
Epoch:  153  	Training Loss: 0.013669461011886597
Test Loss:  0.018554629758000374
Valid Loss:  0.014341932721436024
Epoch:  154  	Training Loss: 0.013666924089193344
Test Loss:  0.01855442300438881
Valid Loss:  0.014340977184474468
Epoch:  155  	Training Loss: 0.01366439275443554
Test Loss:  0.01855417713522911
Valid Loss:  0.014340011402964592
Epoch:  156  	Training Loss: 0.013661868870258331
Test Loss:  0.01855389028787613
Valid Loss:  0.014339035376906395
Epoch:  157  	Training Loss: 0.013659359887242317
Test Loss:  0.01855435036122799
Valid Loss:  0.014338310807943344
Epoch:  158  	Training Loss: 0.013656867668032646
Test Loss:  0.018554840236902237
Valid Loss:  0.014337598346173763
Epoch:  159  	Training Loss: 0.013654394075274467
Test Loss:  0.01855449005961418
Valid Loss:  0.014336612075567245
Epoch:  160  	Training Loss: 0.013651926070451736
Test Loss:  0.01855488494038582
Valid Loss:  0.014335879124701023
Epoch:  161  	Training Loss: 0.013649467378854752
Test Loss:  0.01855522394180298
Valid Loss:  0.014335131272673607
Epoch:  162  	Training Loss: 0.013647021725773811
Test Loss:  0.018555521965026855
Valid Loss:  0.014334384351968765
Epoch:  163  	Training Loss: 0.013644596561789513
Test Loss:  0.018555764108896255
Valid Loss:  0.01433362066745758
Epoch:  164  	Training Loss: 0.013642181642353535
Test Loss:  0.018555959686636925
Valid Loss:  0.014332849532365799
Epoch:  165  	Training Loss: 0.013639776036143303
Test Loss:  0.018556101247668266
Valid Loss:  0.014332059770822525
Epoch:  166  	Training Loss: 0.013637380674481392
Test Loss:  0.018556196242570877
Valid Loss:  0.014331260696053505
Epoch:  167  	Training Loss: 0.013634992763400078
Test Loss:  0.01855623908340931
Valid Loss:  0.014330445788800716
Epoch:  168  	Training Loss: 0.01363261416554451
Test Loss:  0.01855624094605446
Valid Loss:  0.014329621568322182
Epoch:  169  	Training Loss: 0.013630242086946964
Test Loss:  0.018556196242570877
Valid Loss:  0.014328783378005028
Epoch:  170  	Training Loss: 0.013627876527607441
Test Loss:  0.01855611242353916
Valid Loss:  0.014327937737107277
Epoch:  171  	Training Loss: 0.01362551748752594
Test Loss:  0.018555987626314163
Valid Loss:  0.014327078126370907
Epoch:  172  	Training Loss: 0.01362316869199276
Test Loss:  0.01855582371354103
Valid Loss:  0.014326203614473343
Epoch:  173  	Training Loss: 0.013620821759104729
Test Loss:  0.018555616959929466
Valid Loss:  0.014325318858027458
Epoch:  174  	Training Loss: 0.013618480414152145
Test Loss:  0.018555376678705215
Valid Loss:  0.014324427582323551
Epoch:  175  	Training Loss: 0.013616148382425308
Test Loss:  0.01855510286986828
Valid Loss:  0.014323526062071323
Epoch:  176  	Training Loss: 0.013613819144666195
Test Loss:  0.018554793670773506
Valid Loss:  0.014322612434625626
Epoch:  177  	Training Loss: 0.01361149549484253
Test Loss:  0.018554452806711197
Valid Loss:  0.014321688562631607
Epoch:  178  	Training Loss: 0.013609177432954311
Test Loss:  0.018554076552391052
Valid Loss:  0.01432075910270214
Epoch:  179  	Training Loss: 0.013606863096356392
Test Loss:  0.01855367235839367
Valid Loss:  0.014319819398224354
Epoch:  180  	Training Loss: 0.013604553416371346
Test Loss:  0.0185532383620739
Valid Loss:  0.01431887038052082
Epoch:  181  	Training Loss: 0.013602250255644321
Test Loss:  0.01855277642607689
Valid Loss:  0.014317914843559265
Epoch:  182  	Training Loss: 0.013599948957562447
Test Loss:  0.0185522623360157
Valid Loss:  0.014316925778985023
Epoch:  183  	Training Loss: 0.013597634620964527
Test Loss:  0.018551722168922424
Valid Loss:  0.014315927401185036
Epoch:  184  	Training Loss: 0.013595324009656906
Test Loss:  0.018551155924797058
Valid Loss:  0.014314927160739899
Epoch:  185  	Training Loss: 0.013593018054962158
Test Loss:  0.01855056919157505
Valid Loss:  0.014313915744423866
Epoch:  186  	Training Loss: 0.01359071396291256
Test Loss:  0.018549956381320953
Valid Loss:  0.014312895946204662
Epoch:  187  	Training Loss: 0.013588415458798409
Test Loss:  0.018549319356679916
Valid Loss:  0.01431187428534031
Epoch:  188  	Training Loss: 0.013586119748651981
Test Loss:  0.018548663705587387
Valid Loss:  0.014310844242572784
Epoch:  189  	Training Loss: 0.013583828695118427
Test Loss:  0.01854798197746277
Valid Loss:  0.014309808611869812
Epoch:  190  	Training Loss: 0.013581540435552597
Test Loss:  0.018547283485531807
Valid Loss:  0.014308766461908817
Epoch:  191  	Training Loss: 0.013579254038631916
Test Loss:  0.018546562641859055
Valid Loss:  0.014307716861367226
Epoch:  192  	Training Loss: 0.013576971367001534
Test Loss:  0.018545817583799362
Valid Loss:  0.014306667260825634
Epoch:  193  	Training Loss: 0.013574696145951748
Test Loss:  0.018545053899288177
Valid Loss:  0.01430561300367117
Epoch:  194  	Training Loss: 0.013572422787547112
Test Loss:  0.01854426972568035
Valid Loss:  0.014304552227258682
Epoch:  195  	Training Loss: 0.013570152223110199
Test Loss:  0.018543466925621033
Valid Loss:  0.014303486794233322
Epoch:  196  	Training Loss: 0.01356788631528616
Test Loss:  0.01854265294969082
Valid Loss:  0.014302417635917664
Epoch:  197  	Training Loss: 0.01356562040746212
Test Loss:  0.01854182407259941
Valid Loss:  0.014301344752311707
Epoch:  198  	Training Loss: 0.013563361018896103
Test Loss:  0.01854097656905651
Valid Loss:  0.014300266280770302
Epoch:  199  	Training Loss: 0.013561101630330086
Test Loss:  0.018540114164352417
Valid Loss:  0.014299179427325726
Epoch:  200  	Training Loss: 0.013558842241764069
Test Loss:  0.01853923685848713
Valid Loss:  0.014298093505203724
Epoch:  201  	Training Loss: 0.0135565884411335
Test Loss:  0.018538352102041245
Valid Loss:  0.014297005720436573
Epoch:  202  	Training Loss: 0.01355436909943819
Test Loss:  0.018538229167461395
Valid Loss:  0.014296205714344978
Epoch:  203  	Training Loss: 0.013552185148000717
Test Loss:  0.01853807084262371
Valid Loss:  0.014295395463705063
Epoch:  204  	Training Loss: 0.013550007715821266
Test Loss:  0.018537962809205055
Valid Loss:  0.014294607564806938
Epoch:  205  	Training Loss: 0.013547839596867561
Test Loss:  0.018537815660238266
Valid Loss:  0.014293807558715343
Epoch:  206  	Training Loss: 0.013545677997171879
Test Loss:  0.01853763312101364
Valid Loss:  0.014292995445430279
Epoch:  207  	Training Loss: 0.013543521985411644
Test Loss:  0.01853741705417633
Valid Loss:  0.014292173087596893
Epoch:  208  	Training Loss: 0.013541369698941708
Test Loss:  0.018537161871790886
Valid Loss:  0.014291341416537762
Epoch:  209  	Training Loss: 0.013539224863052368
Test Loss:  0.018536876887083054
Valid Loss:  0.014290500432252884
Epoch:  210  	Training Loss: 0.013537082821130753
Test Loss:  0.018536556512117386
Valid Loss:  0.014289646409451962
Epoch:  211  	Training Loss: 0.01353494729846716
Test Loss:  0.01853621006011963
Valid Loss:  0.014288784936070442
Epoch:  212  	Training Loss: 0.013532815501093864
Test Loss:  0.018535833805799484
Valid Loss:  0.01428791880607605
 43%|████▎     | 213/500 [01:18<02:03,  2.32it/s] 43%|████▎     | 215/500 [01:18<01:31,  3.12it/s] 43%|████▎     | 217/500 [01:18<01:09,  4.10it/s] 44%|████▍     | 219/500 [01:18<00:53,  5.26it/s] 44%|████▍     | 221/500 [01:21<02:44,  1.70it/s] 45%|████▍     | 223/500 [01:21<01:59,  2.32it/s] 45%|████▌     | 225/500 [01:21<01:28,  3.11it/s] 45%|████▌     | 227/500 [01:22<01:06,  4.10it/s] 46%|████▌     | 229/500 [01:22<00:51,  5.28it/s] 46%|████▌     | 231/500 [01:25<02:38,  1.69it/s] 47%|████▋     | 233/500 [01:25<01:55,  2.30it/s] 47%|████▋     | 235/500 [01:25<01:25,  3.09it/s] 47%|████▋     | 237/500 [01:25<01:04,  4.07it/s] 48%|████▊     | 239/500 [01:25<00:49,  5.23it/s] 48%|████▊     | 241/500 [01:28<02:33,  1.69it/s] 49%|████▊     | 243/500 [01:28<01:51,  2.31it/s] 49%|████▉     | 245/500 [01:29<01:22,  3.11it/s] 49%|████▉     | 247/500 [01:29<01:01,  4.10it/s] 50%|████▉     | 249/500 [01:29<00:47,  5.26it/s] 50%|█████     | 251/500 [01:32<02:29,  1.67it/s] 51%|█████     | 253/500 [01:32<01:48,  2.27it/s] 51%|█████     | 255/500 [01:32<01:20,  3.05it/s] 51%|█████▏    | 257/500 [01:32<01:00,  4.02it/s] 52%|█████▏    | 259/500 [01:33<00:46,  5.18it/s] 52%|█████▏    | 261/500 [01:36<02:21,  1.68it/s] 53%|█████▎    | 263/500 [01:36<01:43,  2.29it/s] 53%|█████▎    | 265/500 [01:36<01:16,  3.08it/s] 53%|█████▎    | 267/500 [01:36<00:57,  4.07it/s] 54%|█████▍    | 269/500 [01:36<00:44,  5.23it/s] 54%|█████▍    | 271/500 [01:39<02:16,  1.68it/s] 55%|█████▍    | 273/500 [01:39<01:39,  2.28it/s] 55%|█████▌    | 275/500 [01:39<01:13,  3.07it/s] 55%|█████▌    | 277/500 [01:40<00:55,  4.04it/s] 56%|█████▌    | 279/500 [01:40<00:42,  5.19it/s] 56%|█████▌    | 281/500 [01:43<02:08,  1.70it/s]Epoch:  213  	Training Loss: 0.013530690222978592
Test Loss:  0.01853545382618904
Valid Loss:  0.014287041500210762
Epoch:  214  	Training Loss: 0.013528570532798767
Test Loss:  0.018535111099481583
Valid Loss:  0.014286158606410027
Epoch:  215  	Training Loss: 0.013526452705264091
Test Loss:  0.01853475347161293
Valid Loss:  0.014285266399383545
Epoch:  216  	Training Loss: 0.013524340465664864
Test Loss:  0.018534377217292786
Valid Loss:  0.014284364879131317
Epoch:  217  	Training Loss: 0.01352223102003336
Test Loss:  0.01853397861123085
Valid Loss:  0.014283457770943642
Epoch:  218  	Training Loss: 0.013520127162337303
Test Loss:  0.01853356324136257
Valid Loss:  0.014282542280852795
Epoch:  219  	Training Loss: 0.013518025167286396
Test Loss:  0.0185331292450428
Valid Loss:  0.0142816212028265
Epoch:  220  	Training Loss: 0.013515926897525787
Test Loss:  0.01853267475962639
Valid Loss:  0.014280695468187332
Epoch:  221  	Training Loss: 0.013513831421732903
Test Loss:  0.018532199785113335
Valid Loss:  0.014279758557677269
Epoch:  222  	Training Loss: 0.013511739671230316
Test Loss:  0.01853169873356819
Valid Loss:  0.014278809539973736
Epoch:  223  	Training Loss: 0.013509660959243774
Test Loss:  0.018531806766986847
Valid Loss:  0.014278128743171692
Epoch:  224  	Training Loss: 0.013507599011063576
Test Loss:  0.018531881272792816
Valid Loss:  0.014277433976531029
Epoch:  225  	Training Loss: 0.013505546376109123
Test Loss:  0.01853199303150177
Valid Loss:  0.014276757836341858
Epoch:  226  	Training Loss: 0.013503501191735268
Test Loss:  0.018532071262598038
Valid Loss:  0.014276070520281792
Epoch:  227  	Training Loss: 0.013501465320587158
Test Loss:  0.018532108515501022
Valid Loss:  0.01427537202835083
Epoch:  228  	Training Loss: 0.013499436900019646
Test Loss:  0.01853211596608162
Valid Loss:  0.01427465956658125
Epoch:  229  	Training Loss: 0.013497411273419857
Test Loss:  0.018532084301114082
Valid Loss:  0.014273934066295624
Epoch:  230  	Training Loss: 0.01349539216607809
Test Loss:  0.018532022833824158
Valid Loss:  0.014273193664848804
Epoch:  231  	Training Loss: 0.013493377715349197
Test Loss:  0.018531925976276398
Valid Loss:  0.01427244208753109
Epoch:  232  	Training Loss: 0.013491369783878326
Test Loss:  0.018531814217567444
Valid Loss:  0.0142716895788908
Epoch:  233  	Training Loss: 0.01348937302827835
Test Loss:  0.018531670793890953
Valid Loss:  0.014270924963057041
Epoch:  234  	Training Loss: 0.013487379997968674
Test Loss:  0.018531499430537224
Valid Loss:  0.014270150102674961
Epoch:  235  	Training Loss: 0.01348539162427187
Test Loss:  0.018531300127506256
Valid Loss:  0.01426936686038971
Epoch:  236  	Training Loss: 0.013483408838510513
Test Loss:  0.0185310747474432
Valid Loss:  0.014268568716943264
Epoch:  237  	Training Loss: 0.013481427915394306
Test Loss:  0.018530821427702904
Valid Loss:  0.014267764054238796
Epoch:  238  	Training Loss: 0.013479450717568398
Test Loss:  0.018530547618865967
Valid Loss:  0.014266949146986008
Epoch:  239  	Training Loss: 0.013477480970323086
Test Loss:  0.01853024959564209
Valid Loss:  0.014266125857830048
Epoch:  240  	Training Loss: 0.013475513085722923
Test Loss:  0.018529929220676422
Valid Loss:  0.01426529698073864
Epoch:  241  	Training Loss: 0.013473549857735634
Test Loss:  0.018529584631323814
Valid Loss:  0.014264455065131187
Epoch:  242  	Training Loss: 0.013471588492393494
Test Loss:  0.018529213964939117
Valid Loss:  0.014263602904975414
Epoch:  243  	Training Loss: 0.013469626195728779
Test Loss:  0.018528826534748077
Valid Loss:  0.014262743294239044
Epoch:  244  	Training Loss: 0.013467667624354362
Test Loss:  0.01852904073894024
Valid Loss:  0.014262155629694462
Epoch:  245  	Training Loss: 0.01346573419868946
Test Loss:  0.018528664484620094
Valid Loss:  0.014261304400861263
Epoch:  246  	Training Loss: 0.01346382312476635
Test Loss:  0.01852951943874359
Valid Loss:  0.014261013828217983
Epoch:  247  	Training Loss: 0.013461894355714321
Test Loss:  0.01852906495332718
Valid Loss:  0.014260130003094673
Epoch:  248  	Training Loss: 0.01345999538898468
Test Loss:  0.01852983981370926
Valid Loss:  0.014259811490774155
Epoch:  249  	Training Loss: 0.013458078727126122
Test Loss:  0.01852993294596672
Valid Loss:  0.014259183779358864
Epoch:  250  	Training Loss: 0.013456180691719055
Test Loss:  0.018530063331127167
Valid Loss:  0.014258576557040215
Epoch:  251  	Training Loss: 0.013454297557473183
Test Loss:  0.01853063888847828
Valid Loss:  0.01425817608833313
Epoch:  252  	Training Loss: 0.013452422805130482
Test Loss:  0.018530046567320824
Valid Loss:  0.01425723172724247
Epoch:  253  	Training Loss: 0.013450540602207184
Test Loss:  0.018530623987317085
Valid Loss:  0.014256836846470833
Epoch:  254  	Training Loss: 0.013448657467961311
Test Loss:  0.018531225621700287
Valid Loss:  0.014256453141570091
Epoch:  255  	Training Loss: 0.01344679482281208
Test Loss:  0.01853043958544731
Valid Loss:  0.014255428686738014
Epoch:  256  	Training Loss: 0.013444941490888596
Test Loss:  0.018530983477830887
Valid Loss:  0.01425502635538578
Epoch:  257  	Training Loss: 0.013443081639707088
Test Loss:  0.018531402572989464
Valid Loss:  0.014254567213356495
Epoch:  258  	Training Loss: 0.013441231101751328
Test Loss:  0.01853184960782528
Valid Loss:  0.014254127629101276
Epoch:  259  	Training Loss: 0.013439390808343887
Test Loss:  0.018531618639826775
Valid Loss:  0.014253366738557816
Epoch:  260  	Training Loss: 0.013437563553452492
Test Loss:  0.0185319222509861
Valid Loss:  0.014252861961722374
Epoch:  261  	Training Loss: 0.013435734435915947
Test Loss:  0.01853225938975811
Valid Loss:  0.014252379536628723
Epoch:  262  	Training Loss: 0.013433916494250298
Test Loss:  0.018532592803239822
Valid Loss:  0.01425190083682537
Epoch:  263  	Training Loss: 0.013432119973003864
Test Loss:  0.0185328908264637
Valid Loss:  0.014251415617763996
Epoch:  264  	Training Loss: 0.013430335558950901
Test Loss:  0.018533144146203995
Valid Loss:  0.01425090804696083
Epoch:  265  	Training Loss: 0.013428553938865662
Test Loss:  0.018533358350396156
Valid Loss:  0.014250384643673897
Epoch:  266  	Training Loss: 0.01342677976936102
Test Loss:  0.018533531576395035
Valid Loss:  0.01424984261393547
Epoch:  267  	Training Loss: 0.01342501025646925
Test Loss:  0.018533673137426376
Valid Loss:  0.014249287545681
Epoch:  268  	Training Loss: 0.013423248194158077
Test Loss:  0.018533769994974136
Valid Loss:  0.01424871664494276
Epoch:  269  	Training Loss: 0.013421488925814629
Test Loss:  0.01853383518755436
Valid Loss:  0.014248133637011051
Epoch:  270  	Training Loss: 0.013419737108051777
Test Loss:  0.018533870577812195
Valid Loss:  0.014247532933950424
Epoch:  271  	Training Loss: 0.013417990878224373
Test Loss:  0.018533872440457344
Valid Loss:  0.014246921055018902
Epoch:  272  	Training Loss: 0.013416247442364693
Test Loss:  0.018533842638134956
Valid Loss:  0.014246300794184208
Epoch:  273  	Training Loss: 0.013414520770311356
Test Loss:  0.018533777445554733
Valid Loss:  0.01424566749483347
Epoch:  274  	Training Loss: 0.013412797823548317
Test Loss:  0.01853368990123272
Valid Loss:  0.014245024882256985
Epoch:  275  	Training Loss: 0.013411080464720726
Test Loss:  0.01853356882929802
Valid Loss:  0.01424436829984188
Epoch:  276  	Training Loss: 0.013409364968538284
Test Loss:  0.018533427268266678
Valid Loss:  0.014243701472878456
Epoch:  277  	Training Loss: 0.01340765506029129
Test Loss:  0.01853325590491295
Valid Loss:  0.014243025332689285
Epoch:  278  	Training Loss: 0.013405947014689445
Test Loss:  0.01853306032717228
Valid Loss:  0.01424233429133892
Epoch:  279  	Training Loss: 0.013404258526861668
Test Loss:  0.018533552065491676
Valid Loss:  0.014241982251405716
Epoch:  280  	Training Loss: 0.01340256817638874
Test Loss:  0.018533999100327492
Valid Loss:  0.01424160972237587
Epoch:  281  	Training Loss: 0.013400888070464134
Test Loss:  0.01853368431329727
Valid Loss:  0.01424086932092905
Epoch:  282  	Training Loss: 0.0133992163464427
Test Loss:  0.018534045666456223
Valid Loss:  0.014240453019738197
Epoch:  283  	Training Loss: 0.013397530652582645
Test Loss:   57%|█████▋    | 283/500 [01:43<01:33,  2.32it/s] 57%|█████▋    | 285/500 [01:43<01:08,  3.12it/s] 57%|█████▋    | 287/500 [01:43<00:51,  4.11it/s] 58%|█████▊    | 289/500 [01:43<00:39,  5.28it/s] 58%|█████▊    | 291/500 [01:46<02:03,  1.69it/s] 59%|█████▊    | 293/500 [01:46<01:29,  2.30it/s] 59%|█████▉    | 295/500 [01:47<01:06,  3.10it/s] 59%|█████▉    | 297/500 [01:47<00:49,  4.08it/s] 60%|█████▉    | 299/500 [01:47<00:38,  5.25it/s] 60%|██████    | 301/500 [01:50<01:57,  1.70it/s] 61%|██████    | 303/500 [01:50<01:24,  2.32it/s] 61%|██████    | 305/500 [01:50<01:02,  3.11it/s] 61%|██████▏   | 307/500 [01:50<00:47,  4.10it/s] 62%|██████▏   | 309/500 [01:50<00:36,  5.27it/s] 62%|██████▏   | 311/500 [01:54<01:55,  1.64it/s] 63%|██████▎   | 313/500 [01:54<01:23,  2.23it/s] 63%|██████▎   | 315/500 [01:54<01:01,  3.00it/s] 63%|██████▎   | 317/500 [01:54<00:46,  3.96it/s] 64%|██████▍   | 319/500 [01:54<00:35,  5.11it/s] 64%|██████▍   | 321/500 [01:57<01:45,  1.70it/s] 65%|██████▍   | 323/500 [01:57<01:16,  2.32it/s] 65%|██████▌   | 325/500 [01:57<00:56,  3.12it/s] 65%|██████▌   | 327/500 [01:57<00:42,  4.11it/s] 66%|██████▌   | 329/500 [01:58<00:32,  5.27it/s] 66%|██████▌   | 331/500 [02:01<01:42,  1.65it/s] 67%|██████▋   | 333/500 [02:01<01:14,  2.25it/s] 67%|██████▋   | 335/500 [02:01<00:54,  3.03it/s] 67%|██████▋   | 337/500 [02:01<00:40,  3.99it/s] 68%|██████▊   | 339/500 [02:01<00:31,  5.14it/s] 68%|██████▊   | 341/500 [02:04<01:35,  1.66it/s] 69%|██████▊   | 343/500 [02:04<01:09,  2.27it/s] 69%|██████▉   | 345/500 [02:05<00:50,  3.06it/s] 69%|██████▉   | 347/500 [02:05<00:37,  4.04it/s] 70%|██████▉   | 349/500 [02:05<00:29,  5.20it/s] 70%|███████   | 351/500 [02:08<01:27,  1.70it/s]0.01853436417877674
Valid Loss:  0.014240017160773277
Epoch:  284  	Training Loss: 0.013395853340625763
Test Loss:  0.018534643575549126
Valid Loss:  0.014239564538002014
Epoch:  285  	Training Loss: 0.013394182547926903
Test Loss:  0.018534880131483078
Valid Loss:  0.014239095151424408
Epoch:  286  	Training Loss: 0.01339251920580864
Test Loss:  0.018535081297159195
Valid Loss:  0.014238609001040459
Epoch:  287  	Training Loss: 0.013390861451625824
Test Loss:  0.018535245209932327
Valid Loss:  0.014238109812140465
Epoch:  288  	Training Loss: 0.01338921021670103
Test Loss:  0.018535373732447624
Valid Loss:  0.014237593859434128
Epoch:  289  	Training Loss: 0.01338756363838911
Test Loss:  0.018535465002059937
Valid Loss:  0.014237062074244022
Epoch:  290  	Training Loss: 0.013385921716690063
Test Loss:  0.01853552833199501
Valid Loss:  0.014236518181860447
Epoch:  291  	Training Loss: 0.013384287245571613
Test Loss:  0.0185355544090271
Valid Loss:  0.014235958456993103
Epoch:  292  	Training Loss: 0.013382655568420887
Test Loss:  0.018535558134317398
Valid Loss:  0.014235390350222588
Epoch:  293  	Training Loss: 0.013381033204495907
Test Loss:  0.01853553019464016
Valid Loss:  0.014234809204936028
Epoch:  294  	Training Loss: 0.0133794154971838
Test Loss:  0.018535474315285683
Valid Loss:  0.014234215021133423
Epoch:  295  	Training Loss: 0.013377801515161991
Test Loss:  0.018535388633608818
Valid Loss:  0.014233610592782497
Epoch:  296  	Training Loss: 0.013376191258430481
Test Loss:  0.018535278737545013
Valid Loss:  0.014232993125915527
Epoch:  297  	Training Loss: 0.013374587520956993
Test Loss:  0.01853514090180397
Valid Loss:  0.014232365414500237
Epoch:  298  	Training Loss: 0.013372983783483505
Test Loss:  0.018534976989030838
Valid Loss:  0.014231723733246326
Epoch:  299  	Training Loss: 0.013371385633945465
Test Loss:  0.018534790724515915
Valid Loss:  0.014231073670089245
Epoch:  300  	Training Loss: 0.013369789347052574
Test Loss:  0.018534578382968903
Valid Loss:  0.014230413362383842
Epoch:  301  	Training Loss: 0.013368196785449982
Test Loss:  0.0185343436896801
Valid Loss:  0.014229741878807545
Epoch:  302  	Training Loss: 0.013366607949137688
Test Loss:  0.01853409968316555
Valid Loss:  0.014229071326553822
Epoch:  303  	Training Loss: 0.01336502842605114
Test Loss:  0.01853383332490921
Valid Loss:  0.014228393323719501
Epoch:  304  	Training Loss: 0.013363451696932316
Test Loss:  0.018533548340201378
Valid Loss:  0.014227703213691711
Epoch:  305  	Training Loss: 0.013361877761781216
Test Loss:  0.018533241003751755
Valid Loss:  0.014227006584405899
Epoch:  306  	Training Loss: 0.01336030662059784
Test Loss:  0.01853291690349579
Valid Loss:  0.01422630250453949
Epoch:  307  	Training Loss: 0.013358737342059612
Test Loss:  0.01853257790207863
Valid Loss:  0.014225592836737633
Epoch:  308  	Training Loss: 0.013357173651456833
Test Loss:  0.018532216548919678
Valid Loss:  0.01422487199306488
Epoch:  309  	Training Loss: 0.013355610892176628
Test Loss:  0.018531836569309235
Valid Loss:  0.014224180951714516
Epoch:  310  	Training Loss: 0.013354048132896423
Test Loss:  0.018531443551182747
Valid Loss:  0.014223538339138031
Epoch:  311  	Training Loss: 0.013352504000067711
Test Loss:  0.018531668931245804
Valid Loss:  0.014223121106624603
Epoch:  312  	Training Loss: 0.013350971043109894
Test Loss:  0.018531888723373413
Valid Loss:  0.014222676865756512
Epoch:  313  	Training Loss: 0.013349414803087711
Test Loss:  0.018532073125243187
Valid Loss:  0.014222224242985249
Epoch:  314  	Training Loss: 0.013347862288355827
Test Loss:  0.018532220274209976
Valid Loss:  0.014221757650375366
Epoch:  315  	Training Loss: 0.013346316292881966
Test Loss:  0.01853233575820923
Valid Loss:  0.01422128640115261
Epoch:  316  	Training Loss: 0.013344774022698402
Test Loss:  0.018532417714595795
Valid Loss:  0.01422080211341381
Epoch:  317  	Training Loss: 0.013343237340450287
Test Loss:  0.018532464280724525
Valid Loss:  0.014220308512449265
Epoch:  318  	Training Loss: 0.013341706246137619
Test Loss:  0.01853248104453087
Valid Loss:  0.014219806529581547
Epoch:  319  	Training Loss: 0.013340180739760399
Test Loss:  0.018532468006014824
Valid Loss:  0.014219296164810658
Epoch:  320  	Training Loss: 0.013338660821318626
Test Loss:  0.01853242516517639
Valid Loss:  0.014218775555491447
Epoch:  321  	Training Loss: 0.013337140902876854
Test Loss:  0.01853235438466072
Valid Loss:  0.014218250289559364
Epoch:  322  	Training Loss: 0.013335630297660828
Test Loss:  0.01853228360414505
Valid Loss:  0.014217736199498177
Epoch:  323  	Training Loss: 0.013334136456251144
Test Loss:  0.01853218860924244
Valid Loss:  0.014217213727533817
Epoch:  324  	Training Loss: 0.013332650065422058
Test Loss:  0.01853206194937229
Valid Loss:  0.014216681942343712
Epoch:  325  	Training Loss: 0.013331163674592972
Test Loss:  0.018531912937760353
Valid Loss:  0.014216143637895584
Epoch:  326  	Training Loss: 0.01332968007773161
Test Loss:  0.018531741574406624
Valid Loss:  0.014215598814189434
Epoch:  327  	Training Loss: 0.01332820300012827
Test Loss:  0.018531545996665955
Valid Loss:  0.014215046539902687
Epoch:  328  	Training Loss: 0.013326726853847504
Test Loss:  0.018531331792473793
Valid Loss:  0.014214488677680492
Epoch:  329  	Training Loss: 0.013325255364179611
Test Loss:  0.018531087785959244
Valid Loss:  0.014213920570909977
Epoch:  330  	Training Loss: 0.013323785737156868
Test Loss:  0.01853083074092865
Valid Loss:  0.014213349670171738
Epoch:  331  	Training Loss: 0.013322317972779274
Test Loss:  0.018530556932091713
Valid Loss:  0.0142127750441432
Epoch:  332  	Training Loss: 0.013320857658982277
Test Loss:  0.018530264496803284
Valid Loss:  0.014212198555469513
Epoch:  333  	Training Loss: 0.013319400139153004
Test Loss:  0.018529951572418213
Valid Loss:  0.014211613684892654
Epoch:  334  	Training Loss: 0.013317943550646305
Test Loss:  0.01852962002158165
Valid Loss:  0.014211022295057774
Epoch:  335  	Training Loss: 0.013316490687429905
Test Loss:  0.018529269844293594
Valid Loss:  0.014210427179932594
Epoch:  336  	Training Loss: 0.01331503875553608
Test Loss:  0.018528906628489494
Valid Loss:  0.014209823682904243
Epoch:  337  	Training Loss: 0.013313588686287403
Test Loss:  0.018528522923588753
Valid Loss:  0.014209218323230743
Epoch:  338  	Training Loss: 0.013312142342329025
Test Loss:  0.018528126180171967
Valid Loss:  0.014208605512976646
Epoch:  339  	Training Loss: 0.013310696929693222
Test Loss:  0.018527712672948837
Valid Loss:  0.014207988977432251
Epoch:  340  	Training Loss: 0.013309253379702568
Test Loss:  0.018527286127209663
Valid Loss:  0.014207368716597557
Epoch:  341  	Training Loss: 0.013307809829711914
Test Loss:  0.018526840955018997
Valid Loss:  0.014206744730472565
Epoch:  342  	Training Loss: 0.013306370936334133
Test Loss:  0.01852637715637684
Valid Loss:  0.014206107705831528
Epoch:  343  	Training Loss: 0.013304929248988628
Test Loss:  0.018525898456573486
Valid Loss:  0.014205469749867916
Epoch:  344  	Training Loss: 0.013303487561643124
Test Loss:  0.018525410443544388
Valid Loss:  0.014204824343323708
Epoch:  345  	Training Loss: 0.013302047736942768
Test Loss:  0.018524907529354095
Valid Loss:  0.014204179868102074
Epoch:  346  	Training Loss: 0.013300610706210136
Test Loss:  0.01852439157664776
Valid Loss:  0.014203531667590141
Epoch:  347  	Training Loss: 0.013299175538122654
Test Loss:  0.018523868173360825
Valid Loss:  0.014202877879142761
Epoch:  348  	Training Loss: 0.013297740370035172
Test Loss:  0.018523331731557846
Valid Loss:  0.014202222228050232
Epoch:  349  	Training Loss: 0.013296308927237988
Test Loss:  0.018522784113883972
Valid Loss:  0.014201560989022255
Epoch:  350  	Training Loss: 0.013294877484440804
Test Loss:  0.018522225320339203
Valid Loss:  0.014200898818671703
Epoch:  351  	Training Loss: 0.013293448835611343
Test Loss:  0.018521659076213837
Valid Loss:  0.014200231991708279
Epoch:  352  	Training Loss: 0.013292020186781883
Test Loss:  0.018521087244153023
Valid Loss:  0.01419956423342228
Epoch:  353  	Training Loss: 0.013290595263242722
Test Loss:  0.018520507961511612
Valid Loss:   71%|███████   | 353/500 [02:08<01:03,  2.31it/s] 71%|███████   | 355/500 [02:08<00:46,  3.10it/s] 71%|███████▏  | 357/500 [02:08<00:34,  4.09it/s] 72%|███████▏  | 359/500 [02:08<00:26,  5.25it/s] 72%|███████▏  | 361/500 [02:11<01:22,  1.69it/s] 73%|███████▎  | 363/500 [02:12<00:59,  2.30it/s] 73%|███████▎  | 365/500 [02:12<00:43,  3.10it/s] 73%|███████▎  | 367/500 [02:12<00:32,  4.08it/s] 74%|███████▍  | 369/500 [02:12<00:24,  5.25it/s] 74%|███████▍  | 371/500 [02:15<01:16,  1.68it/s] 75%|███████▍  | 373/500 [02:15<00:55,  2.29it/s] 75%|███████▌  | 375/500 [02:15<00:40,  3.07it/s] 75%|███████▌  | 377/500 [02:15<00:30,  4.04it/s] 76%|███████▌  | 379/500 [02:16<00:23,  5.19it/s] 76%|███████▌  | 381/500 [02:19<01:10,  1.68it/s] 77%|███████▋  | 383/500 [02:19<00:50,  2.30it/s] 77%|███████▋  | 385/500 [02:19<00:37,  3.09it/s] 77%|███████▋  | 387/500 [02:19<00:27,  4.07it/s] 78%|███████▊  | 389/500 [02:19<00:21,  5.24it/s] 78%|███████▊  | 391/500 [02:22<01:04,  1.70it/s] 79%|███████▊  | 393/500 [02:22<00:46,  2.30it/s] 79%|███████▉  | 395/500 [02:22<00:33,  3.09it/s] 79%|███████▉  | 397/500 [02:23<00:25,  4.07it/s] 80%|███████▉  | 399/500 [02:23<00:19,  5.23it/s] 80%|████████  | 401/500 [02:26<00:58,  1.69it/s] 81%|████████  | 403/500 [02:26<00:41,  2.31it/s] 81%|████████  | 405/500 [02:26<00:30,  3.11it/s] 81%|████████▏ | 407/500 [02:26<00:22,  4.10it/s] 82%|████████▏ | 409/500 [02:26<00:17,  5.27it/s] 82%|████████▏ | 411/500 [02:29<00:52,  1.70it/s] 83%|████████▎ | 413/500 [02:29<00:37,  2.32it/s] 83%|████████▎ | 415/500 [02:30<00:27,  3.11it/s] 83%|████████▎ | 417/500 [02:30<00:20,  4.09it/s] 84%|████████▍ | 419/500 [02:30<00:15,  5.24it/s] 84%|████████▍ | 421/500 [02:33<00:46,  1.71it/s] 85%|████████▍ | 423/500 [02:33<00:33,  2.33it/s]0.014198891818523407
Epoch:  354  	Training Loss: 0.013289172202348709
Test Loss:  0.018519917502999306
Valid Loss:  0.014198219403624535
Epoch:  355  	Training Loss: 0.013287750072777271
Test Loss:  0.018519317731261253
Valid Loss:  0.014197544194757938
Epoch:  356  	Training Loss: 0.013286328874528408
Test Loss:  0.018518714234232903
Valid Loss:  0.014196865260601044
Epoch:  357  	Training Loss: 0.013284923508763313
Test Loss:  0.01851872354745865
Valid Loss:  0.014196421951055527
Epoch:  358  	Training Loss: 0.013283539563417435
Test Loss:  0.018518781289458275
Valid Loss:  0.014195995405316353
Epoch:  359  	Training Loss: 0.013282161206007004
Test Loss:  0.018518805503845215
Valid Loss:  0.014195555821061134
Epoch:  360  	Training Loss: 0.013280786573886871
Test Loss:  0.018518801778554916
Valid Loss:  0.014195111580193043
Epoch:  361  	Training Loss: 0.013279415667057037
Test Loss:  0.018518773838877678
Valid Loss:  0.014194659888744354
Epoch:  362  	Training Loss: 0.013278050348162651
Test Loss:  0.018518686294555664
Valid Loss:  0.014194175601005554
Epoch:  363  	Training Loss: 0.013276676647365093
Test Loss:  0.01851857826113701
Valid Loss:  0.014193684794008732
Epoch:  364  	Training Loss: 0.013275306671857834
Test Loss:  0.018518442288041115
Valid Loss:  0.014193187467753887
Epoch:  365  	Training Loss: 0.013273940421640873
Test Loss:  0.01851828396320343
Valid Loss:  0.014192679896950722
Epoch:  366  	Training Loss: 0.013272576034069061
Test Loss:  0.018518101423978806
Valid Loss:  0.01419216301292181
Epoch:  367  	Training Loss: 0.013271212577819824
Test Loss:  0.01851789653301239
Valid Loss:  0.0141916424036026
Epoch:  368  	Training Loss: 0.01326985377818346
Test Loss:  0.018517669290304184
Valid Loss:  0.014191113412380219
Epoch:  369  	Training Loss: 0.013268498703837395
Test Loss:  0.018517427146434784
Valid Loss:  0.014190580695867538
Epoch:  370  	Training Loss: 0.013267146423459053
Test Loss:  0.01851716637611389
Valid Loss:  0.014190040528774261
Epoch:  371  	Training Loss: 0.013265795074403286
Test Loss:  0.01851688325405121
Valid Loss:  0.014189491979777813
Epoch:  372  	Training Loss: 0.013264447450637817
Test Loss:  0.018516601994633675
Valid Loss:  0.014188956469297409
Epoch:  373  	Training Loss: 0.01326310820877552
Test Loss:  0.018516309559345245
Valid Loss:  0.014188416302204132
Epoch:  374  	Training Loss: 0.013261772692203522
Test Loss:  0.018515996634960175
Valid Loss:  0.014187870547175407
Epoch:  375  	Training Loss: 0.013260439038276672
Test Loss:  0.01851566508412361
Valid Loss:  0.014187316410243511
Epoch:  376  	Training Loss: 0.013259105384349823
Test Loss:  0.018515314906835556
Valid Loss:  0.01418676134198904
Epoch:  377  	Training Loss: 0.013257775455713272
Test Loss:  0.018514953553676605
Valid Loss:  0.014186196960508823
Epoch:  378  	Training Loss: 0.013256446458399296
Test Loss:  0.01851457543671131
Valid Loss:  0.014185627922415733
Epoch:  379  	Training Loss: 0.013255121186375618
Test Loss:  0.018514182418584824
Valid Loss:  0.014185057953000069
Epoch:  380  	Training Loss: 0.013253796845674515
Test Loss:  0.01851377636194229
Valid Loss:  0.014184480533003807
Epoch:  381  	Training Loss: 0.013252472504973412
Test Loss:  0.018513353541493416
Valid Loss:  0.014183899387717247
Epoch:  382  	Training Loss: 0.013251151889562607
Test Loss:  0.018512915819883347
Valid Loss:  0.014183307066559792
Epoch:  383  	Training Loss: 0.013249830342829227
Test Loss:  0.01851246878504753
Valid Loss:  0.014182711951434612
Epoch:  384  	Training Loss: 0.013248510658740997
Test Loss:  0.018512006849050522
Valid Loss:  0.014182109385728836
Epoch:  385  	Training Loss: 0.013247190974652767
Test Loss:  0.01851153001189232
Valid Loss:  0.014181508682668209
Epoch:  386  	Training Loss: 0.013245875015854836
Test Loss:  0.018511047586798668
Valid Loss:  0.014180903322994709
Epoch:  387  	Training Loss: 0.01324455812573433
Test Loss:  0.018510546535253525
Valid Loss:  0.014180292375385761
Epoch:  388  	Training Loss: 0.013243244960904121
Test Loss:  0.018510043621063232
Valid Loss:  0.01417967677116394
Epoch:  389  	Training Loss: 0.013241930864751339
Test Loss:  0.018509531393647194
Valid Loss:  0.014179063960909843
Epoch:  390  	Training Loss: 0.013240623287856579
Test Loss:  0.018509002402424812
Valid Loss:  0.014178442768752575
Epoch:  391  	Training Loss: 0.013239312916994095
Test Loss:  0.018508465960621834
Valid Loss:  0.014177817851305008
Epoch:  392  	Training Loss: 0.013238005340099335
Test Loss:  0.01850791648030281
Valid Loss:  0.014177192002534866
Epoch:  393  	Training Loss: 0.013236697763204575
Test Loss:  0.01850735768675804
Valid Loss:  0.014176563359797001
Epoch:  394  	Training Loss: 0.01323539949953556
Test Loss:  0.018507398664951324
Valid Loss:  0.01417616382241249
Epoch:  395  	Training Loss: 0.013234119862318039
Test Loss:  0.01850747875869274
Valid Loss:  0.014175785705447197
Epoch:  396  	Training Loss: 0.013232845813035965
Test Loss:  0.01850753277540207
Valid Loss:  0.014175397343933582
Epoch:  397  	Training Loss: 0.01323157548904419
Test Loss:  0.01850755885243416
Valid Loss:  0.014174998737871647
Epoch:  398  	Training Loss: 0.013230308890342712
Test Loss:  0.01850755512714386
Valid Loss:  0.01417459361255169
Epoch:  399  	Training Loss: 0.013229047879576683
Test Loss:  0.018507525324821472
Valid Loss:  0.01417417824268341
Epoch:  400  	Training Loss: 0.013227787800133228
Test Loss:  0.018507473170757294
Valid Loss:  0.014173752628266811
Epoch:  401  	Training Loss: 0.013226531445980072
Test Loss:  0.018507391214370728
Valid Loss:  0.014173313975334167
Epoch:  402  	Training Loss: 0.013225275091826916
Test Loss:  0.018507296219468117
Valid Loss:  0.014172879047691822
Epoch:  403  	Training Loss: 0.013224028050899506
Test Loss:  0.018507175147533417
Valid Loss:  0.014172429218888283
Epoch:  404  	Training Loss: 0.013222779147326946
Test Loss:  0.018507029861211777
Valid Loss:  0.014171972870826721
Epoch:  405  	Training Loss: 0.013221535831689835
Test Loss:  0.018506865948438644
Valid Loss:  0.014171507209539413
Epoch:  406  	Training Loss: 0.013220294378697872
Test Loss:  0.01850667968392372
Valid Loss:  0.014171039685606956
Epoch:  407  	Training Loss: 0.013219054788351059
Test Loss:  0.018506474792957306
Valid Loss:  0.014170562848448753
Epoch:  408  	Training Loss: 0.013217817060649395
Test Loss:  0.01850624941289425
Valid Loss:  0.014170074835419655
Epoch:  409  	Training Loss: 0.013216588646173477
Test Loss:  0.018507227301597595
Valid Loss:  0.014170079492032528
Epoch:  410  	Training Loss: 0.013215389102697372
Test Loss:  0.01850706711411476
Valid Loss:  0.01416962593793869
Epoch:  411  	Training Loss: 0.013214162550866604
Test Loss:  0.018506739288568497
Valid Loss:  0.014169102534651756
Epoch:  412  	Training Loss: 0.013212938793003559
Test Loss:  0.018507564440369606
Valid Loss:  0.01416902244091034
Epoch:  413  	Training Loss: 0.013211708515882492
Test Loss:  0.01850726827979088
Valid Loss:  0.01416850183159113
Epoch:  414  	Training Loss: 0.013210460543632507
Test Loss:  0.018506810069084167
Valid Loss:  0.014167911373078823
Epoch:  415  	Training Loss: 0.013209234923124313
Test Loss:  0.0185076966881752
Valid Loss:  0.014167866669595242
Epoch:  416  	Training Loss: 0.01320800743997097
Test Loss:  0.018507175147533417
Valid Loss:  0.014167255721986294
Epoch:  417  	Training Loss: 0.013206773437559605
Test Loss:  0.018507853150367737
Valid Loss:  0.01416712999343872
Epoch:  418  	Training Loss: 0.01320556178689003
Test Loss:  0.01850741356611252
Valid Loss:  0.014166552573442459
Epoch:  419  	Training Loss: 0.013204334303736687
Test Loss:  0.018508030101656914
Valid Loss:  0.01416640542447567
Epoch:  420  	Training Loss: 0.013203130103647709
Test Loss:  0.01850752905011177
Valid Loss:  0.014165806584060192
Epoch:  421  	Training Loss: 0.013201912865042686
Test Loss:  0.01850808411836624
Valid Loss:  0.014165636152029037
Epoch:  422  	Training Loss: 0.01320070680230856
Test Loss:  0.018507562577724457
Valid Loss:  0.014165036380290985
Epoch:  423  	Training Loss: 0.013199519366025925
Test Loss:  0.01850825734436512
Valid Loss:  0.0141649404540658
 85%|████████▌ | 425/500 [02:33<00:23,  3.13it/s] 85%|████████▌ | 427/500 [02:33<00:17,  4.11it/s] 86%|████████▌ | 429/500 [02:33<00:13,  5.28it/s] 86%|████████▌ | 431/500 [02:36<00:40,  1.69it/s] 87%|████████▋ | 433/500 [02:37<00:29,  2.30it/s] 87%|████████▋ | 435/500 [02:37<00:21,  3.09it/s] 87%|████████▋ | 437/500 [02:37<00:15,  4.07it/s] 88%|████████▊ | 439/500 [02:37<00:11,  5.22it/s] 88%|████████▊ | 441/500 [02:40<00:34,  1.70it/s] 89%|████████▊ | 443/500 [02:40<00:24,  2.32it/s] 89%|████████▉ | 445/500 [02:40<00:17,  3.12it/s] 89%|████████▉ | 447/500 [02:40<00:12,  4.11it/s] 90%|████████▉ | 449/500 [02:40<00:09,  5.29it/s] 90%|█████████ | 451/500 [02:44<00:29,  1.68it/s] 91%|█████████ | 453/500 [02:44<00:20,  2.29it/s] 91%|█████████ | 455/500 [02:44<00:14,  3.08it/s] 91%|█████████▏| 457/500 [02:44<00:10,  4.06it/s] 92%|█████████▏| 459/500 [02:44<00:07,  5.22it/s] 92%|█████████▏| 461/500 [02:47<00:22,  1.70it/s] 93%|█████████▎| 463/500 [02:47<00:15,  2.32it/s] 93%|█████████▎| 465/500 [02:47<00:11,  3.12it/s] 93%|█████████▎| 467/500 [02:47<00:08,  4.10it/s] 94%|█████████▍| 469/500 [02:48<00:05,  5.28it/s] 94%|█████████▍| 471/500 [02:51<00:17,  1.70it/s] 95%|█████████▍| 473/500 [02:51<00:11,  2.31it/s] 95%|█████████▌| 475/500 [02:51<00:08,  3.11it/s] 95%|█████████▌| 477/500 [02:51<00:05,  4.09it/s] 96%|█████████▌| 479/500 [02:51<00:03,  5.26it/s] 96%|█████████▌| 481/500 [02:54<00:11,  1.70it/s] 97%|█████████▋| 483/500 [02:54<00:07,  2.32it/s] 97%|█████████▋| 485/500 [02:54<00:04,  3.12it/s] 97%|█████████▋| 487/500 [02:55<00:03,  4.11it/s] 98%|█████████▊| 489/500 [02:55<00:02,  5.28it/s] 98%|█████████▊| 491/500 [02:58<00:05,  1.68it/s] 99%|█████████▊| 493/500 [02:58<00:03,  2.29it/s]Epoch:  424  	Training Loss: 0.013198330998420715
Test Loss:  0.018507536500692368
Valid Loss:  0.01416426245123148
Epoch:  425  	Training Loss: 0.01319716963917017
Test Loss:  0.0185081847012043
Valid Loss:  0.01416415348649025
Epoch:  426  	Training Loss: 0.013195988722145557
Test Loss:  0.018508782610297203
Valid Loss:  0.014164024963974953
Epoch:  427  	Training Loss: 0.013194821774959564
Test Loss:  0.01850796863436699
Valid Loss:  0.014163307845592499
Epoch:  428  	Training Loss: 0.013193665072321892
Test Loss:  0.01850851997733116
Valid Loss:  0.014163162559270859
Epoch:  429  	Training Loss: 0.013192497193813324
Test Loss:  0.01850903034210205
Valid Loss:  0.01416300144046545
Epoch:  430  	Training Loss: 0.01319133397191763
Test Loss:  0.018509354442358017
Valid Loss:  0.014162767678499222
Epoch:  431  	Training Loss: 0.013190191239118576
Test Loss:  0.018508560955524445
Valid Loss:  0.014162060804665089
Epoch:  432  	Training Loss: 0.013189038261771202
Test Loss:  0.018509013578295708
Valid Loss:  0.014161886647343636
Epoch:  433  	Training Loss: 0.01318790577352047
Test Loss:  0.01850942149758339
Valid Loss:  0.014161697588860989
Epoch:  434  	Training Loss: 0.013186774216592312
Test Loss:  0.018509799614548683
Valid Loss:  0.014161499217152596
Epoch:  435  	Training Loss: 0.013185654766857624
Test Loss:  0.01850999891757965
Valid Loss:  0.014161227270960808
Epoch:  436  	Training Loss: 0.01318453811109066
Test Loss:  0.018510306254029274
Valid Loss:  0.01416100189089775
Epoch:  437  	Training Loss: 0.013183433562517166
Test Loss:  0.018509354442358017
Valid Loss:  0.01416025124490261
Epoch:  438  	Training Loss: 0.013182323426008224
Test Loss:  0.0185096375644207
Valid Loss:  0.014160018414258957
Epoch:  439  	Training Loss: 0.013181215152144432
Test Loss:  0.0185098834335804
Valid Loss:  0.014159774407744408
Epoch:  440  	Training Loss: 0.013180112466216087
Test Loss:  0.01851009950041771
Valid Loss:  0.014159515500068665
Epoch:  441  	Training Loss: 0.01317901536822319
Test Loss:  0.018510285764932632
Valid Loss:  0.014159247279167175
Epoch:  442  	Training Loss: 0.013177918270230293
Test Loss:  0.018510418012738228
Valid Loss:  0.014158945530653
Epoch:  443  	Training Loss: 0.013176806271076202
Test Loss:  0.018510518595576286
Valid Loss:  0.014158632606267929
Epoch:  444  	Training Loss: 0.013175698928534985
Test Loss:  0.018510591238737106
Valid Loss:  0.014158310368657112
Epoch:  445  	Training Loss: 0.013174591585993767
Test Loss:  0.018510635942220688
Valid Loss:  0.01415797509253025
Epoch:  446  	Training Loss: 0.013173489831387997
Test Loss:  0.018510660156607628
Valid Loss:  0.014157633297145367
Epoch:  447  	Training Loss: 0.01317239087074995
Test Loss:  0.01851065456867218
Valid Loss:  0.014157280325889587
Epoch:  448  	Training Loss: 0.013171296566724777
Test Loss:  0.018510624766349792
Valid Loss:  0.014156917110085487
Epoch:  449  	Training Loss: 0.013170203194022179
Test Loss:  0.018510568886995316
Valid Loss:  0.01415654644370079
Epoch:  450  	Training Loss: 0.01316911168396473
Test Loss:  0.018510494381189346
Valid Loss:  0.014156190678477287
Epoch:  451  	Training Loss: 0.013168023899197578
Test Loss:  0.01851039007306099
Valid Loss:  0.014155833050608635
Epoch:  452  	Training Loss: 0.013166937977075577
Test Loss:  0.018510278314352036
Valid Loss:  0.01415548287332058
Epoch:  453  	Training Loss: 0.013165855780243874
Test Loss:  0.01851014420390129
Valid Loss:  0.014155123382806778
Epoch:  454  	Training Loss: 0.013164777308702469
Test Loss:  0.018509991466999054
Valid Loss:  0.014154765754938126
Epoch:  455  	Training Loss: 0.013163700699806213
Test Loss:  0.018509820103645325
Valid Loss:  0.014154396951198578
Epoch:  456  	Training Loss: 0.013162625953555107
Test Loss:  0.018509626388549805
Valid Loss:  0.014154027216136456
Epoch:  457  	Training Loss: 0.013161554001271725
Test Loss:  0.018509414047002792
Valid Loss:  0.014153651893138885
Epoch:  458  	Training Loss: 0.013160482048988342
Test Loss:  0.018509188666939735
Valid Loss:  0.014153270982205868
Epoch:  459  	Training Loss: 0.013159412890672684
Test Loss:  0.018508942797780037
Valid Loss:  0.014152886345982552
Epoch:  460  	Training Loss: 0.013158346526324749
Test Loss:  0.018508680164813995
Valid Loss:  0.014152498915791512
Epoch:  461  	Training Loss: 0.013157282024621964
Test Loss:  0.01850840263068676
Valid Loss:  0.014152107760310173
Epoch:  462  	Training Loss: 0.013156220316886902
Test Loss:  0.018508102744817734
Valid Loss:  0.01415170542895794
Epoch:  463  	Training Loss: 0.01315515860915184
Test Loss:  0.018507789820432663
Valid Loss:  0.014151296578347683
Epoch:  464  	Training Loss: 0.013154099695384502
Test Loss:  0.0185074582695961
Valid Loss:  0.014150887727737427
Epoch:  465  	Training Loss: 0.013153040781617165
Test Loss:  0.01850711554288864
Valid Loss:  0.014150476083159447
Epoch:  466  	Training Loss: 0.013151983730494976
Test Loss:  0.01850675791501999
Valid Loss:  0.014150057919323444
Epoch:  467  	Training Loss: 0.013150927610695362
Test Loss:  0.018506385385990143
Valid Loss:  0.014149637892842293
Epoch:  468  	Training Loss: 0.013149874284863472
Test Loss:  0.0185060016810894
Valid Loss:  0.014149213209748268
Epoch:  469  	Training Loss: 0.013148820027709007
Test Loss:  0.018505604937672615
Valid Loss:  0.014148784801363945
Epoch:  470  	Training Loss: 0.013147767633199692
Test Loss:  0.018505197018384933
Valid Loss:  0.014148354530334473
Epoch:  471  	Training Loss: 0.013146728277206421
Test Loss:  0.018505388870835304
Valid Loss:  0.014148104935884476
Epoch:  472  	Training Loss: 0.013145692646503448
Test Loss:  0.018505647778511047
Valid Loss:  0.01414788980036974
Epoch:  473  	Training Loss: 0.013144680298864841
Test Loss:  0.018505871295928955
Valid Loss:  0.014147668145596981
Epoch:  474  	Training Loss: 0.013143670745193958
Test Loss:  0.018506065011024475
Valid Loss:  0.014147437177598476
Epoch:  475  	Training Loss: 0.013142663985490799
Test Loss:  0.018506228923797607
Valid Loss:  0.0141471978276968
Epoch:  476  	Training Loss: 0.013141663745045662
Test Loss:  0.0185063686221838
Valid Loss:  0.014146951958537102
Epoch:  477  	Training Loss: 0.013140665367245674
Test Loss:  0.018506478518247604
Valid Loss:  0.014146700501441956
Epoch:  478  	Training Loss: 0.013139670714735985
Test Loss:  0.01850656047463417
Valid Loss:  0.014146439731121063
Epoch:  479  	Training Loss: 0.013138677924871445
Test Loss:  0.018506620079278946
Valid Loss:  0.014146174304187298
Epoch:  480  	Training Loss: 0.013137686997652054
Test Loss:  0.018506651744246483
Valid Loss:  0.01414590235799551
Epoch:  481  	Training Loss: 0.013136700727045536
Test Loss:  0.01850666105747223
Valid Loss:  0.014145621098577976
Epoch:  482  	Training Loss: 0.013135716319084167
Test Loss:  0.018506620079278946
Valid Loss:  0.014145316556096077
Epoch:  483  	Training Loss: 0.01313471607863903
Test Loss:  0.01850656233727932
Valid Loss:  0.01414500456303358
Epoch:  484  	Training Loss: 0.013133719563484192
Test Loss:  0.018506476655602455
Valid Loss:  0.014144686982035637
Epoch:  485  	Training Loss: 0.013132724910974503
Test Loss:  0.018506374210119247
Valid Loss:  0.014144363813102245
Epoch:  486  	Training Loss: 0.013131731189787388
Test Loss:  0.0185062475502491
Valid Loss:  0.014144035056233406
Epoch:  487  	Training Loss: 0.013130741193890572
Test Loss:  0.018506105989217758
Valid Loss:  0.014143699780106544
Epoch:  488  	Training Loss: 0.013129751197993755
Test Loss:  0.018505942076444626
Valid Loss:  0.014143360778689384
Epoch:  489  	Training Loss: 0.013128764927387238
Test Loss:  0.01850575953722
Valid Loss:  0.014143015258014202
Epoch:  490  	Training Loss: 0.013127779588103294
Test Loss:  0.018505558371543884
Valid Loss:  0.014142668806016445
Epoch:  491  	Training Loss: 0.013126797042787075
Test Loss:  0.018505342304706573
Valid Loss:  0.014142313972115517
Epoch:  492  	Training Loss: 0.013125813566148281
Test Loss:  0.018505066633224487
Valid Loss:  0.014141915366053581
Epoch:  493  	Training Loss: 0.013124803081154823
Test Loss:  0.018504774197936058
Valid Loss:  0.014141511172056198
Epoch:  494  	Training Loss: 0.01312379539012909
Test Loss:   99%|█████████▉| 495/500 [02:58<00:01,  3.07it/s] 99%|█████████▉| 497/500 [02:58<00:00,  4.05it/s]100%|█████████▉| 499/500 [02:58<00:00,  5.20it/s]100%|██████████| 500/500 [02:58<00:00,  2.80it/s]
0.018504470586776733
Valid Loss:  0.014141105115413666
Epoch:  495  	Training Loss: 0.013122787699103355
Test Loss:  0.018504148349165916
Valid Loss:  0.014140690676867962
Epoch:  496  	Training Loss: 0.01312178373336792
Test Loss:  0.018503813073039055
Valid Loss:  0.014140277169644833
Epoch:  497  	Training Loss: 0.01312077697366476
Test Loss:  0.01850346103310585
Valid Loss:  0.014139857143163681
Epoch:  498  	Training Loss: 0.0131197739392519
Test Loss:  0.0185030959546566
Valid Loss:  0.014139431528747082
Epoch:  499  	Training Loss: 0.013118771836161613
Test Loss:  0.018502715975046158
Valid Loss:  0.014139002189040184
Epoch:  500  	Training Loss: 0.013117771595716476
Test Loss:  0.01850232295691967
Valid Loss:  0.014138570055365562
seed is  2
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:02<24:40,  2.97s/it]  1%|          | 3/500 [00:03<06:45,  1.22it/s]  1%|          | 5/500 [00:03<03:32,  2.33it/s]  1%|▏         | 7/500 [00:03<02:15,  3.64it/s]  2%|▏         | 9/500 [00:03<01:35,  5.13it/s]  2%|▏         | 11/500 [00:06<05:26,  1.50it/s]  3%|▎         | 13/500 [00:06<03:48,  2.13it/s]  3%|▎         | 15/500 [00:09<06:31,  1.24it/s]  3%|▎         | 17/500 [00:09<04:36,  1.74it/s]  4%|▍         | 19/500 [00:10<03:19,  2.41it/s]  4%|▍         | 21/500 [00:15<09:37,  1.21s/it]  5%|▍         | 23/500 [00:16<06:49,  1.17it/s]  5%|▌         | 25/500 [00:19<08:26,  1.07s/it]  5%|▌         | 27/500 [00:19<06:01,  1.31it/s]  6%|▌         | 29/500 [00:19<04:20,  1.81it/s]  6%|▌         | 31/500 [00:25<10:04,  1.29s/it]  7%|▋         | 33/500 [00:25<07:10,  1.09it/s]  7%|▋         | 35/500 [00:28<08:29,  1.10s/it]  7%|▋         | 37/500 [00:28<06:03,  1.27it/s]  8%|▊         | 39/500 [00:28<04:22,  1.76it/s]  8%|▊         | 41/500 [00:31<06:35,  1.16it/s]  9%|▊         | 43/500 [00:32<04:45,  1.60it/s]  9%|▉         | 45/500 [00:35<06:47,  1.12it/s]  9%|▉         | 47/500 [00:35<04:53,  1.55it/s] 10%|▉         | 49/500 [00:35<03:32,  2.12it/s] 10%|█         | 51/500 [00:41<09:07,  1.22s/it] 11%|█         | 53/500 [00:41<06:30,  1.15it/s] 11%|█         | 55/500 [00:44<07:54,  1.07s/it] 11%|█▏        | 57/500 [00:44<05:39,  1.30it/s] 12%|█▏        | 59/500 [00:44<04:05,  1.80it/s] 12%|█▏        | 61/500 [00:50<09:22,  1.28s/it]Epoch:  1  	Training Loss: 0.010152710601687431
Test Loss:  0.01992100477218628
Valid Loss:  0.03350752592086792
Epoch:  2  	Training Loss: 0.018038678914308548
Test Loss:  0.6501497030258179
Valid Loss:  0.5861937999725342
Epoch:  3  	Training Loss: 0.5241788029670715
Test Loss:  16.790924072265625
Valid Loss:  16.211349487304688
Epoch:  4  	Training Loss: 17.707992553710938
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  5  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  6  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  7  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  8  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  9  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  10  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  11  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  12  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  13  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  14  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  15  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  16  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  17  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  18  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  19  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  20  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  21  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  22  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  23  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  24  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  25  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  26  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  27  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  28  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  29  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  30  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  31  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  32  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  33  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  34  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  35  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  36  	Training Loss: 0.010569371283054352
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  37  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  38  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  39  	Training Loss: 0.010569371283054352
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  40  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  41  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  42  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  43  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  44  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  45  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  46  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  47  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  48  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  49  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  50  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  51  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  52  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  53  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  54  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  55  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  56  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  57  	Training Loss: 0.010569371283054352
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  58  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  59  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  60  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  61  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  62  	Training Loss: 0.010569370351731777
 13%|█▎        | 63/500 [00:50<06:41,  1.09it/s] 13%|█▎        | 65/500 [00:53<07:57,  1.10s/it] 13%|█▎        | 67/500 [00:53<05:40,  1.27it/s] 14%|█▍        | 69/500 [00:54<04:05,  1.75it/s] 14%|█▍        | 71/500 [01:00<09:13,  1.29s/it] 15%|█▍        | 73/500 [01:00<06:33,  1.08it/s] 15%|█▌        | 75/500 [01:03<07:49,  1.11s/it] 15%|█▌        | 77/500 [01:03<05:35,  1.26it/s] 16%|█▌        | 79/500 [01:03<04:02,  1.74it/s] 16%|█▌        | 81/500 [01:09<09:03,  1.30s/it] 17%|█▋        | 83/500 [01:09<06:26,  1.08it/s] 17%|█▋        | 85/500 [01:12<07:37,  1.10s/it] 17%|█▋        | 87/500 [01:12<05:26,  1.26it/s] 18%|█▊        | 89/500 [01:12<03:55,  1.75it/s] 18%|█▊        | 91/500 [01:18<08:50,  1.30s/it] 19%|█▊        | 93/500 [01:18<06:17,  1.08it/s] 19%|█▉        | 95/500 [01:22<07:27,  1.11s/it] 19%|█▉        | 97/500 [01:22<05:20,  1.26it/s] 20%|█▉        | 99/500 [01:22<03:50,  1.74it/s] 20%|██        | 101/500 [01:28<08:34,  1.29s/it] 21%|██        | 103/500 [01:28<06:06,  1.08it/s] 21%|██        | 105/500 [01:31<07:14,  1.10s/it] 21%|██▏       | 107/500 [01:31<05:10,  1.27it/s] 22%|██▏       | 109/500 [01:31<03:43,  1.75it/s] 22%|██▏       | 111/500 [01:37<08:23,  1.29s/it] 23%|██▎       | 113/500 [01:37<05:58,  1.08it/s] 23%|██▎       | 115/500 [01:40<07:04,  1.10s/it] 23%|██▎       | 117/500 [01:40<05:03,  1.26it/s] 24%|██▍       | 119/500 [01:41<03:39,  1.74it/s]Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  63  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  64  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  65  	Training Loss: 0.010569371283054352
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  66  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  67  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  68  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  69  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  70  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  71  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  72  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  73  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  74  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  75  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  76  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  77  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  78  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  79  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  80  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  81  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  82  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  83  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  84  	Training Loss: 0.010569371283054352
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  85  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  86  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  87  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  88  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  89  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  90  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  91  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  92  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  93  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  94  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  95  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  96  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  97  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  98  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  99  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  100  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  101  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  102  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  103  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  104  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  105  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  106  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  107  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  108  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  109  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  110  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  111  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  112  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  113  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  114  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  115  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  116  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  117  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  118  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  119  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  120  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
 24%|██▍       | 121/500 [01:47<08:14,  1.30s/it] 25%|██▍       | 123/500 [01:47<05:51,  1.07it/s] 25%|██▌       | 125/500 [01:50<06:57,  1.11s/it] 25%|██▌       | 127/500 [01:50<04:57,  1.25it/s] 26%|██▌       | 129/500 [01:50<03:34,  1.73it/s] 26%|██▌       | 131/500 [01:56<08:02,  1.31s/it] 27%|██▋       | 133/500 [01:56<05:43,  1.07it/s] 27%|██▋       | 135/500 [01:59<06:49,  1.12s/it] 27%|██▋       | 137/500 [01:59<04:52,  1.24it/s] 28%|██▊       | 139/500 [02:00<03:30,  1.72it/s] 28%|██▊       | 141/500 [02:06<07:49,  1.31s/it] 29%|██▊       | 143/500 [02:06<05:33,  1.07it/s] 29%|██▉       | 145/500 [02:09<06:37,  1.12s/it] 29%|██▉       | 147/500 [02:09<04:43,  1.25it/s] 30%|██▉       | 149/500 [02:09<03:23,  1.72it/s] 30%|███       | 151/500 [02:15<07:35,  1.31s/it] 31%|███       | 153/500 [02:15<05:23,  1.07it/s] 31%|███       | 155/500 [02:18<06:23,  1.11s/it] 31%|███▏      | 157/500 [02:18<04:34,  1.25it/s] 32%|███▏      | 159/500 [02:18<03:17,  1.73it/s] 32%|███▏      | 161/500 [02:24<07:22,  1.30s/it] 33%|███▎      | 163/500 [02:25<05:14,  1.07it/s] 33%|███▎      | 165/500 [02:28<06:11,  1.11s/it] 33%|███▎      | 167/500 [02:28<04:24,  1.26it/s] 34%|███▍      | 169/500 [02:28<03:10,  1.74it/s] 34%|███▍      | 171/500 [02:34<07:11,  1.31s/it] 35%|███▍      | 173/500 [02:34<05:06,  1.07it/s] 35%|███▌      | 175/500 [02:37<06:03,  1.12s/it] 35%|███▌      | 177/500 [02:37<04:19,  1.25it/s]**************************************************learning rate decay**************************************************
Epoch:  121  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  122  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  123  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  124  	Training Loss: 0.010569371283054352
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  125  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  126  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  127  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  128  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  129  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  130  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  131  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  132  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  133  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  134  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  135  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  136  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  137  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  138  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  139  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  140  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  141  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  142  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  143  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  144  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  145  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  146  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  147  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  148  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  149  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  150  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  151  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  152  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  153  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  154  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  155  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  156  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  157  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  158  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  159  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  160  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  161  	Training Loss: 0.010569371283054352
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  162  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  163  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  164  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  165  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  166  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  167  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  168  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  169  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  170  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  171  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  172  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  173  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  174  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  175  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  176  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  177  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  178  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
 36%|███▌      | 179/500 [02:37<03:06,  1.72it/s] 36%|███▌      | 181/500 [02:43<06:55,  1.30s/it] 37%|███▋      | 183/500 [02:44<04:55,  1.07it/s] 37%|███▋      | 185/500 [02:47<05:48,  1.11s/it] 37%|███▋      | 187/500 [02:47<04:08,  1.26it/s] 38%|███▊      | 189/500 [02:47<02:58,  1.74it/s] 38%|███▊      | 191/500 [02:53<06:40,  1.29s/it] 39%|███▊      | 193/500 [02:53<04:44,  1.08it/s] 39%|███▉      | 195/500 [02:56<05:36,  1.10s/it] 39%|███▉      | 197/500 [02:56<04:00,  1.26it/s] 40%|███▉      | 199/500 [02:56<02:52,  1.74it/s] 40%|████      | 201/500 [03:02<06:28,  1.30s/it] 41%|████      | 203/500 [03:02<04:36,  1.07it/s] 41%|████      | 205/500 [03:05<05:27,  1.11s/it] 41%|████▏     | 207/500 [03:05<03:52,  1.26it/s] 42%|████▏     | 209/500 [03:06<02:47,  1.74it/s] 42%|████▏     | 211/500 [03:12<06:16,  1.30s/it] 43%|████▎     | 213/500 [03:12<04:27,  1.07it/s] 43%|████▎     | 215/500 [03:15<05:16,  1.11s/it] 43%|████▎     | 217/500 [03:15<03:46,  1.25it/s] 44%|████▍     | 219/500 [03:15<02:43,  1.72it/s] 44%|████▍     | 221/500 [03:21<06:03,  1.30s/it] 45%|████▍     | 223/500 [03:21<04:18,  1.07it/s] 45%|████▌     | 225/500 [03:24<05:04,  1.11s/it] 45%|████▌     | 227/500 [03:24<03:37,  1.26it/s] 46%|████▌     | 229/500 [03:24<02:36,  1.74it/s] 46%|████▌     | 231/500 [03:30<05:48,  1.30s/it] 47%|████▋     | 233/500 [03:31<04:07,  1.08it/s] 47%|████▋     | 235/500 [03:34<04:52,  1.11s/it]Valid Loss:  0.007832221686840057
Epoch:  179  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  180  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  181  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  182  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  183  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  184  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  185  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  186  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  187  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  188  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  189  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  190  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  191  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  192  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  193  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  194  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  195  	Training Loss: 0.010569371283054352
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  196  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  197  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  198  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  199  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  200  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  201  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  202  	Training Loss: 0.010569371283054352
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  203  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  204  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  205  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  206  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  207  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  208  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  209  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  210  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  211  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  212  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  213  	Training Loss: 0.010569371283054352
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  214  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  215  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  216  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  217  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  218  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  219  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  220  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  221  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  222  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  223  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  224  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  225  	Training Loss: 0.010569371283054352
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  226  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  227  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  228  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  229  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  230  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  231  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  232  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  233  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  234  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  235  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  236  	Training Loss: 0.010569369420409203 47%|████▋     | 237/500 [03:34<03:28,  1.26it/s] 48%|████▊     | 239/500 [03:34<02:30,  1.74it/s] 48%|████▊     | 241/500 [03:40<05:35,  1.30s/it] 49%|████▊     | 243/500 [03:40<03:58,  1.08it/s] 49%|████▉     | 245/500 [03:43<04:42,  1.11s/it] 49%|████▉     | 247/500 [03:43<03:21,  1.26it/s] 50%|████▉     | 249/500 [03:43<02:24,  1.74it/s] 50%|█████     | 251/500 [03:49<05:22,  1.30s/it] 51%|█████     | 253/500 [03:49<03:48,  1.08it/s] 51%|█████     | 255/500 [03:52<04:30,  1.10s/it] 51%|█████▏    | 257/500 [03:53<03:12,  1.26it/s] 52%|█████▏    | 259/500 [03:53<02:18,  1.74it/s] 52%|█████▏    | 261/500 [03:59<05:09,  1.30s/it] 53%|█████▎    | 263/500 [03:59<03:40,  1.08it/s] 53%|█████▎    | 265/500 [04:02<04:19,  1.10s/it] 53%|█████▎    | 267/500 [04:02<03:04,  1.26it/s] 54%|█████▍    | 269/500 [04:02<02:12,  1.75it/s] 54%|█████▍    | 271/500 [04:08<04:56,  1.29s/it] 55%|█████▍    | 273/500 [04:08<03:30,  1.08it/s] 55%|█████▌    | 275/500 [04:11<04:09,  1.11s/it] 55%|█████▌    | 277/500 [04:11<02:57,  1.26it/s] 56%|█████▌    | 279/500 [04:11<02:07,  1.73it/s] 56%|█████▌    | 281/500 [04:17<04:44,  1.30s/it] 57%|█████▋    | 283/500 [04:18<03:21,  1.08it/s] 57%|█████▋    | 285/500 [04:21<03:56,  1.10s/it] 57%|█████▋    | 287/500 [04:21<02:48,  1.26it/s] 58%|█████▊    | 289/500 [04:21<02:00,  1.75it/s] 58%|█████▊    | 291/500 [04:27<04:29,  1.29s/it] 59%|█████▊    | 293/500 [04:27<03:10,  1.08it/s]
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  237  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  238  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  239  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  240  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  241  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  242  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  243  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  244  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  245  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  246  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  247  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  248  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  249  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  250  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  251  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  252  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  253  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  254  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  255  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  256  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  257  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  258  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  259  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  260  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  261  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  262  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  263  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  264  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  265  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  266  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  267  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  268  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  269  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  270  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  271  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  272  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  273  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  274  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  275  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  276  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  277  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  278  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  279  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  280  	Training Loss: 0.010569371283054352
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  281  	Training Loss: 0.010569371283054352
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  282  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  283  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  284  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  285  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  286  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  287  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  288  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  289  	Training Loss: 0.010569371283054352
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  290  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  291  	Training Loss: 0.010569371283054352
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  292  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  293  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  294  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
 59%|█████▉    | 295/500 [04:30<03:46,  1.10s/it] 59%|█████▉    | 297/500 [04:30<02:40,  1.26it/s] 60%|█████▉    | 299/500 [04:30<01:55,  1.74it/s] 60%|██████    | 301/500 [04:36<04:18,  1.30s/it] 61%|██████    | 303/500 [04:36<03:03,  1.07it/s] 61%|██████    | 305/500 [04:39<03:35,  1.11s/it] 61%|██████▏   | 307/500 [04:40<02:33,  1.26it/s] 62%|██████▏   | 309/500 [04:40<01:49,  1.74it/s] 62%|██████▏   | 311/500 [04:46<04:05,  1.30s/it] 63%|██████▎   | 313/500 [04:46<02:53,  1.08it/s] 63%|██████▎   | 315/500 [04:49<03:25,  1.11s/it] 63%|██████▎   | 317/500 [04:49<02:26,  1.25it/s] 64%|██████▍   | 319/500 [04:49<01:44,  1.73it/s] 64%|██████▍   | 321/500 [04:55<03:52,  1.30s/it] 65%|██████▍   | 323/500 [04:55<02:44,  1.07it/s] 65%|██████▌   | 325/500 [04:58<03:13,  1.11s/it] 65%|██████▌   | 327/500 [04:58<02:17,  1.26it/s] 66%|██████▌   | 329/500 [04:58<01:38,  1.74it/s] 66%|██████▌   | 331/500 [05:04<03:39,  1.30s/it] 67%|██████▋   | 333/500 [05:05<02:35,  1.08it/s] 67%|██████▋   | 335/500 [05:08<03:02,  1.11s/it] 67%|██████▋   | 337/500 [05:08<02:09,  1.26it/s] 68%|██████▊   | 339/500 [05:08<01:32,  1.74it/s] 68%|██████▊   | 341/500 [05:14<03:27,  1.30s/it] 69%|██████▊   | 343/500 [05:14<02:26,  1.07it/s] 69%|██████▉   | 345/500 [05:17<02:51,  1.11s/it] 69%|██████▉   | 347/500 [05:17<02:01,  1.26it/s] 70%|██████▉   | 349/500 [05:17<01:26,  1.74it/s] 70%|███████   | 351/500 [05:23<03:12,  1.29s/it]Epoch:  295  	Training Loss: 0.010569371283054352
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  296  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  297  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  298  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  299  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  300  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  301  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  302  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  303  	Training Loss: 0.010569371283054352
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  304  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  305  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  306  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  307  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  308  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  309  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  310  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  311  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  312  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  313  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  314  	Training Loss: 0.010569371283054352
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  315  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  316  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  317  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  318  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  319  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  320  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  321  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  322  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  323  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  324  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  325  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  326  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  327  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  328  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  329  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  330  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  331  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  332  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  333  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  334  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  335  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  336  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  337  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  338  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  339  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  340  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  341  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  342  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  343  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  344  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  345  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  346  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  347  	Training Loss: 0.010569371283054352
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  348  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  349  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  350  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  351  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  352  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
 71%|███████   | 353/500 [05:23<02:15,  1.08it/s] 71%|███████   | 355/500 [05:26<02:40,  1.11s/it] 71%|███████▏  | 357/500 [05:27<01:53,  1.25it/s] 72%|███████▏  | 359/500 [05:27<01:21,  1.73it/s] 72%|███████▏  | 361/500 [05:33<03:00,  1.30s/it] 73%|███████▎  | 363/500 [05:33<02:07,  1.07it/s] 73%|███████▎  | 365/500 [05:36<02:28,  1.10s/it] 73%|███████▎  | 367/500 [05:36<01:45,  1.26it/s] 74%|███████▍  | 369/500 [05:36<01:14,  1.75it/s] 74%|███████▍  | 371/500 [05:42<02:47,  1.30s/it] 75%|███████▍  | 373/500 [05:42<01:57,  1.08it/s] 75%|███████▌  | 375/500 [05:45<02:18,  1.11s/it] 75%|███████▌  | 377/500 [05:45<01:37,  1.26it/s] 76%|███████▌  | 379/500 [05:46<01:09,  1.74it/s] 76%|███████▌  | 381/500 [05:51<02:34,  1.30s/it] 77%|███████▋  | 383/500 [05:52<01:48,  1.08it/s] 77%|███████▋  | 385/500 [05:55<02:06,  1.10s/it] 77%|███████▋  | 387/500 [05:55<01:29,  1.26it/s] 78%|███████▊  | 389/500 [05:55<01:03,  1.75it/s] 78%|███████▊  | 391/500 [06:01<02:20,  1.29s/it] 79%|███████▊  | 393/500 [06:01<01:38,  1.08it/s] 79%|███████▉  | 395/500 [06:04<01:55,  1.10s/it] 79%|███████▉  | 397/500 [06:04<01:21,  1.26it/s] 80%|███████▉  | 399/500 [06:04<00:58,  1.74it/s] 80%|████████  | 401/500 [06:10<02:07,  1.29s/it] 81%|████████  | 403/500 [06:10<01:29,  1.08it/s] 81%|████████  | 405/500 [06:13<01:44,  1.10s/it] 81%|████████▏ | 407/500 [06:13<01:13,  1.26it/s] 82%|████████▏ | 409/500 [06:14<00:52,  1.75it/s]Valid Loss:  0.007832221686840057
Epoch:  353  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  354  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  355  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  356  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  357  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  358  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  359  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  360  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  361  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  362  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  363  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  364  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  365  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  366  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  367  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  368  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  369  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  370  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  371  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  372  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  373  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  374  	Training Loss: 0.010569371283054352
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  375  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  376  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  377  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  378  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  379  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  380  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  381  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  382  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  383  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  384  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  385  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  386  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  387  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  388  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  389  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  390  	Training Loss: 0.010569371283054352
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  391  	Training Loss: 0.010569371283054352
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  392  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  393  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  394  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  395  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  396  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  397  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  398  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  399  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  400  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  401  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  402  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  403  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  404  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  405  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  406  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  407  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  408  	Training Loss: 0.010569371283054352
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  409  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  410  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
 82%|████████▏ | 411/500 [06:20<01:54,  1.29s/it] 83%|████████▎ | 413/500 [06:20<01:20,  1.08it/s] 83%|████████▎ | 415/500 [06:23<01:34,  1.11s/it] 83%|████████▎ | 417/500 [06:23<01:06,  1.26it/s] 84%|████████▍ | 419/500 [06:23<00:46,  1.74it/s] 84%|████████▍ | 421/500 [06:29<01:42,  1.29s/it] 85%|████████▍ | 423/500 [06:29<01:11,  1.08it/s] 85%|████████▌ | 425/500 [06:32<01:22,  1.10s/it] 85%|████████▌ | 427/500 [06:32<00:57,  1.26it/s] 86%|████████▌ | 429/500 [06:32<00:40,  1.74it/s] 86%|████████▌ | 431/500 [06:38<01:29,  1.30s/it] 87%|████████▋ | 433/500 [06:38<01:02,  1.08it/s] 87%|████████▋ | 435/500 [06:42<01:11,  1.11s/it] 87%|████████▋ | 437/500 [06:42<00:50,  1.26it/s] 88%|████████▊ | 439/500 [06:42<00:35,  1.74it/s] 88%|████████▊ | 441/500 [06:48<01:16,  1.30s/it] 89%|████████▊ | 443/500 [06:48<00:52,  1.08it/s] 89%|████████▉ | 445/500 [06:51<01:00,  1.11s/it] 89%|████████▉ | 447/500 [06:51<00:42,  1.26it/s] 90%|████████▉ | 449/500 [06:51<00:29,  1.74it/s] 90%|█████████ | 451/500 [06:57<01:03,  1.30s/it] 91%|█████████ | 453/500 [06:57<00:43,  1.08it/s] 91%|█████████ | 455/500 [07:00<00:49,  1.11s/it] 91%|█████████▏| 457/500 [07:00<00:34,  1.26it/s] 92%|█████████▏| 459/500 [07:01<00:23,  1.74it/s] 92%|█████████▏| 461/500 [07:07<00:50,  1.30s/it] 93%|█████████▎| 463/500 [07:07<00:34,  1.08it/s] 93%|█████████▎| 465/500 [07:10<00:38,  1.11s/it] 93%|█████████▎| 467/500 [07:10<00:26,  1.26it/s]**************************************************learning rate decay**************************************************
Epoch:  411  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  412  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  413  	Training Loss: 0.010569371283054352
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  414  	Training Loss: 0.010569371283054352
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  415  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  416  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  417  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  418  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  419  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  420  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  421  	Training Loss: 0.010569371283054352
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  422  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  423  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  424  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  425  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  426  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  427  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  428  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  429  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  430  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  431  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  432  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  433  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  434  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  435  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  436  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  437  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  438  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  439  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  440  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  441  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  442  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  443  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  444  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  445  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  446  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  447  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  448  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832220755517483
Epoch:  449  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  450  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  451  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  452  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  453  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  454  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  455  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  456  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  457  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  458  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  459  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  460  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  461  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  462  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  463  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  464  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  465  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  466  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  467  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  468  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
 94%|█████████▍| 469/500 [07:10<00:17,  1.74it/s] 94%|█████████▍| 471/500 [07:16<00:37,  1.30s/it] 95%|█████████▍| 473/500 [07:16<00:25,  1.08it/s] 95%|█████████▌| 475/500 [07:19<00:27,  1.11s/it] 95%|█████████▌| 477/500 [07:19<00:18,  1.26it/s] 96%|█████████▌| 479/500 [07:19<00:12,  1.73it/s] 96%|█████████▌| 481/500 [07:25<00:24,  1.30s/it] 97%|█████████▋| 483/500 [07:26<00:15,  1.07it/s] 97%|█████████▋| 485/500 [07:29<00:16,  1.11s/it] 97%|█████████▋| 487/500 [07:29<00:10,  1.26it/s] 98%|█████████▊| 489/500 [07:29<00:06,  1.74it/s] 98%|█████████▊| 491/500 [07:35<00:11,  1.30s/it] 99%|█████████▊| 493/500 [07:35<00:06,  1.08it/s] 99%|█████████▉| 495/500 [07:38<00:05,  1.11s/it] 99%|█████████▉| 497/500 [07:38<00:02,  1.26it/s]100%|█████████▉| 499/500 [07:38<00:00,  1.73it/s]100%|██████████| 500/500 [07:41<00:00,  1.08it/s]
Valid Loss:  0.007832221686840057
Epoch:  469  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  470  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  471  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  472  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  473  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  474  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  475  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  476  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  477  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  478  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  479  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  480  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  481  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  482  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  483  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  484  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  485  	Training Loss: 0.010569371283054352
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
**************************************************learning rate decay**************************************************
Epoch:  486  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  487  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  488  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  489  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  490  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  491  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  492  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  493  	Training Loss: 0.010569370351731777
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  494  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  495  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
Epoch:  496  	Training Loss: 0.010569371283054352
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  497  	Training Loss: 0.010569369420409203
Test Loss:  0.007892880588769913
Valid Loss:  0.007832221686840057
Epoch:  498  	Training Loss: 0.010569369420409203
Test Loss:  0.007892881520092487
Valid Loss:  0.007832220755517483
Epoch:  499  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
Epoch:  500  	Training Loss: 0.010569370351731777
Test Loss:  0.007892881520092487
Valid Loss:  0.007832221686840057
**************************************************learning rate decay**************************************************
seed is  3
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:03<24:58,  3.00s/it]  1%|          | 3/500 [00:03<06:50,  1.21it/s]  1%|          | 5/500 [00:03<03:34,  2.30it/s]  1%|▏         | 7/500 [00:03<02:16,  3.61it/s]  2%|▏         | 9/500 [00:03<01:36,  5.08it/s]  2%|▏         | 11/500 [00:06<05:24,  1.51it/s]  3%|▎         | 13/500 [00:06<03:46,  2.15it/s]  3%|▎         | 15/500 [00:06<02:42,  2.98it/s]  3%|▎         | 17/500 [00:06<02:00,  4.00it/s]  4%|▍         | 19/500 [00:07<01:32,  5.21it/s]  4%|▍         | 21/500 [00:10<04:45,  1.68it/s]  5%|▍         | 23/500 [00:10<03:26,  2.31it/s]  5%|▌         | 25/500 [00:10<02:32,  3.11it/s]  5%|▌         | 27/500 [00:10<01:55,  4.11it/s]  6%|▌         | 29/500 [00:10<01:29,  5.28it/s]  6%|▌         | 31/500 [00:13<04:37,  1.69it/s]  7%|▋         | 33/500 [00:13<03:22,  2.30it/s]  7%|▋         | 35/500 [00:13<02:30,  3.10it/s]  7%|▋         | 37/500 [00:14<01:53,  4.08it/s]  8%|▊         | 39/500 [00:14<01:28,  5.22it/s]  8%|▊         | 41/500 [00:17<04:30,  1.69it/s]  9%|▊         | 43/500 [00:17<03:17,  2.31it/s]  9%|▉         | 45/500 [00:17<02:26,  3.11it/s]  9%|▉         | 47/500 [00:17<01:50,  4.10it/s] 10%|▉         | 49/500 [00:17<01:25,  5.27it/s] 10%|█         | 51/500 [00:20<04:24,  1.70it/s] 11%|█         | 53/500 [00:20<03:13,  2.31it/s] 11%|█         | 55/500 [00:21<02:23,  3.10it/s] 11%|█▏        | 57/500 [00:21<01:48,  4.09it/s] 12%|█▏        | 59/500 [00:21<01:24,  5.23it/s] 12%|█▏        | 61/500 [00:24<04:17,  1.70it/s] 13%|█▎        | 63/500 [00:24<03:08,  2.32it/s] 13%|█▎        | 65/500 [00:24<02:19,  3.12it/s] 13%|█▎        | 67/500 [00:24<01:45,  4.11it/s] 14%|█▍        | 69/500 [00:24<01:21,  5.29it/s] 14%|█▍        | 71/500 [00:27<04:14,  1.68it/s]Epoch:  1  	Training Loss: 0.04131733253598213
Test Loss:  6.918168067932129
Valid Loss:  6.956857681274414
Epoch:  2  	Training Loss: 6.179417610168457
Test Loss:  5.083561420440674
Valid Loss:  4.821292877197266
Epoch:  3  	Training Loss: 4.678304672241211
Test Loss:  0.04871524125337601
Valid Loss:  0.05558798089623451
Epoch:  4  	Training Loss: 0.03965424746274948
Test Loss:  0.04596821591258049
Valid Loss:  0.05281471088528633
Epoch:  5  	Training Loss: 0.03769069164991379
Test Loss:  0.04378195106983185
Valid Loss:  0.05016010254621506
Epoch:  6  	Training Loss: 0.035831380635499954
Test Loss:  0.04113764688372612
Valid Loss:  0.04770083725452423
Epoch:  7  	Training Loss: 0.03405870497226715
Test Loss:  0.03931368887424469
Valid Loss:  0.0453178808093071
Epoch:  8  	Training Loss: 0.032369211316108704
Test Loss:  0.03752408176660538
Valid Loss:  0.04404337331652641
Epoch:  9  	Training Loss: 0.031233884394168854
Test Loss:  0.037606921046972275
Valid Loss:  0.043783046305179596
Epoch:  10  	Training Loss: 0.03082241490483284
Test Loss:  0.037325263023376465
Valid Loss:  0.04364686831831932
Epoch:  11  	Training Loss: 0.030461259186267853
Test Loss:  0.037355322390794754
Valid Loss:  0.04346850514411926
Epoch:  12  	Training Loss: 0.03012768179178238
Test Loss:  0.037234384566545486
Valid Loss:  0.043335311114788055
Epoch:  13  	Training Loss: 0.02983492985367775
Test Loss:  0.03725309669971466
Valid Loss:  0.043204981833696365
Epoch:  14  	Training Loss: 0.029575148597359657
Test Loss:  0.03714410215616226
Valid Loss:  0.043089620769023895
Epoch:  15  	Training Loss: 0.029339754953980446
Test Loss:  0.03713732212781906
Valid Loss:  0.04298517107963562
Epoch:  16  	Training Loss: 0.029124144464731216
Test Loss:  0.03707705810666084
Valid Loss:  0.042888082563877106
Epoch:  17  	Training Loss: 0.028926244005560875
Test Loss:  0.037035007029771805
Valid Loss:  0.04280410706996918
Epoch:  18  	Training Loss: 0.028747394680976868
Test Loss:  0.037030406296253204
Valid Loss:  0.04272911697626114
Epoch:  19  	Training Loss: 0.028585705906152725
Test Loss:  0.03697771579027176
Valid Loss:  0.04266573488712311
Epoch:  20  	Training Loss: 0.028438447043299675
Test Loss:  0.03697020560503006
Valid Loss:  0.04260524734854698
Epoch:  21  	Training Loss: 0.02830391749739647
Test Loss:  0.03692222386598587
Valid Loss:  0.04255098104476929
Epoch:  22  	Training Loss: 0.0281793475151062
Test Loss:  0.03687955066561699
Valid Loss:  0.042499788105487823
Epoch:  23  	Training Loss: 0.028062831610441208
Test Loss:  0.036879658699035645
Valid Loss:  0.042453791946172714
Epoch:  24  	Training Loss: 0.0279573742300272
Test Loss:  0.03684491664171219
Valid Loss:  0.042410243302583694
Epoch:  25  	Training Loss: 0.027861498296260834
Test Loss:  0.03684987872838974
Valid Loss:  0.04237332567572594
Epoch:  26  	Training Loss: 0.027773654088377953
Test Loss:  0.036818236112594604
Valid Loss:  0.042336076498031616
Epoch:  27  	Training Loss: 0.027692075818777084
Test Loss:  0.03677579015493393
Valid Loss:  0.04229844734072685
Epoch:  28  	Training Loss: 0.02761320397257805
Test Loss:  0.03673327714204788
Valid Loss:  0.042261071503162384
Epoch:  29  	Training Loss: 0.027536354959011078
Test Loss:  0.036690641194581985
Valid Loss:  0.042224299162626266
Epoch:  30  	Training Loss: 0.02746177837252617
Test Loss:  0.03666694462299347
Valid Loss:  0.04219093546271324
Epoch:  31  	Training Loss: 0.027391403913497925
Test Loss:  0.0366366021335125
Valid Loss:  0.042157284915447235
Epoch:  32  	Training Loss: 0.02732452191412449
Test Loss:  0.036628544330596924
Valid Loss:  0.04212944954633713
Epoch:  33  	Training Loss: 0.027261845767498016
Test Loss:  0.03660622984170914
Valid Loss:  0.04209981858730316
Epoch:  34  	Training Loss: 0.027202405035495758
Test Loss:  0.03659037873148918
Valid Loss:  0.0420728400349617
Epoch:  35  	Training Loss: 0.02714601904153824
Test Loss:  0.03656753897666931
Valid Loss:  0.042044565081596375
Epoch:  36  	Training Loss: 0.027091341093182564
Test Loss:  0.03654381260275841
Valid Loss:  0.04201599210500717
Epoch:  37  	Training Loss: 0.02703792229294777
Test Loss:  0.03652137890458107
Valid Loss:  0.041987642645835876
Epoch:  38  	Training Loss: 0.026986014097929
Test Loss:  0.0365091972053051
Valid Loss:  0.04196290299296379
Epoch:  39  	Training Loss: 0.026936855167150497
Test Loss:  0.036492712795734406
Valid Loss:  0.041937217116355896
Epoch:  40  	Training Loss: 0.026889679953455925
Test Loss:  0.0364825576543808
Valid Loss:  0.04191465675830841
Epoch:  41  	Training Loss: 0.026844572275877
Test Loss:  0.0364675372838974
Valid Loss:  0.04189082607626915
Epoch:  42  	Training Loss: 0.026800673454999924
Test Loss:  0.03645071014761925
Valid Loss:  0.04186686873435974
Epoch:  43  	Training Loss: 0.0267578586935997
Test Loss:  0.03643323853611946
Valid Loss:  0.041843049228191376
Epoch:  44  	Training Loss: 0.026715926826000214
Test Loss:  0.036415696144104004
Valid Loss:  0.04181957244873047
Epoch:  45  	Training Loss: 0.02667485922574997
Test Loss:  0.03640097752213478
Valid Loss:  0.04179650545120239
Epoch:  46  	Training Loss: 0.02663462981581688
Test Loss:  0.03638722002506256
Valid Loss:  0.04177388548851013
Epoch:  47  	Training Loss: 0.026595987379550934
Test Loss:  0.0363847017288208
Valid Loss:  0.04175737872719765
Epoch:  48  	Training Loss: 0.02655971050262451
Test Loss:  0.036376968026161194
Valid Loss:  0.04173891991376877
Epoch:  49  	Training Loss: 0.02652481570839882
Test Loss:  0.0363665372133255
Valid Loss:  0.04172160103917122
Epoch:  50  	Training Loss: 0.026490982621908188
Test Loss:  0.036360662430524826
Valid Loss:  0.041706643998622894
Epoch:  51  	Training Loss: 0.02645903453230858
Test Loss:  0.03635706752538681
Valid Loss:  0.0416930615901947
Epoch:  52  	Training Loss: 0.026428649201989174
Test Loss:  0.036349087953567505
Valid Loss:  0.041678398847579956
Epoch:  53  	Training Loss: 0.02639930695295334
Test Loss:  0.036344800144433975
Valid Loss:  0.04166554659605026
Epoch:  54  	Training Loss: 0.02637147530913353
Test Loss:  0.03633677959442139
Valid Loss:  0.041651658713817596
Epoch:  55  	Training Loss: 0.026344401761889458
Test Loss:  0.036327116191387177
Valid Loss:  0.04163740575313568
Epoch:  56  	Training Loss: 0.02631787583231926
Test Loss:  0.036316804587841034
Valid Loss:  0.04162311553955078
Epoch:  57  	Training Loss: 0.026292191818356514
Test Loss:  0.036311667412519455
Valid Loss:  0.04161117225885391
Epoch:  58  	Training Loss: 0.026267603039741516
Test Loss:  0.036303721368312836
Valid Loss:  0.041598401963710785
Epoch:  59  	Training Loss: 0.02624359540641308
Test Loss:  0.03629455715417862
Valid Loss:  0.04158533737063408
Epoch:  60  	Training Loss: 0.026220044121146202
Test Loss:  0.0362849198281765
Valid Loss:  0.04157228395342827
Epoch:  61  	Training Loss: 0.02619725652039051
Test Loss:  0.03628537803888321
Valid Loss:  0.04156389832496643
Epoch:  62  	Training Loss: 0.026175931096076965
Test Loss:  0.03628062456846237
Valid Loss:  0.0415547639131546
Epoch:  63  	Training Loss: 0.026155488565564156
Test Loss:  0.03627342730760574
Valid Loss:  0.04154520854353905
Epoch:  64  	Training Loss: 0.026135500520467758
Test Loss:  0.03626514971256256
Valid Loss:  0.04153551161289215
Epoch:  65  	Training Loss: 0.026115871965885162
Test Loss:  0.036256443709135056
Valid Loss:  0.04152583330869675
Epoch:  66  	Training Loss: 0.026096684858202934
Test Loss:  0.03625250607728958
Valid Loss:  0.04151773452758789
Epoch:  67  	Training Loss: 0.02607845515012741
Test Loss:  0.036246076226234436
Valid Loss:  0.041509103029966354
Epoch:  68  	Training Loss: 0.026060642674565315
Test Loss:  0.03623851761221886
Valid Loss:  0.0415002778172493
Epoch:  69  	Training Loss: 0.02604314684867859
Test Loss:  0.03623049333691597
Valid Loss:  0.041491441428661346
Epoch:  70  	Training Loss: 0.026026256382465363
Test Loss:  0.03622698783874512
Valid Loss:  0.04148416966199875
Epoch:  71  	Training Loss: 0.026009995490312576
Test Loss:  0.03622110188007355
Valid Loss:  0.04147633537650108
Epoch:  72  	Training Loss: 0.025994107127189636
Test Loss:  0.03621407970786095
Valid Loss:  0.04146822169423103
 15%|█▍        | 73/500 [00:28<03:06,  2.29it/s] 15%|█▌        | 75/500 [00:28<02:18,  3.07it/s] 15%|█▌        | 77/500 [00:28<01:44,  4.05it/s] 16%|█▌        | 79/500 [00:28<01:20,  5.20it/s] 16%|█▌        | 81/500 [00:31<04:06,  1.70it/s] 17%|█▋        | 83/500 [00:31<03:00,  2.32it/s] 17%|█▋        | 85/500 [00:31<02:13,  3.11it/s] 17%|█▋        | 87/500 [00:31<01:40,  4.10it/s] 18%|█▊        | 89/500 [00:31<01:17,  5.27it/s] 18%|█▊        | 91/500 [00:35<04:00,  1.70it/s] 19%|█▊        | 93/500 [00:35<02:55,  2.32it/s] 19%|█▉        | 95/500 [00:35<02:10,  3.11it/s] 19%|█▉        | 97/500 [00:35<01:38,  4.09it/s] 20%|█▉        | 99/500 [00:35<01:17,  5.21it/s] 20%|██        | 101/500 [00:38<03:56,  1.68it/s] 21%|██        | 103/500 [00:38<02:52,  2.30it/s] 21%|██        | 105/500 [00:38<02:07,  3.09it/s] 21%|██▏       | 107/500 [00:38<01:36,  4.07it/s] 22%|██▏       | 109/500 [00:39<01:14,  5.24it/s] 22%|██▏       | 111/500 [00:42<03:48,  1.71it/s] 23%|██▎       | 113/500 [00:42<02:46,  2.32it/s] 23%|██▎       | 115/500 [00:42<02:03,  3.11it/s] 23%|██▎       | 117/500 [00:42<01:33,  4.10it/s] 24%|██▍       | 119/500 [00:42<01:12,  5.26it/s] 24%|██▍       | 121/500 [00:45<03:42,  1.70it/s] 25%|██▍       | 123/500 [00:45<02:42,  2.33it/s] 25%|██▌       | 125/500 [00:45<01:59,  3.13it/s] 25%|██▌       | 127/500 [00:46<01:30,  4.12it/s] 26%|██▌       | 129/500 [00:46<01:10,  5.29it/s] 26%|██▌       | 131/500 [00:49<03:36,  1.70it/s] 27%|██▋       | 133/500 [00:49<02:38,  2.32it/s] 27%|██▋       | 135/500 [00:49<01:57,  3.12it/s] 27%|██▋       | 137/500 [00:49<01:28,  4.10it/s] 28%|██▊       | 139/500 [00:49<01:08,  5.25it/s] 28%|██▊       | 141/500 [00:52<03:29,  1.71it/s] 29%|██▊       | 143/500 [00:52<02:33,  2.33it/s]Epoch:  73  	Training Loss: 0.025978460907936096
Test Loss:  0.03620660677552223
Valid Loss:  0.041460078209638596
Epoch:  74  	Training Loss: 0.025963403284549713
Test Loss:  0.03620342165231705
Valid Loss:  0.04145348072052002
Epoch:  75  	Training Loss: 0.025948837399482727
Test Loss:  0.0361979678273201
Valid Loss:  0.041446298360824585
Epoch:  76  	Training Loss: 0.025934789329767227
Test Loss:  0.03619586303830147
Valid Loss:  0.04144041985273361
Epoch:  77  	Training Loss: 0.025921303778886795
Test Loss:  0.03619110584259033
Valid Loss:  0.041433803737163544
Epoch:  78  	Training Loss: 0.025908175855875015
Test Loss:  0.03618510439991951
Valid Loss:  0.04142685979604721
Epoch:  79  	Training Loss: 0.02589528262615204
Test Loss:  0.03617854416370392
Valid Loss:  0.04141979664564133
Epoch:  80  	Training Loss: 0.02588270790874958
Test Loss:  0.03617598116397858
Valid Loss:  0.04141431301832199
Epoch:  81  	Training Loss: 0.02587069198489189
Test Loss:  0.036171264946460724
Valid Loss:  0.04140816628932953
Epoch:  82  	Training Loss: 0.025858964771032333
Test Loss:  0.03616547957062721
Valid Loss:  0.0414016954600811
Epoch:  83  	Training Loss: 0.025847390294075012
Test Loss:  0.03615922853350639
Valid Loss:  0.04139512777328491
Epoch:  84  	Training Loss: 0.025836095213890076
Test Loss:  0.03615689277648926
Valid Loss:  0.041390158236026764
Epoch:  85  	Training Loss: 0.025825321674346924
Test Loss:  0.0361529141664505
Valid Loss:  0.04138495773077011
Epoch:  86  	Training Loss: 0.025814790278673172
Test Loss:  0.03614848852157593
Valid Loss:  0.04137996584177017
Epoch:  87  	Training Loss: 0.02580457553267479
Test Loss:  0.036147117614746094
Valid Loss:  0.041375987231731415
Epoch:  88  	Training Loss: 0.025794770568609238
Test Loss:  0.03614390641450882
Valid Loss:  0.04137156158685684
Epoch:  89  	Training Loss: 0.02578519657254219
Test Loss:  0.036139801144599915
Valid Loss:  0.04136693850159645
Epoch:  90  	Training Loss: 0.025775782763957977
Test Loss:  0.03613527864217758
Valid Loss:  0.04136223345994949
Epoch:  91  	Training Loss: 0.02576649934053421
Test Loss:  0.03613058105111122
Valid Loss:  0.04135752469301224
Epoch:  92  	Training Loss: 0.02575739100575447
Test Loss:  0.03612888231873512
Valid Loss:  0.04135385900735855
Epoch:  93  	Training Loss: 0.02574874460697174
Test Loss:  0.03612595796585083
Valid Loss:  0.04134989529848099
Epoch:  94  	Training Loss: 0.025740303099155426
Test Loss:  0.03612219914793968
Valid Loss:  0.041345708072185516
Epoch:  95  	Training Loss: 0.02573198825120926
Test Loss:  0.0361180454492569
Valid Loss:  0.04134143888950348
Epoch:  96  	Training Loss: 0.025723792612552643
Test Loss:  0.03611372783780098
Valid Loss:  0.041337162256240845
Epoch:  97  	Training Loss: 0.025715705007314682
Test Loss:  0.036109354346990585
Valid Loss:  0.041332900524139404
Epoch:  98  	Training Loss: 0.025707721710205078
Test Loss:  0.03610498085618019
Valid Loss:  0.041328687220811844
Epoch:  99  	Training Loss: 0.02569984644651413
Test Loss:  0.03610064089298248
Valid Loss:  0.041324518620967865
Epoch:  100  	Training Loss: 0.025692075490951538
Test Loss:  0.03609634190797806
Valid Loss:  0.04132039472460747
Epoch:  101  	Training Loss: 0.025684408843517303
Test Loss:  0.03609209507703781
Valid Loss:  0.04131632298231125
Epoch:  102  	Training Loss: 0.025676840916275978
Test Loss:  0.03608793020248413
Valid Loss:  0.04131235182285309
Epoch:  103  	Training Loss: 0.02566939778625965
Test Loss:  0.036083824932575226
Valid Loss:  0.04130842909216881
Epoch:  104  	Training Loss: 0.025662053376436234
Test Loss:  0.0360797718167305
Valid Loss:  0.041304562240839005
Epoch:  105  	Training Loss: 0.025654809549450874
Test Loss:  0.03607577085494995
Valid Loss:  0.04130074754357338
Epoch:  106  	Training Loss: 0.025647658854722977
Test Loss:  0.03607182577252388
Valid Loss:  0.041296981275081635
Epoch:  107  	Training Loss: 0.02564060501754284
Test Loss:  0.03606793284416199
Valid Loss:  0.041293270885944366
Epoch:  108  	Training Loss: 0.025633642449975014
Test Loss:  0.03606409579515457
Valid Loss:  0.04128960892558098
Epoch:  109  	Training Loss: 0.02562677301466465
Test Loss:  0.03606030344963074
Valid Loss:  0.04128599539399147
Epoch:  110  	Training Loss: 0.025619996711611748
Test Loss:  0.03605657070875168
Valid Loss:  0.04128243774175644
Epoch:  111  	Training Loss: 0.02561330795288086
Test Loss:  0.0360528826713562
Valid Loss:  0.04127892851829529
Epoch:  112  	Training Loss: 0.025606710463762283
Test Loss:  0.036049261689186096
Valid Loss:  0.04127546399831772
Epoch:  113  	Training Loss: 0.02560020610690117
Test Loss:  0.03604569286108017
Valid Loss:  0.04127205163240433
Epoch:  114  	Training Loss: 0.02559378743171692
Test Loss:  0.036042168736457825
Valid Loss:  0.041268691420555115
Epoch:  115  	Training Loss: 0.025587456300854683
Test Loss:  0.03603868931531906
Valid Loss:  0.041265372186899185
Epoch:  116  	Training Loss: 0.02558121457695961
Test Loss:  0.03603542596101761
Valid Loss:  0.041262149810791016
Epoch:  117  	Training Loss: 0.025575194507837296
Test Loss:  0.03603421896696091
Valid Loss:  0.041259679943323135
Epoch:  118  	Training Loss: 0.025569353252649307
Test Loss:  0.036031998693943024
Valid Loss:  0.04125691205263138
Epoch:  119  	Training Loss: 0.02556362748146057
Test Loss:  0.036029282957315445
Valid Loss:  0.04125401750206947
Epoch:  120  	Training Loss: 0.0255579836666584
Test Loss:  0.03602633625268936
Valid Loss:  0.04125106334686279
Epoch:  121  	Training Loss: 0.0255524180829525
Test Loss:  0.03602328896522522
Valid Loss:  0.04124810919165611
Epoch:  122  	Training Loss: 0.025546923279762268
Test Loss:  0.03602016717195511
Valid Loss:  0.04124511033296585
Epoch:  123  	Training Loss: 0.025541463866829872
Test Loss:  0.03601757064461708
Valid Loss:  0.041242148727178574
Epoch:  124  	Training Loss: 0.025536075234413147
Test Loss:  0.036014996469020844
Valid Loss:  0.04123920947313309
Epoch:  125  	Training Loss: 0.025530755519866943
Test Loss:  0.0360124409198761
Valid Loss:  0.04123631864786148
Epoch:  126  	Training Loss: 0.02552550472319126
Test Loss:  0.03600992262363434
Valid Loss:  0.041233450174331665
Epoch:  127  	Training Loss: 0.025520317256450653
Test Loss:  0.03600743040442467
Valid Loss:  0.04123062640428543
Epoch:  128  	Training Loss: 0.025515202432870865
Test Loss:  0.036004967987537384
Valid Loss:  0.041227832436561584
Epoch:  129  	Training Loss: 0.025510149076581
Test Loss:  0.03600253909826279
Valid Loss:  0.04122508317232132
Epoch:  130  	Training Loss: 0.025505276396870613
Test Loss:  0.036001771688461304
Valid Loss:  0.04122305288910866
Epoch:  131  	Training Loss: 0.02550053410232067
Test Loss:  0.03600025549530983
Valid Loss:  0.04122075438499451
Epoch:  132  	Training Loss: 0.02549589052796364
Test Loss:  0.0359983816742897
Valid Loss:  0.041218351572752
Epoch:  133  	Training Loss: 0.02549133077263832
Test Loss:  0.035996321588754654
Valid Loss:  0.04121590033173561
Epoch:  134  	Training Loss: 0.02548683062195778
Test Loss:  0.035994186997413635
Valid Loss:  0.04121343791484833
Epoch:  135  	Training Loss: 0.025482386350631714
Test Loss:  0.03599201887845993
Valid Loss:  0.04121097922325134
Epoch:  136  	Training Loss: 0.025477997958660126
Test Loss:  0.03598986566066742
Valid Loss:  0.041208550333976746
Epoch:  137  	Training Loss: 0.025473667308688164
Test Loss:  0.035987719893455505
Valid Loss:  0.04120614379644394
Epoch:  138  	Training Loss: 0.02546939067542553
Test Loss:  0.035985589027404785
Valid Loss:  0.04120376706123352
Epoch:  139  	Training Loss: 0.025465166196227074
Test Loss:  0.03598348796367645
Valid Loss:  0.04120141267776489
Epoch:  140  	Training Loss: 0.02546112984418869
Test Loss:  0.0359843373298645
Valid Loss:  0.04120039567351341
Epoch:  141  	Training Loss: 0.0254572331905365
Test Loss:  0.03598383814096451
Valid Loss:  0.041198842227458954
Epoch:  142  	Training Loss: 0.02545350417494774
Test Loss:  0.035982646048069
Valid Loss:  0.041197024285793304
Epoch:  143  	Training Loss: 0.02544986456632614
Test Loss:  0.03598109632730484
Valid Loss:  0.041195064783096313
Epoch:  144  	Training Loss: 0.025446275249123573
Test Loss:   29%|██▉       | 145/500 [00:53<01:53,  3.13it/s] 29%|██▉       | 147/500 [00:53<01:25,  4.12it/s] 30%|██▉       | 149/500 [00:53<01:06,  5.29it/s] 30%|███       | 151/500 [00:56<03:26,  1.69it/s] 31%|███       | 153/500 [00:56<02:30,  2.30it/s] 31%|███       | 155/500 [00:56<01:51,  3.10it/s] 31%|███▏      | 157/500 [00:56<01:24,  4.08it/s] 32%|███▏      | 159/500 [00:56<01:05,  5.24it/s] 32%|███▏      | 161/500 [00:59<03:18,  1.71it/s] 33%|███▎      | 163/500 [00:59<02:24,  2.33it/s] 33%|███▎      | 165/500 [01:00<01:46,  3.13it/s] 33%|███▎      | 167/500 [01:00<01:20,  4.12it/s] 34%|███▍      | 169/500 [01:00<01:02,  5.30it/s] 34%|███▍      | 171/500 [01:03<03:14,  1.69it/s] 35%|███▍      | 173/500 [01:03<02:22,  2.30it/s] 35%|███▌      | 175/500 [01:03<01:45,  3.09it/s] 35%|███▌      | 177/500 [01:03<01:19,  4.07it/s] 36%|███▌      | 179/500 [01:03<01:01,  5.22it/s] 36%|███▌      | 181/500 [01:06<03:08,  1.69it/s] 37%|███▋      | 183/500 [01:07<02:17,  2.31it/s] 37%|███▋      | 185/500 [01:07<01:41,  3.10it/s] 37%|███▋      | 187/500 [01:07<01:16,  4.09it/s] 38%|███▊      | 189/500 [01:07<00:59,  5.25it/s] 38%|███▊      | 191/500 [01:10<03:01,  1.70it/s] 39%|███▊      | 193/500 [01:10<02:12,  2.31it/s] 39%|███▉      | 195/500 [01:10<01:38,  3.10it/s] 39%|███▉      | 197/500 [01:10<01:14,  4.09it/s] 40%|███▉      | 199/500 [01:11<00:57,  5.25it/s] 40%|████      | 201/500 [01:14<02:56,  1.70it/s] 41%|████      | 203/500 [01:14<02:08,  2.32it/s] 41%|████      | 205/500 [01:14<01:34,  3.11it/s] 41%|████▏     | 207/500 [01:14<01:11,  4.10it/s] 42%|████▏     | 209/500 [01:14<00:55,  5.28it/s] 42%|████▏     | 211/500 [01:17<02:50,  1.70it/s] 43%|████▎     | 213/500 [01:17<02:04,  2.31it/s]0.035979367792606354
Valid Loss:  0.041193049401044846
Epoch:  145  	Training Loss: 0.025442734360694885
Test Loss:  0.0359775647521019
Valid Loss:  0.041191015392541885
Epoch:  146  	Training Loss: 0.025439275428652763
Test Loss:  0.03597715124487877
Valid Loss:  0.04118964821100235
Epoch:  147  	Training Loss: 0.025435946881771088
Test Loss:  0.03597605973482132
Valid Loss:  0.04118800163269043
Epoch:  148  	Training Loss: 0.025432689115405083
Test Loss:  0.03597462922334671
Valid Loss:  0.04118621349334717
Epoch:  149  	Training Loss: 0.025429479777812958
Test Loss:  0.03597302734851837
Valid Loss:  0.041184358298778534
Epoch:  150  	Training Loss: 0.025426305830478668
Test Loss:  0.03597133606672287
Valid Loss:  0.04118247702717781
Epoch:  151  	Training Loss: 0.025423172861337662
Test Loss:  0.03596962243318558
Valid Loss:  0.04118059575557709
Epoch:  152  	Training Loss: 0.02542007528245449
Test Loss:  0.0359678752720356
Valid Loss:  0.041178714483976364
Epoch:  153  	Training Loss: 0.02541700005531311
Test Loss:  0.03596614673733711
Valid Loss:  0.041176848113536835
Epoch:  154  	Training Loss: 0.025413963943719864
Test Loss:  0.03596442565321922
Valid Loss:  0.041175007820129395
Epoch:  155  	Training Loss: 0.025410961359739304
Test Loss:  0.035962723195552826
Valid Loss:  0.04117317870259285
Epoch:  156  	Training Loss: 0.02540799416601658
Test Loss:  0.03596104308962822
Valid Loss:  0.041171517223119736
Epoch:  157  	Training Loss: 0.02540506049990654
Test Loss:  0.03595946356654167
Valid Loss:  0.041170064359903336
Epoch:  158  	Training Loss: 0.025402234867215157
Test Loss:  0.03595902770757675
Valid Loss:  0.04116901010274887
Epoch:  159  	Training Loss: 0.025399483740329742
Test Loss:  0.03595804423093796
Valid Loss:  0.04116780310869217
Epoch:  160  	Training Loss: 0.025396784767508507
Test Loss:  0.03595677763223648
Valid Loss:  0.041166502982378006
Epoch:  161  	Training Loss: 0.025394123047590256
Test Loss:  0.03595537319779396
Valid Loss:  0.041165173053741455
Epoch:  162  	Training Loss: 0.025391489267349243
Test Loss:  0.03595392405986786
Valid Loss:  0.041163839399814606
Epoch:  163  	Training Loss: 0.025388900190591812
Test Loss:  0.03595244139432907
Valid Loss:  0.04116252064704895
Epoch:  164  	Training Loss: 0.02538633719086647
Test Loss:  0.03595095872879028
Valid Loss:  0.041161201894283295
Epoch:  165  	Training Loss: 0.025383807718753815
Test Loss:  0.035949476063251495
Valid Loss:  0.04115989804267883
Epoch:  166  	Training Loss: 0.02538130432367325
Test Loss:  0.0359480120241642
Valid Loss:  0.04115860164165497
Epoch:  167  	Training Loss: 0.02537882700562477
Test Loss:  0.035946547985076904
Valid Loss:  0.0411573201417923
Epoch:  168  	Training Loss: 0.025376377627253532
Test Loss:  0.0359451100230217
Valid Loss:  0.04115605354309082
Epoch:  169  	Training Loss: 0.025373956188559532
Test Loss:  0.03594368323683739
Valid Loss:  0.04115480184555054
Epoch:  170  	Training Loss: 0.025371558964252472
Test Loss:  0.03594227135181427
Valid Loss:  0.04115356504917145
Epoch:  171  	Training Loss: 0.02536919340491295
Test Loss:  0.03594087436795235
Valid Loss:  0.04115234315395355
Epoch:  172  	Training Loss: 0.025366846472024918
Test Loss:  0.03593948483467102
Valid Loss:  0.04115112125873566
Epoch:  173  	Training Loss: 0.025364525616168976
Test Loss:  0.035938113927841187
Valid Loss:  0.041149917989969254
Epoch:  174  	Training Loss: 0.025362228974699974
Test Loss:  0.035936757922172546
Valid Loss:  0.04114873334765434
Epoch:  175  	Training Loss: 0.02535995841026306
Test Loss:  0.0359354242682457
Valid Loss:  0.04114755615592003
Epoch:  176  	Training Loss: 0.02535771206021309
Test Loss:  0.035934098064899445
Valid Loss:  0.04114639386534691
Epoch:  177  	Training Loss: 0.025355495512485504
Test Loss:  0.035932786762714386
Valid Loss:  0.041145242750644684
Epoch:  178  	Training Loss: 0.02535329945385456
Test Loss:  0.03593148663640022
Valid Loss:  0.04114411398768425
Epoch:  179  	Training Loss: 0.025351127609610558
Test Loss:  0.03593020886182785
Valid Loss:  0.041142988950014114
Epoch:  180  	Training Loss: 0.025348979979753494
Test Loss:  0.03592894226312637
Valid Loss:  0.041141875088214874
Epoch:  181  	Training Loss: 0.02534685656428337
Test Loss:  0.03592769056558609
Valid Loss:  0.041140779852867126
Epoch:  182  	Training Loss: 0.02534475550055504
Test Loss:  0.035926442593336105
Valid Loss:  0.041139695793390274
Epoch:  183  	Training Loss: 0.025342674925923347
Test Loss:  0.03592521697282791
Valid Loss:  0.041138626635074615
Epoch:  184  	Training Loss: 0.025340620428323746
Test Loss:  0.03592400252819061
Valid Loss:  0.041137561202049255
Epoch:  185  	Training Loss: 0.025338584557175636
Test Loss:  0.035922806710004807
Valid Loss:  0.04113651067018509
Epoch:  186  	Training Loss: 0.02533658780157566
Test Loss:  0.03592244163155556
Valid Loss:  0.041135773062705994
Epoch:  187  	Training Loss: 0.025334671139717102
Test Loss:  0.03592177480459213
Valid Loss:  0.04113493859767914
Epoch:  188  	Training Loss: 0.025332791730761528
Test Loss:  0.0359208881855011
Valid Loss:  0.04113403707742691
Epoch:  189  	Training Loss: 0.025330936536192894
Test Loss:  0.03591989353299141
Valid Loss:  0.041133105754852295
Epoch:  190  	Training Loss: 0.0253291055560112
Test Loss:  0.035918839275836945
Valid Loss:  0.041132163256406784
Epoch:  191  	Training Loss: 0.025327293202280998
Test Loss:  0.03591776266694069
Valid Loss:  0.041131213307380676
Epoch:  192  	Training Loss: 0.025325501337647438
Test Loss:  0.03591667115688324
Valid Loss:  0.04113025590777397
Epoch:  193  	Training Loss: 0.025323718786239624
Test Loss:  0.03591558337211609
Valid Loss:  0.041129305958747864
Epoch:  194  	Training Loss: 0.02532195672392845
Test Loss:  0.035914499312639236
Valid Loss:  0.04112836718559265
Epoch:  195  	Training Loss: 0.02532021328806877
Test Loss:  0.03591342270374298
Valid Loss:  0.04112743213772774
Epoch:  196  	Training Loss: 0.025318488478660583
Test Loss:  0.03591236472129822
Valid Loss:  0.04112651199102402
Epoch:  197  	Training Loss: 0.025316782295703888
Test Loss:  0.035911306738853455
Valid Loss:  0.041125595569610596
Epoch:  198  	Training Loss: 0.025315096601843834
Test Loss:  0.035910263657569885
Valid Loss:  0.041124697774648666
Epoch:  199  	Training Loss: 0.025313425809144974
Test Loss:  0.03590923175215721
Valid Loss:  0.041123807430267334
Epoch:  200  	Training Loss: 0.025311773642897606
Test Loss:  0.03590821474790573
Valid Loss:  0.0411229208111763
Epoch:  201  	Training Loss: 0.02531014010310173
Test Loss:  0.035907208919525146
Valid Loss:  0.04112204909324646
Epoch:  202  	Training Loss: 0.025308525189757347
Test Loss:  0.035906217992305756
Valid Loss:  0.041121192276477814
Epoch:  203  	Training Loss: 0.025306930765509605
Test Loss:  0.03590523451566696
Valid Loss:  0.04112035036087036
Epoch:  204  	Training Loss: 0.025305353105068207
Test Loss:  0.03590427339076996
Valid Loss:  0.041119519621133804
Epoch:  205  	Training Loss: 0.02530379593372345
Test Loss:  0.03590331971645355
Valid Loss:  0.041118696331977844
Epoch:  206  	Training Loss: 0.025302253663539886
Test Loss:  0.03590237349271774
Valid Loss:  0.04111788049340248
Epoch:  207  	Training Loss: 0.025300726294517517
Test Loss:  0.03590143471956253
Valid Loss:  0.04111707583069801
Epoch:  208  	Training Loss: 0.02529921755194664
Test Loss:  0.03590051084756851
Valid Loss:  0.04111628606915474
Epoch:  209  	Training Loss: 0.025297727435827255
Test Loss:  0.03589959442615509
Valid Loss:  0.041115496307611465
Epoch:  210  	Training Loss: 0.025296250358223915
Test Loss:  0.035898689180612564
Valid Loss:  0.04111471772193909
Epoch:  211  	Training Loss: 0.025294791907072067
Test Loss:  0.03589779511094093
Valid Loss:  0.041113950312137604
Epoch:  212  	Training Loss: 0.025293342769145966
Test Loss:  0.035896919667720795
Valid Loss:  0.041113194078207016
Epoch:  213  	Training Loss: 0.02529192715883255
Test Loss:  0.035896699875593185
Valid Loss:  0.041112691164016724
Epoch:  214  	Training Loss: 0.02529056742787361
Test Loss:  0.035896193236112595
Valid Loss:  0.04111209511756897
Epoch:  215  	Training Loss: 0.02528923563659191
Test Loss:   43%|████▎     | 215/500 [01:17<01:31,  3.10it/s] 43%|████▎     | 217/500 [01:18<01:09,  4.09it/s] 44%|████▍     | 219/500 [01:18<00:53,  5.26it/s] 44%|████▍     | 221/500 [01:21<02:43,  1.71it/s] 45%|████▍     | 223/500 [01:21<01:58,  2.33it/s] 45%|████▌     | 225/500 [01:21<01:27,  3.13it/s] 45%|████▌     | 227/500 [01:21<01:06,  4.12it/s] 46%|████▌     | 229/500 [01:21<00:51,  5.29it/s] 46%|████▌     | 231/500 [01:24<02:38,  1.69it/s] 47%|████▋     | 233/500 [01:24<01:55,  2.31it/s] 47%|████▋     | 235/500 [01:25<01:25,  3.10it/s] 47%|████▋     | 237/500 [01:25<01:04,  4.08it/s] 48%|████▊     | 239/500 [01:25<00:49,  5.24it/s] 48%|████▊     | 241/500 [01:28<02:33,  1.69it/s] 49%|████▊     | 243/500 [01:28<01:51,  2.30it/s] 49%|████▉     | 245/500 [01:28<01:22,  3.10it/s] 49%|████▉     | 247/500 [01:28<01:02,  4.08it/s] 50%|████▉     | 249/500 [01:28<00:47,  5.24it/s] 50%|█████     | 251/500 [01:31<02:26,  1.70it/s] 51%|█████     | 253/500 [01:32<01:46,  2.31it/s] 51%|█████     | 255/500 [01:32<01:18,  3.11it/s] 51%|█████▏    | 257/500 [01:32<00:59,  4.09it/s] 52%|█████▏    | 259/500 [01:32<00:45,  5.26it/s] 52%|█████▏    | 261/500 [01:35<02:20,  1.70it/s] 53%|█████▎    | 263/500 [01:35<01:42,  2.32it/s] 53%|█████▎    | 265/500 [01:35<01:15,  3.11it/s] 53%|█████▎    | 267/500 [01:35<00:56,  4.10it/s] 54%|█████▍    | 269/500 [01:35<00:43,  5.27it/s] 54%|█████▍    | 271/500 [01:38<02:14,  1.70it/s] 55%|█████▍    | 273/500 [01:39<01:38,  2.32it/s] 55%|█████▌    | 275/500 [01:39<01:12,  3.11it/s] 55%|█████▌    | 277/500 [01:39<00:54,  4.10it/s] 56%|█████▌    | 279/500 [01:39<00:41,  5.27it/s] 56%|█████▌    | 281/500 [01:42<02:08,  1.71it/s] 57%|█████▋    | 283/500 [01:42<01:33,  2.33it/s] 57%|█████▋    | 285/500 [01:42<01:08,  3.13it/s]0.035895541310310364
Valid Loss:  0.04111144691705704
Epoch:  216  	Training Loss: 0.025287918746471405
Test Loss:  0.03589480370283127
Valid Loss:  0.041110772639513016
Epoch:  217  	Training Loss: 0.025286614894866943
Test Loss:  0.03589402884244919
Valid Loss:  0.0411100871860981
Epoch:  218  	Training Loss: 0.025285327807068825
Test Loss:  0.03589324280619621
Valid Loss:  0.04110940545797348
Epoch:  219  	Training Loss: 0.0252840518951416
Test Loss:  0.03589244186878204
Valid Loss:  0.041108716279268265
Epoch:  220  	Training Loss: 0.025282807648181915
Test Loss:  0.03589288145303726
Valid Loss:  0.041108522564172745
Epoch:  221  	Training Loss: 0.025281619280576706
Test Loss:  0.03589277341961861
Valid Loss:  0.041108138859272
Epoch:  222  	Training Loss: 0.025280483067035675
Test Loss:  0.03589237108826637
Valid Loss:  0.041107647120952606
Epoch:  223  	Training Loss: 0.025279372930526733
Test Loss:  0.03589180111885071
Valid Loss:  0.04110708832740784
Epoch:  224  	Training Loss: 0.025278273969888687
Test Loss:  0.03589114546775818
Valid Loss:  0.04110650718212128
Epoch:  225  	Training Loss: 0.025277188047766685
Test Loss:  0.03589046001434326
Valid Loss:  0.041105978190898895
Epoch:  226  	Training Loss: 0.02527611330151558
Test Loss:  0.03588974475860596
Valid Loss:  0.0411054790019989
Epoch:  227  	Training Loss: 0.02527504786849022
Test Loss:  0.035889022052288055
Valid Loss:  0.041104987263679504
Epoch:  228  	Training Loss: 0.025273993611335754
Test Loss:  0.035888299345970154
Valid Loss:  0.04110448807477951
Epoch:  229  	Training Loss: 0.025272948667407036
Test Loss:  0.03588758781552315
Valid Loss:  0.04110400378704071
Epoch:  230  	Training Loss: 0.025271914899349213
Test Loss:  0.035886868834495544
Valid Loss:  0.04110351577401161
Epoch:  231  	Training Loss: 0.025270892307162285
Test Loss:  0.035886168479919434
Valid Loss:  0.04110304266214371
Epoch:  232  	Training Loss: 0.025269877165555954
Test Loss:  0.03588545322418213
Valid Loss:  0.04110252857208252
Epoch:  233  	Training Loss: 0.025268858298659325
Test Loss:  0.03588474169373512
Valid Loss:  0.04110201820731163
Epoch:  234  	Training Loss: 0.02526784874498844
Test Loss:  0.03588404133915901
Valid Loss:  0.041101522743701935
Epoch:  235  	Training Loss: 0.025266852229833603
Test Loss:  0.0358833447098732
Valid Loss:  0.04110103100538254
Epoch:  236  	Training Loss: 0.025265861302614212
Test Loss:  0.03588265925645828
Valid Loss:  0.041100531816482544
Epoch:  237  	Training Loss: 0.025264881551265717
Test Loss:  0.03588198125362396
Valid Loss:  0.04110005125403404
Epoch:  238  	Training Loss: 0.025263914838433266
Test Loss:  0.03588130325078964
Valid Loss:  0.04109956696629524
Epoch:  239  	Training Loss: 0.025262951850891113
Test Loss:  0.035880669951438904
Valid Loss:  0.041099101305007935
Epoch:  240  	Training Loss: 0.025262020528316498
Test Loss:  0.035880498588085175
Valid Loss:  0.04109877347946167
Epoch:  241  	Training Loss: 0.025261113420128822
Test Loss:  0.03588011860847473
Valid Loss:  0.041098400950431824
Epoch:  242  	Training Loss: 0.02526022121310234
Test Loss:  0.035879652947187424
Valid Loss:  0.04109803959727287
Epoch:  243  	Training Loss: 0.025259368121623993
Test Loss:  0.03587912768125534
Valid Loss:  0.04109766334295273
Epoch:  244  	Training Loss: 0.025258522480726242
Test Loss:  0.03587856888771057
Valid Loss:  0.041097287088632584
Epoch:  245  	Training Loss: 0.025257684290409088
Test Loss:  0.035878006368875504
Valid Loss:  0.04109690338373184
Epoch:  246  	Training Loss: 0.02525685727596283
Test Loss:  0.03587742894887924
Valid Loss:  0.0410965271294117
Epoch:  247  	Training Loss: 0.025256037712097168
Test Loss:  0.03587685525417328
Valid Loss:  0.041096147149801254
Epoch:  248  	Training Loss: 0.025255223736166954
Test Loss:  0.035876285284757614
Valid Loss:  0.04109577089548111
Epoch:  249  	Training Loss: 0.025254417210817337
Test Loss:  0.035875722765922546
Valid Loss:  0.04109539836645126
Epoch:  250  	Training Loss: 0.025253619998693466
Test Loss:  0.03587515652179718
Valid Loss:  0.041095029562711716
Epoch:  251  	Training Loss: 0.02525283396244049
Test Loss:  0.03587459772825241
Valid Loss:  0.041094668209552765
Epoch:  252  	Training Loss: 0.025252047926187515
Test Loss:  0.035874031484127045
Valid Loss:  0.04109428822994232
Epoch:  253  	Training Loss: 0.02525126375257969
Test Loss:  0.03587348014116287
Valid Loss:  0.04109391197562218
Epoch:  254  	Training Loss: 0.02525048330426216
Test Loss:  0.0358729213476181
Valid Loss:  0.04109354317188263
Epoch:  255  	Training Loss: 0.02524971403181553
Test Loss:  0.03587237745523453
Valid Loss:  0.04109317064285278
Epoch:  256  	Training Loss: 0.025248952209949493
Test Loss:  0.03587183356285095
Valid Loss:  0.04109281301498413
Epoch:  257  	Training Loss: 0.025248195976018906
Test Loss:  0.035871297121047974
Valid Loss:  0.04109245538711548
Epoch:  258  	Training Loss: 0.025247447192668915
Test Loss:  0.03587077185511589
Valid Loss:  0.041092097759246826
Epoch:  259  	Training Loss: 0.02524670772254467
Test Loss:  0.03587024658918381
Valid Loss:  0.04109176620841026
Epoch:  260  	Training Loss: 0.025245973840355873
Test Loss:  0.03586972504854202
Valid Loss:  0.04109149053692818
Epoch:  261  	Training Loss: 0.025245247408747673
Test Loss:  0.035869210958480835
Valid Loss:  0.04109121486544609
Epoch:  262  	Training Loss: 0.02524452656507492
Test Loss:  0.03586869686841965
Valid Loss:  0.0410909429192543
Epoch:  263  	Training Loss: 0.025243809446692467
Test Loss:  0.03586818650364876
Valid Loss:  0.04109066724777222
Epoch:  264  	Training Loss: 0.02524309977889061
Test Loss:  0.03586767613887787
Valid Loss:  0.04109039902687073
Epoch:  265  	Training Loss: 0.02524239756166935
Test Loss:  0.035867176949977875
Valid Loss:  0.04109013453125954
Epoch:  266  	Training Loss: 0.025241700932383537
Test Loss:  0.03586668521165848
Valid Loss:  0.04108986631035805
Epoch:  267  	Training Loss: 0.025241009891033173
Test Loss:  0.03586619719862938
Valid Loss:  0.041089605540037155
Epoch:  268  	Training Loss: 0.025240326300263405
Test Loss:  0.03586571291089058
Valid Loss:  0.04108934476971626
Epoch:  269  	Training Loss: 0.025239648297429085
Test Loss:  0.03586523234844208
Valid Loss:  0.04108908772468567
Epoch:  270  	Training Loss: 0.02523897774517536
Test Loss:  0.035864755511283875
Valid Loss:  0.041088834404945374
Epoch:  271  	Training Loss: 0.025238312780857086
Test Loss:  0.03586428612470627
Valid Loss:  0.04108858108520508
Epoch:  272  	Training Loss: 0.025237655267119408
Test Loss:  0.03586381673812866
Valid Loss:  0.04108832776546478
Epoch:  273  	Training Loss: 0.025237001478672028
Test Loss:  0.03586335480213165
Valid Loss:  0.04108808562159538
Epoch:  274  	Training Loss: 0.025236357003450394
Test Loss:  0.03586290031671524
Valid Loss:  0.041087836027145386
Epoch:  275  	Training Loss: 0.025235716253519058
Test Loss:  0.03586244583129883
Valid Loss:  0.041087593883275986
Epoch:  276  	Training Loss: 0.02523508295416832
Test Loss:  0.03586199879646301
Valid Loss:  0.041087355464696884
Epoch:  277  	Training Loss: 0.02523445524275303
Test Loss:  0.035861559212207794
Valid Loss:  0.04108712077140808
Epoch:  278  	Training Loss: 0.025233834981918335
Test Loss:  0.035861119627952576
Valid Loss:  0.04108688235282898
Epoch:  279  	Training Loss: 0.02523321658372879
Test Loss:  0.035860687494277954
Valid Loss:  0.04108665511012077
Epoch:  280  	Training Loss: 0.025232607498764992
Test Loss:  0.03586025908589363
Valid Loss:  0.04108642041683197
Epoch:  281  	Training Loss: 0.02523200213909149
Test Loss:  0.035859834402799606
Valid Loss:  0.04108619689941406
Epoch:  282  	Training Loss: 0.025231406092643738
Test Loss:  0.03585942089557648
Valid Loss:  0.041085973381996155
Epoch:  283  	Training Loss: 0.02523081749677658
Test Loss:  0.035859014838933945
Valid Loss:  0.041085757315158844
Epoch:  284  	Training Loss: 0.02523023635149002
Test Loss:  0.03585860878229141
Valid Loss:  0.04108554497361183
Epoch:  285  	Training Loss: 0.02522966079413891
Test Loss:  0.03585821017622948
Valid Loss:  0.04108533635735512
Epoch:  286  	Training Loss: 0.025229088962078094
Test Loss:   57%|█████▋    | 287/500 [01:42<00:51,  4.13it/s] 58%|█████▊    | 289/500 [01:43<00:39,  5.30it/s] 58%|█████▊    | 291/500 [01:46<02:02,  1.70it/s] 59%|█████▊    | 293/500 [01:46<01:29,  2.32it/s] 59%|█████▉    | 295/500 [01:46<01:05,  3.11it/s] 59%|█████▉    | 297/500 [01:46<00:49,  4.10it/s] 60%|█████▉    | 299/500 [01:46<00:38,  5.27it/s] 60%|██████    | 301/500 [01:49<01:57,  1.69it/s] 61%|██████    | 303/500 [01:49<01:25,  2.30it/s] 61%|██████    | 305/500 [01:49<01:02,  3.10it/s] 61%|██████▏   | 307/500 [01:50<00:47,  4.08it/s] 62%|██████▏   | 309/500 [01:50<00:36,  5.24it/s] 62%|██████▏   | 311/500 [01:53<01:55,  1.64it/s] 63%|██████▎   | 313/500 [01:53<01:23,  2.23it/s] 63%|██████▎   | 315/500 [01:53<01:01,  3.00it/s] 63%|██████▎   | 317/500 [01:53<00:46,  3.96it/s] 64%|██████▍   | 319/500 [01:53<00:35,  5.11it/s] 64%|██████▍   | 321/500 [01:56<01:45,  1.70it/s] 65%|██████▍   | 323/500 [01:56<01:16,  2.32it/s] 65%|██████▌   | 325/500 [01:57<00:56,  3.11it/s] 65%|██████▌   | 327/500 [01:57<00:42,  4.10it/s] 66%|██████▌   | 329/500 [01:57<00:32,  5.27it/s] 66%|██████▌   | 331/500 [02:00<01:39,  1.70it/s] 67%|██████▋   | 333/500 [02:00<01:12,  2.31it/s] 67%|██████▋   | 335/500 [02:00<00:53,  3.10it/s] 67%|██████▋   | 337/500 [02:00<00:39,  4.09it/s] 68%|██████▊   | 339/500 [02:00<00:30,  5.25it/s] 68%|██████▊   | 341/500 [02:03<01:33,  1.70it/s] 69%|██████▊   | 343/500 [02:04<01:07,  2.33it/s] 69%|██████▉   | 345/500 [02:04<00:49,  3.12it/s] 69%|██████▉   | 347/500 [02:04<00:37,  4.11it/s] 70%|██████▉   | 349/500 [02:04<00:28,  5.29it/s] 70%|███████   | 351/500 [02:07<01:27,  1.70it/s] 71%|███████   | 353/500 [02:07<01:03,  2.31it/s] 71%|███████   | 355/500 [02:07<00:46,  3.11it/s]0.03585781157016754
Valid Loss:  0.041085124015808105
Epoch:  287  	Training Loss: 0.025228524580597878
Test Loss:  0.0358574241399765
Valid Loss:  0.04108491912484169
Epoch:  288  	Training Loss: 0.02522796392440796
Test Loss:  0.03585703670978546
Valid Loss:  0.041084714233875275
Epoch:  289  	Training Loss: 0.025227412581443787
Test Loss:  0.035856943577528
Valid Loss:  0.04108457639813423
Epoch:  290  	Training Loss: 0.025226883590221405
Test Loss:  0.03585673123598099
Valid Loss:  0.0410844124853611
Epoch:  291  	Training Loss: 0.02522636204957962
Test Loss:  0.03585644066333771
Valid Loss:  0.04108423739671707
Epoch:  292  	Training Loss: 0.025225846096873283
Test Loss:  0.035856105387210846
Valid Loss:  0.041084036231040955
Epoch:  293  	Training Loss: 0.0252253245562315
Test Loss:  0.035855747759342194
Valid Loss:  0.04108383506536484
Epoch:  294  	Training Loss: 0.025224808603525162
Test Loss:  0.035855382680892944
Valid Loss:  0.041083626449108124
Epoch:  295  	Training Loss: 0.025224298238754272
Test Loss:  0.0358550138771534
Valid Loss:  0.04108342528343201
Epoch:  296  	Training Loss: 0.02522379159927368
Test Loss:  0.03585464879870415
Valid Loss:  0.04108321666717529
Epoch:  297  	Training Loss: 0.02522328868508339
Test Loss:  0.0358542837202549
Valid Loss:  0.04108302295207977
Epoch:  298  	Training Loss: 0.025222791358828545
Test Loss:  0.03585392236709595
Valid Loss:  0.041082825511693954
Epoch:  299  	Training Loss: 0.02522229589521885
Test Loss:  0.035853561013936996
Valid Loss:  0.04108262434601784
Epoch:  300  	Training Loss: 0.0252218097448349
Test Loss:  0.035853203386068344
Valid Loss:  0.04108242690563202
Epoch:  301  	Training Loss: 0.0252213254570961
Test Loss:  0.03585285320878029
Valid Loss:  0.0410822331905365
Epoch:  302  	Training Loss: 0.0252208448946476
Test Loss:  0.03585249185562134
Valid Loss:  0.04108203202486038
Epoch:  303  	Training Loss: 0.025220362469553947
Test Loss:  0.03585217893123627
Valid Loss:  0.04108182340860367
Epoch:  304  	Training Loss: 0.025219883769750595
Test Loss:  0.035851866006851196
Valid Loss:  0.041081614792346954
Epoch:  305  	Training Loss: 0.025219406932592392
Test Loss:  0.035851556807756424
Valid Loss:  0.04108141362667084
Epoch:  306  	Training Loss: 0.025218935683369637
Test Loss:  0.03585125133395195
Valid Loss:  0.04108121618628502
Epoch:  307  	Training Loss: 0.02521847002208233
Test Loss:  0.03585094213485718
Valid Loss:  0.041081011295318604
Epoch:  308  	Training Loss: 0.02521800994873047
Test Loss:  0.035850640386343
Valid Loss:  0.041080817580223083
Epoch:  309  	Training Loss: 0.02521754801273346
Test Loss:  0.035850342363119125
Valid Loss:  0.041080623865127563
Epoch:  310  	Training Loss: 0.025217093527317047
Test Loss:  0.03585004806518555
Valid Loss:  0.04108043015003204
Epoch:  311  	Training Loss: 0.025216642767190933
Test Loss:  0.03584975376725197
Valid Loss:  0.04108023643493652
Epoch:  312  	Training Loss: 0.025216199457645416
Test Loss:  0.03584948182106018
Valid Loss:  0.04108006879687309
Epoch:  313  	Training Loss: 0.025215767323970795
Test Loss:  0.035849206149578094
Valid Loss:  0.04107990115880966
Epoch:  314  	Training Loss: 0.02521534264087677
Test Loss:  0.035848937928676605
Valid Loss:  0.04107974097132683
Epoch:  315  	Training Loss: 0.025214917957782745
Test Loss:  0.035848669707775116
Valid Loss:  0.0410795733332634
Epoch:  316  	Training Loss: 0.025214500725269318
Test Loss:  0.035848408937454224
Valid Loss:  0.04107941687107086
Epoch:  317  	Training Loss: 0.02521408721804619
Test Loss:  0.03584814816713333
Valid Loss:  0.041079260408878326
Epoch:  318  	Training Loss: 0.02521367557346821
Test Loss:  0.035847894847393036
Valid Loss:  0.041079096496105194
Epoch:  319  	Training Loss: 0.025213271379470825
Test Loss:  0.035847634077072144
Valid Loss:  0.04107894003391266
Epoch:  320  	Training Loss: 0.025212865322828293
Test Loss:  0.03584738075733185
Valid Loss:  0.04107878357172012
Epoch:  321  	Training Loss: 0.025212466716766357
Test Loss:  0.03584713488817215
Valid Loss:  0.04107862710952759
Epoch:  322  	Training Loss: 0.02521206997334957
Test Loss:  0.035846881568431854
Valid Loss:  0.04107847809791565
Epoch:  323  	Training Loss: 0.025211676955223083
Test Loss:  0.03584663197398186
Valid Loss:  0.041078321635723114
Epoch:  324  	Training Loss: 0.025211283937096596
Test Loss:  0.03584638610482216
Valid Loss:  0.041078172624111176
Epoch:  325  	Training Loss: 0.025210898369550705
Test Loss:  0.03584614396095276
Valid Loss:  0.04107802361249924
Epoch:  326  	Training Loss: 0.025210510939359665
Test Loss:  0.035846076905727386
Valid Loss:  0.04107794165611267
Epoch:  327  	Training Loss: 0.025210145860910416
Test Loss:  0.03584592789411545
Valid Loss:  0.04107784479856491
Epoch:  328  	Training Loss: 0.025209784507751465
Test Loss:  0.03584574908018112
Valid Loss:  0.041077740490436554
Epoch:  329  	Training Loss: 0.025209425017237663
Test Loss:  0.03584554046392441
Valid Loss:  0.0410776361823082
Epoch:  330  	Training Loss: 0.02520906925201416
Test Loss:  0.0358453206717968
Valid Loss:  0.04107752442359924
Epoch:  331  	Training Loss: 0.025208719074726105
Test Loss:  0.03584510087966919
Valid Loss:  0.041077420115470886
Epoch:  332  	Training Loss: 0.02520836889743805
Test Loss:  0.03584488481283188
Valid Loss:  0.04107731580734253
Epoch:  333  	Training Loss: 0.02520802617073059
Test Loss:  0.03584466502070427
Valid Loss:  0.041077204048633575
Epoch:  334  	Training Loss: 0.025207683444023132
Test Loss:  0.03584444522857666
Valid Loss:  0.04107709228992462
Epoch:  335  	Training Loss: 0.02520734816789627
Test Loss:  0.03584422916173935
Valid Loss:  0.041076984256505966
Epoch:  336  	Training Loss: 0.02520701289176941
Test Loss:  0.03584401309490204
Valid Loss:  0.04107687622308731
Epoch:  337  	Training Loss: 0.025206679478287697
Test Loss:  0.035843800753355026
Valid Loss:  0.041076771914958954
Epoch:  338  	Training Loss: 0.025206351652741432
Test Loss:  0.035843588411808014
Valid Loss:  0.0410766676068306
Epoch:  339  	Training Loss: 0.025206023827195168
Test Loss:  0.0358433797955513
Valid Loss:  0.04107656329870224
Epoch:  340  	Training Loss: 0.02520570158958435
Test Loss:  0.035843174904584885
Valid Loss:  0.04107646271586418
Epoch:  341  	Training Loss: 0.025205381214618683
Test Loss:  0.03584297001361847
Valid Loss:  0.041076358407735825
Epoch:  342  	Training Loss: 0.025205064564943314
Test Loss:  0.03584276884794235
Valid Loss:  0.04107627272605896
Epoch:  343  	Training Loss: 0.02520475536584854
Test Loss:  0.03584257513284683
Valid Loss:  0.0410761833190918
Epoch:  344  	Training Loss: 0.025204449892044067
Test Loss:  0.0358424037694931
Valid Loss:  0.041076093912124634
Epoch:  345  	Training Loss: 0.025204148143529892
Test Loss:  0.03584223985671997
Valid Loss:  0.04107601195573807
Epoch:  346  	Training Loss: 0.025203846395015717
Test Loss:  0.03584207594394684
Valid Loss:  0.041075922548770905
Epoch:  347  	Training Loss: 0.02520354464650154
Test Loss:  0.035841912031173706
Valid Loss:  0.04107583686709404
Epoch:  348  	Training Loss: 0.025203248485922813
Test Loss:  0.035841748118400574
Valid Loss:  0.041075751185417175
Epoch:  349  	Training Loss: 0.025202956050634384
Test Loss:  0.03584159165620804
Valid Loss:  0.04107566550374031
Epoch:  350  	Training Loss: 0.025202667340636253
Test Loss:  0.035841427743434906
Valid Loss:  0.04107558727264404
Epoch:  351  	Training Loss: 0.025202378630638123
Test Loss:  0.03584127500653267
Valid Loss:  0.04107550531625748
Epoch:  352  	Training Loss: 0.02520209364593029
Test Loss:  0.035841118544340134
Valid Loss:  0.04107541963458061
Epoch:  353  	Training Loss: 0.025201808661222458
Test Loss:  0.0358409620821476
Valid Loss:  0.041075337678194046
Epoch:  354  	Training Loss: 0.025201529264450073
Test Loss:  0.03584080934524536
Valid Loss:  0.04107525199651718
Epoch:  355  	Training Loss: 0.02520124986767769
Test Loss:  0.035840656608343124
Valid Loss:  0.041075170040130615
Epoch:  356  	Training Loss: 0.025200974196195602
Test Loss:  0.03584061563014984
Valid Loss:  0.04107511043548584
Epoch:  357  	Training Loss: 0.02520070970058441
Test Loss:  0.035840533673763275
 71%|███████▏  | 357/500 [02:07<00:34,  4.09it/s] 72%|███████▏  | 359/500 [02:08<00:26,  5.26it/s] 72%|███████▏  | 361/500 [02:11<01:21,  1.70it/s] 73%|███████▎  | 363/500 [02:11<00:58,  2.32it/s] 73%|███████▎  | 365/500 [02:11<00:43,  3.12it/s] 73%|███████▎  | 367/500 [02:11<00:32,  4.11it/s] 74%|███████▍  | 369/500 [02:11<00:24,  5.28it/s] 74%|███████▍  | 371/500 [02:14<01:15,  1.70it/s] 75%|███████▍  | 373/500 [02:14<00:54,  2.31it/s] 75%|███████▌  | 375/500 [02:14<00:40,  3.10it/s] 75%|███████▌  | 377/500 [02:15<00:30,  4.08it/s] 76%|███████▌  | 379/500 [02:15<00:23,  5.24it/s] 76%|███████▌  | 381/500 [02:18<01:09,  1.70it/s] 77%|███████▋  | 383/500 [02:18<00:50,  2.32it/s] 77%|███████▋  | 385/500 [02:18<00:36,  3.12it/s] 77%|███████▋  | 387/500 [02:18<00:27,  4.11it/s] 78%|███████▊  | 389/500 [02:18<00:21,  5.27it/s] 78%|███████▊  | 391/500 [02:21<01:04,  1.68it/s] 79%|███████▊  | 393/500 [02:21<00:46,  2.29it/s] 79%|███████▉  | 395/500 [02:22<00:34,  3.07it/s] 79%|███████▉  | 397/500 [02:22<00:25,  4.05it/s] 80%|███████▉  | 399/500 [02:22<00:19,  5.20it/s] 80%|████████  | 401/500 [02:25<00:58,  1.70it/s] 81%|████████  | 403/500 [02:25<00:41,  2.32it/s] 81%|████████  | 405/500 [02:25<00:30,  3.11it/s] 81%|████████▏ | 407/500 [02:25<00:22,  4.10it/s] 82%|████████▏ | 409/500 [02:25<00:17,  5.27it/s] 82%|████████▏ | 411/500 [02:28<00:52,  1.69it/s] 83%|████████▎ | 413/500 [02:29<00:37,  2.30it/s] 83%|████████▎ | 415/500 [02:29<00:27,  3.10it/s] 83%|████████▎ | 417/500 [02:29<00:20,  4.07it/s] 84%|████████▍ | 419/500 [02:29<00:15,  5.24it/s] 84%|████████▍ | 421/500 [02:32<00:46,  1.70it/s] 85%|████████▍ | 423/500 [02:32<00:33,  2.32it/s] 85%|████████▌ | 425/500 [02:32<00:24,  3.12it/s] 85%|████████▌ | 427/500 [02:32<00:17,  4.12it/s]Valid Loss:  0.041075050830841064
Epoch:  358  	Training Loss: 0.02520044520497322
Test Loss:  0.03584041818976402
Valid Loss:  0.041074976325035095
Epoch:  359  	Training Loss: 0.025200188159942627
Test Loss:  0.035840291529893875
Valid Loss:  0.041074901819229126
Epoch:  360  	Training Loss: 0.025199932977557182
Test Loss:  0.03584015741944313
Valid Loss:  0.041074831038713455
Epoch:  361  	Training Loss: 0.025199677795171738
Test Loss:  0.03584001958370209
Valid Loss:  0.041074756532907486
Epoch:  362  	Training Loss: 0.02519942633807659
Test Loss:  0.03583987057209015
Valid Loss:  0.041074663400650024
Epoch:  363  	Training Loss: 0.02519916743040085
Test Loss:  0.035839714109897614
Valid Loss:  0.04107457399368286
Epoch:  364  	Training Loss: 0.025198910385370255
Test Loss:  0.03583956137299538
Valid Loss:  0.0410744845867157
Epoch:  365  	Training Loss: 0.02519865520298481
Test Loss:  0.03583941608667374
Valid Loss:  0.041074395179748535
Epoch:  366  	Training Loss: 0.025198403745889664
Test Loss:  0.0358392633497715
Valid Loss:  0.04107430577278137
Epoch:  367  	Training Loss: 0.02519815042614937
Test Loss:  0.03583911061286926
Valid Loss:  0.04107421636581421
Epoch:  368  	Training Loss: 0.02519790455698967
Test Loss:  0.03583896905183792
Valid Loss:  0.041074130684137344
Epoch:  369  	Training Loss: 0.025197656825184822
Test Loss:  0.03583882004022598
Valid Loss:  0.04107404500246048
Epoch:  370  	Training Loss: 0.025197412818670273
Test Loss:  0.03583867475390434
Valid Loss:  0.04107395559549332
Epoch:  371  	Training Loss: 0.025197170674800873
Test Loss:  0.0358385294675827
Valid Loss:  0.04107387736439705
Epoch:  372  	Training Loss: 0.025196930393576622
Test Loss:  0.03583838418126106
Valid Loss:  0.041073787957429886
Epoch:  373  	Training Loss: 0.02519669197499752
Test Loss:  0.03583823889493942
Valid Loss:  0.041073694825172424
Epoch:  374  	Training Loss: 0.02519644983112812
Test Loss:  0.03583809360861778
Valid Loss:  0.04107360914349556
Epoch:  375  	Training Loss: 0.025196213275194168
Test Loss:  0.03583795204758644
Valid Loss:  0.041073523461818695
Epoch:  376  	Training Loss: 0.025195978581905365
Test Loss:  0.0358378142118454
Valid Loss:  0.04107343405485153
Epoch:  377  	Training Loss: 0.02519574761390686
Test Loss:  0.03583766520023346
Valid Loss:  0.041073352098464966
Epoch:  378  	Training Loss: 0.025195516645908356
Test Loss:  0.035837531089782715
Valid Loss:  0.0410732626914978
Epoch:  379  	Training Loss: 0.02519528567790985
Test Loss:  0.03583738952875137
Valid Loss:  0.04107318073511124
Epoch:  380  	Training Loss: 0.025195058435201645
Test Loss:  0.03583725541830063
Valid Loss:  0.04107309877872467
Epoch:  381  	Training Loss: 0.02519483119249344
Test Loss:  0.035837121307849884
Valid Loss:  0.041073013097047806
Epoch:  382  	Training Loss: 0.02519460767507553
Test Loss:  0.035836998373270035
Valid Loss:  0.04107294976711273
Epoch:  383  	Training Loss: 0.025194399058818817
Test Loss:  0.035836879163980484
Valid Loss:  0.04107289016246796
Epoch:  384  	Training Loss: 0.025194186717271805
Test Loss:  0.03583676367998123
Valid Loss:  0.04107282683253288
Epoch:  385  	Training Loss: 0.02519398182630539
Test Loss:  0.03583664447069168
Valid Loss:  0.04107276350259781
Epoch:  386  	Training Loss: 0.025193775072693825
Test Loss:  0.03583653271198273
Valid Loss:  0.04107270389795303
Epoch:  387  	Training Loss: 0.02519357204437256
Test Loss:  0.035836417227983475
Valid Loss:  0.04107264056801796
Epoch:  388  	Training Loss: 0.025193367153406143
Test Loss:  0.03583630174398422
Valid Loss:  0.04107258468866348
Epoch:  389  	Training Loss: 0.025193165987730026
Test Loss:  0.03583618998527527
Valid Loss:  0.04107252135872841
Epoch:  390  	Training Loss: 0.025192968547344208
Test Loss:  0.035836078226566315
Valid Loss:  0.041072458028793335
Epoch:  391  	Training Loss: 0.02519277110695839
Test Loss:  0.03583596646785736
Valid Loss:  0.04107240214943886
Epoch:  392  	Training Loss: 0.02519257552921772
Test Loss:  0.035835858434438705
Valid Loss:  0.04107234254479408
Epoch:  393  	Training Loss: 0.02519238367676735
Test Loss:  0.03583575040102005
Valid Loss:  0.041072286665439606
Epoch:  394  	Training Loss: 0.02519219182431698
Test Loss:  0.03583564609289169
Valid Loss:  0.04107223451137543
Epoch:  395  	Training Loss: 0.025192001834511757
Test Loss:  0.035835541784763336
Valid Loss:  0.04107217490673065
Epoch:  396  	Training Loss: 0.025191813707351685
Test Loss:  0.03583543375134468
Valid Loss:  0.04107212275266647
Epoch:  397  	Training Loss: 0.02519162744283676
Test Loss:  0.03583533316850662
Valid Loss:  0.041072066873311996
Epoch:  398  	Training Loss: 0.025191443040966988
Test Loss:  0.035835228860378265
Valid Loss:  0.04107201099395752
Epoch:  399  	Training Loss: 0.025191262364387512
Test Loss:  0.035835132002830505
Valid Loss:  0.04107195883989334
Epoch:  400  	Training Loss: 0.02519107796251774
Test Loss:  0.03583503141999245
Valid Loss:  0.041071899235248566
Epoch:  401  	Training Loss: 0.025190897285938263
Test Loss:  0.03583493083715439
Valid Loss:  0.04107184708118439
Epoch:  402  	Training Loss: 0.025190718472003937
Test Loss:  0.03583483397960663
Valid Loss:  0.04107179120182991
Epoch:  403  	Training Loss: 0.02519053965806961
Test Loss:  0.03583472967147827
Valid Loss:  0.04107173532247543
Epoch:  404  	Training Loss: 0.025190364569425583
Test Loss:  0.03583463281393051
Valid Loss:  0.04107168689370155
Epoch:  405  	Training Loss: 0.025190189480781555
Test Loss:  0.03583453223109245
Valid Loss:  0.041071634739637375
Epoch:  406  	Training Loss: 0.025190018117427826
Test Loss:  0.03583443909883499
Valid Loss:  0.041071586310863495
Epoch:  407  	Training Loss: 0.025189846754074097
Test Loss:  0.03583434224128723
Valid Loss:  0.041071534156799316
Epoch:  408  	Training Loss: 0.025189675390720367
Test Loss:  0.03583424910902977
Valid Loss:  0.04107148200273514
Epoch:  409  	Training Loss: 0.025189507752656937
Test Loss:  0.03583415970206261
Valid Loss:  0.04107142984867096
Epoch:  410  	Training Loss: 0.025189340114593506
Test Loss:  0.03583406284451485
Valid Loss:  0.04107138141989708
Epoch:  411  	Training Loss: 0.025189172476530075
Test Loss:  0.035833973437547684
Valid Loss:  0.0410713329911232
Epoch:  412  	Training Loss: 0.025189008563756943
Test Loss:  0.035833895206451416
Valid Loss:  0.04107128828763962
Epoch:  413  	Training Loss: 0.025188852101564407
Test Loss:  0.03583381325006485
Valid Loss:  0.041071247309446335
Epoch:  414  	Training Loss: 0.025188695639371872
Test Loss:  0.035833731293678284
Valid Loss:  0.04107120633125305
Epoch:  415  	Training Loss: 0.025188541039824486
Test Loss:  0.035833653062582016
Valid Loss:  0.04107116907835007
Epoch:  416  	Training Loss: 0.025188390165567398
Test Loss:  0.03583357483148575
Valid Loss:  0.04107113182544708
Epoch:  417  	Training Loss: 0.02518823742866516
Test Loss:  0.03583350032567978
Valid Loss:  0.0410710945725441
Epoch:  418  	Training Loss: 0.025188086554408073
Test Loss:  0.03583342581987381
Valid Loss:  0.041071053594350815
Epoch:  419  	Training Loss: 0.025187939405441284
Test Loss:  0.03583335131406784
Valid Loss:  0.04107102006673813
Epoch:  420  	Training Loss: 0.025187792256474495
Test Loss:  0.03583327308297157
Valid Loss:  0.041070982813835144
Epoch:  421  	Training Loss: 0.025187645107507706
Test Loss:  0.0358332023024559
Valid Loss:  0.04107094183564186
Epoch:  422  	Training Loss: 0.025187501683831215
Test Loss:  0.035833120346069336
Valid Loss:  0.04107089340686798
Epoch:  423  	Training Loss: 0.025187350809574127
Test Loss:  0.03583303838968277
Valid Loss:  0.0410708524286747
Epoch:  424  	Training Loss: 0.02518720179796219
Test Loss:  0.035832956433296204
Valid Loss:  0.041070807725191116
Epoch:  425  	Training Loss: 0.02518705651164055
Test Loss:  0.035832881927490234
Valid Loss:  0.04107077047228813
Epoch:  426  	Training Loss: 0.02518691122531891
Test Loss:  0.03583279997110367
Valid Loss:  0.04107072204351425
Epoch:  427  	Training Loss: 0.025186769664287567
Test Loss:  0.03583277016878128
Valid Loss:  0.041070692241191864
Epoch:  428  	Training Loss: 0.025186629965901375
Test Loss:  0.0358327180147171
Valid Loss:   86%|████████▌ | 429/500 [02:32<00:13,  5.29it/s] 86%|████████▌ | 431/500 [02:35<00:40,  1.69it/s] 87%|████████▋ | 433/500 [02:36<00:29,  2.30it/s] 87%|████████▋ | 435/500 [02:36<00:21,  3.09it/s] 87%|████████▋ | 437/500 [02:36<00:15,  4.07it/s] 88%|████████▊ | 439/500 [02:36<00:11,  5.23it/s] 88%|████████▊ | 441/500 [02:39<00:34,  1.69it/s] 89%|████████▊ | 443/500 [02:39<00:24,  2.31it/s] 89%|████████▉ | 445/500 [02:39<00:17,  3.11it/s] 89%|████████▉ | 447/500 [02:39<00:12,  4.10it/s] 90%|████████▉ | 449/500 [02:40<00:09,  5.27it/s] 90%|█████████ | 451/500 [02:43<00:28,  1.70it/s] 91%|█████████ | 453/500 [02:43<00:20,  2.32it/s] 91%|█████████ | 455/500 [02:43<00:14,  3.11it/s] 91%|█████████▏| 457/500 [02:43<00:10,  4.09it/s] 92%|█████████▏| 459/500 [02:43<00:07,  5.26it/s] 92%|█████████▏| 461/500 [02:46<00:22,  1.71it/s] 93%|█████████▎| 463/500 [02:46<00:15,  2.34it/s] 93%|█████████▎| 465/500 [02:46<00:11,  3.13it/s] 93%|█████████▎| 467/500 [02:47<00:08,  4.12it/s] 94%|█████████▍| 469/500 [02:47<00:05,  5.29it/s] 94%|█████████▍| 471/500 [02:50<00:17,  1.69it/s] 95%|█████████▍| 473/500 [02:50<00:11,  2.29it/s] 95%|█████████▌| 475/500 [02:50<00:08,  3.08it/s] 95%|█████████▌| 477/500 [02:50<00:05,  4.06it/s] 96%|█████████▌| 479/500 [02:50<00:04,  5.22it/s] 96%|█████████▌| 481/500 [02:53<00:11,  1.71it/s] 97%|█████████▋| 483/500 [02:53<00:07,  2.33it/s] 97%|█████████▋| 485/500 [02:53<00:04,  3.13it/s] 97%|█████████▋| 487/500 [02:54<00:03,  4.12it/s] 98%|█████████▊| 489/500 [02:54<00:02,  5.30it/s] 98%|█████████▊| 491/500 [02:57<00:05,  1.67it/s] 99%|█████████▊| 493/500 [02:57<00:03,  2.28it/s] 99%|█████████▉| 495/500 [02:57<00:01,  3.06it/s] 99%|█████████▉| 497/500 [02:57<00:00,  4.02it/s]0.04107065498828888
Epoch:  429  	Training Loss: 0.025186490267515182
Test Loss:  0.03583265841007233
Valid Loss:  0.04107062146067619
Epoch:  430  	Training Loss: 0.025186356157064438
Test Loss:  0.03583258390426636
Valid Loss:  0.04107058048248291
Epoch:  431  	Training Loss: 0.025186218321323395
Test Loss:  0.03583251312375069
Valid Loss:  0.041070543229579926
Epoch:  432  	Training Loss: 0.02518608421087265
Test Loss:  0.035832442343235016
Valid Loss:  0.041070498526096344
Epoch:  433  	Training Loss: 0.025185950100421906
Test Loss:  0.03583236783742905
Valid Loss:  0.04107045754790306
Epoch:  434  	Training Loss: 0.02518581971526146
Test Loss:  0.03583229333162308
Valid Loss:  0.04107041656970978
Epoch:  435  	Training Loss: 0.025185685604810715
Test Loss:  0.03583221882581711
Valid Loss:  0.04107037931680679
Epoch:  436  	Training Loss: 0.02518555521965027
Test Loss:  0.03583214432001114
Valid Loss:  0.04107033833861351
Epoch:  437  	Training Loss: 0.025185424834489822
Test Loss:  0.03583207726478577
Valid Loss:  0.041070301085710526
Epoch:  438  	Training Loss: 0.025185298174619675
Test Loss:  0.0358319990336895
Valid Loss:  0.04107026010751724
Epoch:  439  	Training Loss: 0.025185171514749527
Test Loss:  0.035831935703754425
Valid Loss:  0.04107021912932396
Epoch:  440  	Training Loss: 0.02518504485487938
Test Loss:  0.035831861197948456
Valid Loss:  0.041070178151130676
Epoch:  441  	Training Loss: 0.02518491819500923
Test Loss:  0.035831790417432785
Valid Loss:  0.04107014089822769
Epoch:  442  	Training Loss: 0.025184793397784233
Test Loss:  0.035831719636917114
Valid Loss:  0.04107010364532471
Epoch:  443  	Training Loss: 0.025184668600559235
Test Loss:  0.035831648856401443
Valid Loss:  0.041070062667131424
Epoch:  444  	Training Loss: 0.025184545665979385
Test Loss:  0.03583157807588577
Valid Loss:  0.04107002541422844
Epoch:  445  	Training Loss: 0.025184422731399536
Test Loss:  0.0358315110206604
Valid Loss:  0.041069984436035156
Epoch:  446  	Training Loss: 0.025184301659464836
Test Loss:  0.03583143651485443
Valid Loss:  0.041069939732551575
Epoch:  447  	Training Loss: 0.025184180587530136
Test Loss:  0.03583136945962906
Valid Loss:  0.04106990247964859
Epoch:  448  	Training Loss: 0.025184061378240585
Test Loss:  0.035831302404403687
Valid Loss:  0.041069865226745605
Epoch:  449  	Training Loss: 0.025183942168951035
Test Loss:  0.03583123907446861
Valid Loss:  0.04106982797384262
Epoch:  450  	Training Loss: 0.025183826684951782
Test Loss:  0.03583117201924324
Valid Loss:  0.041069790720939636
Epoch:  451  	Training Loss: 0.02518370933830738
Test Loss:  0.03583110123872757
Valid Loss:  0.04106975346803665
Epoch:  452  	Training Loss: 0.02518359199166298
Test Loss:  0.035831041634082794
Valid Loss:  0.041069719940423965
Epoch:  453  	Training Loss: 0.025183480232954025
Test Loss:  0.03583098202943802
Valid Loss:  0.04106968641281128
Epoch:  454  	Training Loss: 0.02518336847424507
Test Loss:  0.03583092242479324
Valid Loss:  0.04106965661048889
Epoch:  455  	Training Loss: 0.025183256715536118
Test Loss:  0.03583086282014847
Valid Loss:  0.041069623082876205
Epoch:  456  	Training Loss: 0.025183148682117462
Test Loss:  0.03583080321550369
Valid Loss:  0.04106958955526352
Epoch:  457  	Training Loss: 0.025183040648698807
Test Loss:  0.03583074361085892
Valid Loss:  0.04106955602765083
Epoch:  458  	Training Loss: 0.025182930752635002
Test Loss:  0.03583068400621414
Valid Loss:  0.041069526225328445
Epoch:  459  	Training Loss: 0.025182824581861496
Test Loss:  0.035830624401569366
Valid Loss:  0.04106949269771576
Epoch:  460  	Training Loss: 0.02518272027373314
Test Loss:  0.03583057224750519
Valid Loss:  0.04106946289539337
Epoch:  461  	Training Loss: 0.025182615965604782
Test Loss:  0.03583051264286041
Valid Loss:  0.041069429367780685
Epoch:  462  	Training Loss: 0.025182509794831276
Test Loss:  0.035830460488796234
Valid Loss:  0.041069403290748596
Epoch:  463  	Training Loss: 0.025182407349348068
Test Loss:  0.035830408334732056
Valid Loss:  0.04106937348842621
Epoch:  464  	Training Loss: 0.025182310491800308
Test Loss:  0.03583035618066788
Valid Loss:  0.04106935113668442
Epoch:  465  	Training Loss: 0.02518220990896225
Test Loss:  0.0358303040266037
Valid Loss:  0.04106932133436203
Epoch:  466  	Training Loss: 0.02518210932612419
Test Loss:  0.03583025187253952
Valid Loss:  0.04106929525732994
Epoch:  467  	Training Loss: 0.02518201246857643
Test Loss:  0.03583019971847534
Valid Loss:  0.04106926918029785
Epoch:  468  	Training Loss: 0.02518191561102867
Test Loss:  0.03583014756441116
Valid Loss:  0.04106924310326576
Epoch:  469  	Training Loss: 0.02518181875348091
Test Loss:  0.03583009913563728
Valid Loss:  0.04106921702623367
Epoch:  470  	Training Loss: 0.0251817237585783
Test Loss:  0.0358300507068634
Valid Loss:  0.04106919467449188
Epoch:  471  	Training Loss: 0.02518162876367569
Test Loss:  0.035829998552799225
Valid Loss:  0.04106916859745979
Epoch:  472  	Training Loss: 0.02518153376877308
Test Loss:  0.035829946398735046
Valid Loss:  0.041069138795137405
Epoch:  473  	Training Loss: 0.02518143691122532
Test Loss:  0.03582989424467087
Valid Loss:  0.04106910526752472
Epoch:  474  	Training Loss: 0.025181341916322708
Test Loss:  0.03582984209060669
Valid Loss:  0.04106907546520233
Epoch:  475  	Training Loss: 0.025181245058774948
Test Loss:  0.03582978993654251
Valid Loss:  0.041069045662879944
Epoch:  476  	Training Loss: 0.025181151926517487
Test Loss:  0.03582973778247833
Valid Loss:  0.041069015860557556
Epoch:  477  	Training Loss: 0.025181058794260025
Test Loss:  0.035829685628414154
Valid Loss:  0.04106898978352547
Epoch:  478  	Training Loss: 0.025180963799357414
Test Loss:  0.03582964092493057
Valid Loss:  0.04106895998120308
Epoch:  479  	Training Loss: 0.02518087439239025
Test Loss:  0.035829588770866394
Valid Loss:  0.04106893390417099
Epoch:  480  	Training Loss: 0.02518078126013279
Test Loss:  0.035829540342092514
Valid Loss:  0.0410689041018486
Epoch:  481  	Training Loss: 0.025180689990520477
Test Loss:  0.035829488188028336
Valid Loss:  0.041068874299526215
Epoch:  482  	Training Loss: 0.025180600583553314
Test Loss:  0.03582943230867386
Valid Loss:  0.04106883332133293
Epoch:  483  	Training Loss: 0.025180505588650703
Test Loss:  0.035829368978738785
Valid Loss:  0.04106879234313965
Epoch:  484  	Training Loss: 0.025180408731102943
Test Loss:  0.03582931309938431
Valid Loss:  0.041068751364946365
Epoch:  485  	Training Loss: 0.025180313736200333
Test Loss:  0.03582925349473953
Valid Loss:  0.04106871411204338
Epoch:  486  	Training Loss: 0.025180216878652573
Test Loss:  0.03582919389009476
Valid Loss:  0.0410686731338501
Epoch:  487  	Training Loss: 0.02518012374639511
Test Loss:  0.03582913801074028
Valid Loss:  0.04106863588094711
Epoch:  488  	Training Loss: 0.02518003061413765
Test Loss:  0.0358290821313858
Valid Loss:  0.04106859117746353
Epoch:  489  	Training Loss: 0.025179937481880188
Test Loss:  0.035829029977321625
Valid Loss:  0.041068557649850845
Epoch:  490  	Training Loss: 0.025179848074913025
Test Loss:  0.035828977823257446
Valid Loss:  0.04106851667165756
Epoch:  491  	Training Loss: 0.025179758667945862
Test Loss:  0.03582891821861267
Valid Loss:  0.041068486869335175
Epoch:  492  	Training Loss: 0.02517966739833355
Test Loss:  0.035828880965709686
Valid Loss:  0.041068464517593384
Epoch:  493  	Training Loss: 0.025179587304592133
Test Loss:  0.0358288399875164
Valid Loss:  0.041068438440561295
Epoch:  494  	Training Loss: 0.025179509073495865
Test Loss:  0.03582879900932312
Valid Loss:  0.041068412363529205
Epoch:  495  	Training Loss: 0.025179430842399597
Test Loss:  0.03582875803112984
Valid Loss:  0.04106839373707771
Epoch:  496  	Training Loss: 0.02517935261130333
Test Loss:  0.035828717052936554
Valid Loss:  0.04106837511062622
Epoch:  497  	Training Loss: 0.02517927624285221
Test Loss:  0.03582867980003357
Valid Loss:  0.04106835648417473
Epoch:  498  	Training Loss: 0.025179198011755943
Test Loss:  0.035828642547130585
Valid Loss:  0.041068337857723236
Epoch:  499  	Training Loss: 0.025179125368595123
Test Loss:  0.0358286052942276
Valid Loss:  100%|█████████▉| 499/500 [02:57<00:00,  5.16it/s]100%|██████████| 500/500 [02:57<00:00,  2.81it/s]
0.041068319231271744
Epoch:  500  	Training Loss: 0.025179047137498856
Test Loss:  0.035828568041324615
Valid Loss:  0.04106830433011055
seed is  4
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:02<24:53,  2.99s/it]  1%|          | 3/500 [00:03<06:49,  1.21it/s]  1%|          | 5/500 [00:03<03:34,  2.31it/s]  1%|▏         | 7/500 [00:03<02:16,  3.62it/s]  2%|▏         | 9/500 [00:03<01:36,  5.10it/s]  2%|▏         | 11/500 [00:06<05:22,  1.52it/s]  3%|▎         | 13/500 [00:06<03:45,  2.16it/s]  3%|▎         | 15/500 [00:09<06:30,  1.24it/s]  3%|▎         | 17/500 [00:09<04:36,  1.75it/s]  4%|▍         | 19/500 [00:09<03:19,  2.42it/s]  4%|▍         | 21/500 [00:15<09:36,  1.20s/it]  5%|▍         | 23/500 [00:16<06:48,  1.17it/s]  5%|▌         | 25/500 [00:19<08:21,  1.06s/it]  5%|▌         | 27/500 [00:19<05:58,  1.32it/s]  6%|▌         | 29/500 [00:19<04:18,  1.83it/s]  6%|▌         | 31/500 [00:25<10:01,  1.28s/it]  7%|▋         | 33/500 [00:25<07:08,  1.09it/s]  7%|▋         | 35/500 [00:28<08:29,  1.10s/it]  7%|▋         | 37/500 [00:28<06:03,  1.27it/s]  8%|▊         | 39/500 [00:28<04:22,  1.76it/s]  8%|▊         | 41/500 [00:34<09:53,  1.29s/it]  9%|▊         | 43/500 [00:34<07:02,  1.08it/s]  9%|▉         | 45/500 [00:37<08:22,  1.11s/it]  9%|▉         | 47/500 [00:38<05:59,  1.26it/s] 10%|▉         | 49/500 [00:38<04:19,  1.74it/s] 10%|█         | 51/500 [00:44<09:42,  1.30s/it] 11%|█         | 53/500 [00:44<06:54,  1.08it/s] 11%|█         | 55/500 [00:47<08:12,  1.11s/it] 11%|█▏        | 57/500 [00:47<05:51,  1.26it/s] 12%|█▏        | 59/500 [00:47<04:13,  1.74it/s] 12%|█▏        | 61/500 [00:53<09:28,  1.29s/it]Epoch:  1  	Training Loss: 0.10208308696746826
Test Loss:  257.9630126953125
Valid Loss:  248.42575073242188
Epoch:  2  	Training Loss: 251.45921325683594
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  3  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  4  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  5  	Training Loss: 0.13260148465633392
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  6  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  7  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  8  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510004222393036
Epoch:  9  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  10  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  11  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  12  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  13  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  14  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  15  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  16  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  17  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  18  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  19  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  20  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  21  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  22  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510004222393036
Epoch:  23  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  24  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510004222393036
Epoch:  25  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  26  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  27  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  28  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  29  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  30  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  31  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  32  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  33  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  34  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  35  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  36  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  37  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  38  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  39  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  40  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  41  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  42  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  43  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  44  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  45  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  46  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  47  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  48  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  49  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  50  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  51  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  52  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  53  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  54  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  55  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  56  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  57  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510004222393036
Epoch:  58  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  59  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  60  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  61  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  62  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155 13%|█▎        | 63/500 [00:53<06:44,  1.08it/s] 13%|█▎        | 65/500 [00:56<07:59,  1.10s/it] 13%|█▎        | 67/500 [00:56<05:43,  1.26it/s] 14%|█▍        | 69/500 [00:56<04:07,  1.74it/s] 14%|█▍        | 71/500 [01:02<09:15,  1.29s/it] 15%|█▍        | 73/500 [01:03<06:35,  1.08it/s] 15%|█▌        | 75/500 [01:06<07:48,  1.10s/it] 15%|█▌        | 77/500 [01:06<05:34,  1.26it/s] 16%|█▌        | 79/500 [01:06<04:01,  1.75it/s] 16%|█▌        | 81/500 [01:12<09:05,  1.30s/it] 17%|█▋        | 83/500 [01:12<06:28,  1.07it/s] 17%|█▋        | 85/500 [01:15<07:41,  1.11s/it] 17%|█▋        | 87/500 [01:15<05:30,  1.25it/s] 18%|█▊        | 89/500 [01:15<03:58,  1.73it/s] 18%|█▊        | 91/500 [01:21<08:51,  1.30s/it] 19%|█▊        | 93/500 [01:21<06:18,  1.08it/s] 19%|█▉        | 95/500 [01:24<07:30,  1.11s/it] 19%|█▉        | 97/500 [01:25<05:21,  1.25it/s] 20%|█▉        | 99/500 [01:25<03:51,  1.73it/s] 20%|██        | 101/500 [01:31<08:39,  1.30s/it] 21%|██        | 103/500 [01:31<06:09,  1.08it/s] 21%|██        | 105/500 [01:34<07:17,  1.11s/it] 21%|██▏       | 107/500 [01:34<05:12,  1.26it/s] 22%|██▏       | 109/500 [01:34<03:45,  1.73it/s] 22%|██▏       | 111/500 [01:40<08:28,  1.31s/it] 23%|██▎       | 113/500 [01:40<06:02,  1.07it/s] 23%|██▎       | 115/500 [01:43<07:06,  1.11s/it] 23%|██▎       | 117/500 [01:43<05:04,  1.26it/s] 24%|██▍       | 119/500 [01:44<03:39,  1.74it/s] 24%|██▍       | 121/500 [01:50<08:13,  1.30s/it]
Epoch:  63  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  64  	Training Loss: 0.13260148465633392
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  65  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  66  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  67  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  68  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  69  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510004222393036
Epoch:  70  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  71  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  72  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  73  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  74  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  75  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510004222393036
**************************************************learning rate decay**************************************************
Epoch:  76  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  77  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  78  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510004222393036
Epoch:  79  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  80  	Training Loss: 0.13260148465633392
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  81  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  82  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  83  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  84  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  85  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  86  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  87  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  88  	Training Loss: 0.13260148465633392
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  89  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  90  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  91  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  92  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  93  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  94  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510004222393036
Epoch:  95  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  96  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  97  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  98  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  99  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  100  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  101  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510004222393036
Epoch:  102  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  103  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  104  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  105  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  106  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  107  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  108  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  109  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  110  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  111  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  112  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  113  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  114  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  115  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  116  	Training Loss: 0.13260148465633392
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  117  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  118  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  119  	Training Loss: 0.13260148465633392
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  120  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510004222393036
**************************************************learning rate decay**************************************************
Epoch:  121  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  122  	Training Loss: 0.13260149955749512
Test Loss:   25%|██▍       | 123/500 [01:50<05:50,  1.08it/s] 25%|██▌       | 125/500 [01:53<06:54,  1.11s/it] 25%|██▌       | 127/500 [01:53<04:56,  1.26it/s] 26%|██▌       | 129/500 [01:53<03:33,  1.74it/s] 26%|██▌       | 131/500 [01:59<07:58,  1.30s/it] 27%|██▋       | 133/500 [01:59<05:40,  1.08it/s] 27%|██▋       | 135/500 [02:02<06:46,  1.11s/it] 27%|██▋       | 137/500 [02:02<04:50,  1.25it/s] 28%|██▊       | 139/500 [02:02<03:28,  1.73it/s] 28%|██▊       | 141/500 [02:09<07:52,  1.32s/it] 29%|██▊       | 143/500 [02:09<05:35,  1.06it/s] 29%|██▉       | 145/500 [02:12<06:36,  1.12s/it] 29%|██▉       | 147/500 [02:12<04:43,  1.25it/s] 30%|██▉       | 149/500 [02:12<03:24,  1.72it/s] 30%|███       | 151/500 [02:18<07:38,  1.31s/it] 31%|███       | 153/500 [02:18<05:25,  1.06it/s] 31%|███       | 155/500 [02:21<06:22,  1.11s/it] 31%|███▏      | 157/500 [02:21<04:32,  1.26it/s] 32%|███▏      | 159/500 [02:21<03:16,  1.74it/s] 32%|███▏      | 161/500 [02:27<07:24,  1.31s/it] 33%|███▎      | 163/500 [02:28<05:15,  1.07it/s] 33%|███▎      | 165/500 [02:31<06:14,  1.12s/it] 33%|███▎      | 167/500 [02:31<04:27,  1.24it/s] 34%|███▍      | 169/500 [02:31<03:12,  1.72it/s] 34%|███▍      | 171/500 [02:37<07:11,  1.31s/it] 35%|███▍      | 173/500 [02:37<05:06,  1.07it/s] 35%|███▌      | 175/500 [02:40<06:01,  1.11s/it] 35%|███▌      | 177/500 [02:40<04:17,  1.25it/s] 36%|███▌      | 179/500 [02:40<03:05,  1.73it/s]0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  123  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  124  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  125  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  126  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  127  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  128  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  129  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  130  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  131  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  132  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  133  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  134  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  135  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  136  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  137  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  138  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  139  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  140  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  141  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  142  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510004222393036
Epoch:  143  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  144  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  145  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  146  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  147  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  148  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  149  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510004222393036
Epoch:  150  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  151  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  152  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  153  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  154  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  155  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510004222393036
**************************************************learning rate decay**************************************************
Epoch:  156  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  157  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  158  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  159  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  160  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  161  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  162  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  163  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  164  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  165  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  166  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  167  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  168  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  169  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  170  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  171  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  172  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  173  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  174  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  175  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510004222393036
**************************************************learning rate decay**************************************************
Epoch:  176  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  177  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  178  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  179  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  180  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  181  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
 36%|███▌      | 181/500 [02:46<06:54,  1.30s/it] 37%|███▋      | 183/500 [02:46<04:54,  1.08it/s] 37%|███▋      | 185/500 [02:50<05:49,  1.11s/it] 37%|███▋      | 187/500 [02:50<04:09,  1.25it/s] 38%|███▊      | 189/500 [02:50<03:00,  1.72it/s] 38%|███▊      | 191/500 [02:56<06:41,  1.30s/it] 39%|███▊      | 193/500 [02:56<04:45,  1.07it/s] 39%|███▉      | 195/500 [02:59<05:38,  1.11s/it] 39%|███▉      | 197/500 [02:59<04:01,  1.26it/s] 40%|███▉      | 199/500 [02:59<02:53,  1.73it/s] 40%|████      | 201/500 [03:05<06:29,  1.30s/it] 41%|████      | 203/500 [03:05<04:36,  1.07it/s] 41%|████      | 205/500 [03:08<05:27,  1.11s/it] 41%|████▏     | 207/500 [03:09<03:53,  1.26it/s] 42%|████▏     | 209/500 [03:09<02:47,  1.73it/s] 42%|████▏     | 211/500 [03:15<06:15,  1.30s/it] 43%|████▎     | 213/500 [03:15<04:26,  1.08it/s] 43%|████▎     | 215/500 [03:18<05:16,  1.11s/it] 43%|████▎     | 217/500 [03:18<03:45,  1.26it/s] 44%|████▍     | 219/500 [03:18<02:41,  1.74it/s] 44%|████▍     | 221/500 [03:24<06:02,  1.30s/it] 45%|████▍     | 223/500 [03:24<04:17,  1.08it/s] 45%|████▌     | 225/500 [03:27<05:05,  1.11s/it] 45%|████▌     | 227/500 [03:27<03:37,  1.25it/s] 46%|████▌     | 229/500 [03:28<02:36,  1.73it/s] 46%|████▌     | 231/500 [03:34<05:50,  1.30s/it] 47%|████▋     | 233/500 [03:34<04:08,  1.07it/s] 47%|████▋     | 235/500 [03:37<04:53,  1.11s/it] 47%|████▋     | 237/500 [03:37<03:28,  1.26it/s] 48%|████▊     | 239/500 [03:37<02:29,  1.74it/s]Valid Loss:  0.14510005712509155
Epoch:  182  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  183  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  184  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  185  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  186  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  187  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  188  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  189  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  190  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  191  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  192  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  193  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  194  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  195  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  196  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  197  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  198  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  199  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  200  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  201  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  202  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  203  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  204  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510004222393036
Epoch:  205  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  206  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510004222393036
Epoch:  207  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  208  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  209  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  210  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  211  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  212  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  213  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  214  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  215  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  216  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  217  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  218  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  219  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  220  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  221  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  222  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  223  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  224  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  225  	Training Loss: 0.13260148465633392
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  226  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  227  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  228  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  229  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  230  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  231  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  232  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  233  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  234  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  235  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  236  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  237  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  238  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  239  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  240  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
 48%|████▊     | 241/500 [03:43<05:35,  1.30s/it] 49%|████▊     | 243/500 [03:43<03:58,  1.08it/s] 49%|████▉     | 245/500 [03:46<04:41,  1.11s/it] 49%|████▉     | 247/500 [03:46<03:20,  1.26it/s] 50%|████▉     | 249/500 [03:46<02:24,  1.74it/s] 50%|█████     | 251/500 [03:52<05:22,  1.29s/it] 51%|█████     | 253/500 [03:52<03:48,  1.08it/s] 51%|█████     | 255/500 [03:55<04:31,  1.11s/it] 51%|█████▏    | 257/500 [03:56<03:12,  1.26it/s] 52%|█████▏    | 259/500 [03:56<02:18,  1.74it/s] 52%|█████▏    | 261/500 [04:02<05:10,  1.30s/it] 53%|█████▎    | 263/500 [04:02<03:40,  1.08it/s] 53%|█████▎    | 265/500 [04:05<04:21,  1.11s/it] 53%|█████▎    | 267/500 [04:05<03:06,  1.25it/s] 54%|█████▍    | 269/500 [04:05<02:13,  1.73it/s] 54%|█████▍    | 271/500 [04:11<04:56,  1.30s/it] 55%|█████▍    | 273/500 [04:11<03:30,  1.08it/s] 55%|█████▌    | 275/500 [04:14<04:08,  1.10s/it] 55%|█████▌    | 277/500 [04:14<02:56,  1.26it/s] 56%|█████▌    | 279/500 [04:15<02:06,  1.74it/s] 56%|█████▌    | 281/500 [04:20<04:43,  1.30s/it] 57%|█████▋    | 283/500 [04:21<03:21,  1.08it/s] 57%|█████▋    | 285/500 [04:24<03:57,  1.10s/it] 57%|█████▋    | 287/500 [04:24<02:49,  1.26it/s] 58%|█████▊    | 289/500 [04:24<02:01,  1.74it/s] 58%|█████▊    | 291/500 [04:30<04:29,  1.29s/it] 59%|█████▊    | 293/500 [04:30<03:11,  1.08it/s] 59%|█████▉    | 295/500 [04:33<03:45,  1.10s/it] 59%|█████▉    | 297/500 [04:33<02:40,  1.27it/s] 60%|█████▉    | 299/500 [04:33<01:54,  1.75it/s]**************************************************learning rate decay**************************************************
Epoch:  241  	Training Loss: 0.13260148465633392
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  242  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  243  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  244  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  245  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  246  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  247  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  248  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  249  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  250  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  251  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  252  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  253  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  254  	Training Loss: 0.13260148465633392
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  255  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  256  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  257  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  258  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  259  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  260  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  261  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  262  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  263  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  264  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  265  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  266  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  267  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  268  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  269  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  270  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  271  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  272  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  273  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  274  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  275  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  276  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  277  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  278  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  279  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  280  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510004222393036
**************************************************learning rate decay**************************************************
Epoch:  281  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  282  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  283  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  284  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  285  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  286  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  287  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  288  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  289  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  290  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  291  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  292  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  293  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  294  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  295  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  296  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  297  	Training Loss: 0.13260146975517273
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  298  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  299  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
 60%|██████    | 301/500 [04:39<04:20,  1.31s/it] 61%|██████    | 303/500 [04:39<03:04,  1.07it/s] 61%|██████    | 305/500 [04:43<03:38,  1.12s/it] 61%|██████▏   | 307/500 [04:43<02:35,  1.24it/s] 62%|██████▏   | 309/500 [04:43<01:51,  1.71it/s] 62%|██████▏   | 311/500 [04:49<04:07,  1.31s/it] 63%|██████▎   | 313/500 [04:49<02:55,  1.07it/s] 63%|██████▎   | 315/500 [04:52<03:26,  1.12s/it] 63%|██████▎   | 317/500 [04:52<02:26,  1.25it/s] 64%|██████▍   | 319/500 [04:52<01:44,  1.73it/s] 64%|██████▍   | 321/500 [04:58<03:52,  1.30s/it] 65%|██████▍   | 323/500 [04:58<02:44,  1.08it/s] 65%|██████▌   | 325/500 [05:01<03:13,  1.11s/it] 65%|██████▌   | 327/500 [05:02<02:17,  1.26it/s] 66%|██████▌   | 329/500 [05:02<01:38,  1.74it/s] 66%|██████▌   | 331/500 [05:08<03:40,  1.30s/it] 67%|██████▋   | 333/500 [05:08<02:35,  1.07it/s] 67%|██████▋   | 335/500 [05:11<03:03,  1.11s/it] 67%|██████▋   | 337/500 [05:11<02:10,  1.25it/s] 68%|██████▊   | 339/500 [05:11<01:33,  1.73it/s] 68%|██████▊   | 341/500 [05:17<03:26,  1.30s/it] 69%|██████▊   | 343/500 [05:17<02:25,  1.08it/s] 69%|██████▉   | 345/500 [05:20<02:51,  1.11s/it] 69%|██████▉   | 347/500 [05:20<02:01,  1.26it/s] 70%|██████▉   | 349/500 [05:21<01:27,  1.73it/s] 70%|███████   | 351/500 [05:27<03:12,  1.29s/it] 71%|███████   | 353/500 [05:27<02:16,  1.08it/s] 71%|███████   | 355/500 [05:30<02:40,  1.11s/it] 71%|███████▏  | 357/500 [05:30<01:53,  1.26it/s]Epoch:  300  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  301  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  302  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  303  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  304  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  305  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  306  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  307  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510004222393036
Epoch:  308  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  309  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  310  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  311  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  312  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  313  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  314  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  315  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  316  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  317  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  318  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  319  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  320  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  321  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  322  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  323  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  324  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  325  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  326  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  327  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  328  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  329  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  330  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  331  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  332  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  333  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  334  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  335  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  336  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  337  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  338  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510004222393036
Epoch:  339  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  340  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  341  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  342  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510004222393036
Epoch:  343  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  344  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  345  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  346  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  347  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  348  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  349  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  350  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  351  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  352  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  353  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  354  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  355  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  356  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  357  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  358  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
 72%|███████▏  | 359/500 [05:30<01:21,  1.74it/s] 72%|███████▏  | 361/500 [05:36<02:59,  1.29s/it] 73%|███████▎  | 363/500 [05:36<02:06,  1.08it/s] 73%|███████▎  | 365/500 [05:39<02:28,  1.10s/it] 73%|███████▎  | 367/500 [05:39<01:45,  1.27it/s] 74%|███████▍  | 369/500 [05:39<01:14,  1.75it/s] 74%|███████▍  | 371/500 [05:45<02:47,  1.30s/it] 75%|███████▍  | 373/500 [05:45<01:57,  1.08it/s] 75%|███████▌  | 375/500 [05:48<02:17,  1.10s/it] 75%|███████▌  | 377/500 [05:49<01:37,  1.27it/s] 76%|███████▌  | 379/500 [05:49<01:09,  1.75it/s] 76%|███████▌  | 381/500 [05:55<02:34,  1.30s/it] 77%|███████▋  | 383/500 [05:55<01:48,  1.08it/s] 77%|███████▋  | 385/500 [05:58<02:07,  1.11s/it] 77%|███████▋  | 387/500 [05:58<01:30,  1.25it/s] 78%|███████▊  | 389/500 [05:58<01:04,  1.73it/s] 78%|███████▊  | 391/500 [06:04<02:21,  1.30s/it] 79%|███████▊  | 393/500 [06:04<01:39,  1.08it/s] 79%|███████▉  | 395/500 [06:07<01:55,  1.10s/it] 79%|███████▉  | 397/500 [06:07<01:21,  1.26it/s] 80%|███████▉  | 399/500 [06:07<00:57,  1.74it/s] 80%|████████  | 401/500 [06:13<02:08,  1.30s/it] 81%|████████  | 403/500 [06:14<01:29,  1.08it/s] 81%|████████  | 405/500 [06:17<01:45,  1.11s/it] 81%|████████▏ | 407/500 [06:17<01:13,  1.26it/s] 82%|████████▏ | 409/500 [06:17<00:52,  1.73it/s] 82%|████████▏ | 411/500 [06:23<01:56,  1.31s/it] 83%|████████▎ | 413/500 [06:23<01:21,  1.07it/s] 83%|████████▎ | 415/500 [06:26<01:34,  1.11s/it] 83%|████████▎ | 417/500 [06:26<01:06,  1.26it/s]Epoch:  359  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  360  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  361  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  362  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  363  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  364  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  365  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  366  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  367  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  368  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  369  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  370  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  371  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  372  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  373  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510004222393036
Epoch:  374  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  375  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  376  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  377  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  378  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  379  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  380  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  381  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  382  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  383  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  384  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  385  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  386  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  387  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510004222393036
Epoch:  388  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  389  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  390  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  391  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  392  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  393  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  394  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  395  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  396  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  397  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  398  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  399  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  400  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  401  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  402  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  403  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  404  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  405  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  406  	Training Loss: 0.13260148465633392
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  407  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  408  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  409  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  410  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  411  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  412  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  413  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  414  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  415  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  416  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  417  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
 84%|████████▍ | 419/500 [06:26<00:46,  1.74it/s] 84%|████████▍ | 421/500 [06:32<01:42,  1.30s/it] 85%|████████▍ | 423/500 [06:32<01:11,  1.07it/s] 85%|████████▌ | 425/500 [06:35<01:22,  1.11s/it] 85%|████████▌ | 427/500 [06:36<00:58,  1.26it/s] 86%|████████▌ | 429/500 [06:36<00:40,  1.74it/s] 86%|████████▌ | 431/500 [06:42<01:29,  1.30s/it] 87%|████████▋ | 433/500 [06:42<01:02,  1.08it/s] 87%|████████▋ | 435/500 [06:45<01:12,  1.11s/it] 87%|████████▋ | 437/500 [06:45<00:50,  1.26it/s] 88%|████████▊ | 439/500 [06:45<00:35,  1.74it/s] 88%|████████▊ | 441/500 [06:51<01:16,  1.30s/it] 89%|████████▊ | 443/500 [06:51<00:52,  1.08it/s] 89%|████████▉ | 445/500 [06:54<01:00,  1.11s/it] 89%|████████▉ | 447/500 [06:54<00:42,  1.26it/s] 90%|████████▉ | 449/500 [06:55<00:29,  1.73it/s] 90%|█████████ | 451/500 [07:01<01:03,  1.30s/it] 91%|█████████ | 453/500 [07:01<00:43,  1.07it/s] 91%|█████████ | 455/500 [07:04<00:50,  1.12s/it] 91%|█████████▏| 457/500 [07:04<00:34,  1.25it/s] 92%|█████████▏| 459/500 [07:04<00:23,  1.73it/s] 92%|█████████▏| 461/500 [07:10<00:51,  1.31s/it] 93%|█████████▎| 463/500 [07:10<00:34,  1.07it/s] 93%|█████████▎| 465/500 [07:13<00:38,  1.11s/it] 93%|█████████▎| 467/500 [07:13<00:26,  1.25it/s] 94%|█████████▍| 469/500 [07:14<00:17,  1.73it/s] 94%|█████████▍| 471/500 [07:20<00:37,  1.31s/it] 95%|█████████▍| 473/500 [07:20<00:25,  1.07it/s] 95%|█████████▌| 475/500 [07:23<00:27,  1.11s/it]Epoch:  418  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  419  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  420  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  421  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  422  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  423  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  424  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510004222393036
Epoch:  425  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  426  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  427  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  428  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  429  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  430  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  431  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  432  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  433  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  434  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  435  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  436  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  437  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510004222393036
Epoch:  438  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510004222393036
Epoch:  439  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  440  	Training Loss: 0.13260148465633392
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  441  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  442  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  443  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  444  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  445  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  446  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  447  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  448  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  449  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  450  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  451  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  452  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510004222393036
Epoch:  453  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  454  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  455  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  456  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  457  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  458  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  459  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  460  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  461  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  462  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  463  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  464  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  465  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  466  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  467  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  468  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  469  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  470  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  471  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  472  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  473  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  474  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  475  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  476  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
 95%|█████████▌| 477/500 [07:23<00:18,  1.25it/s] 96%|█████████▌| 479/500 [07:23<00:12,  1.73it/s] 96%|█████████▌| 481/500 [07:29<00:24,  1.30s/it] 97%|█████████▋| 483/500 [07:29<00:15,  1.08it/s] 97%|█████████▋| 485/500 [07:32<00:16,  1.11s/it] 97%|█████████▋| 487/500 [07:32<00:10,  1.26it/s] 98%|█████████▊| 489/500 [07:32<00:06,  1.73it/s] 98%|█████████▊| 491/500 [07:38<00:11,  1.30s/it] 99%|█████████▊| 493/500 [07:38<00:06,  1.08it/s] 99%|█████████▉| 495/500 [07:42<00:05,  1.11s/it] 99%|█████████▉| 497/500 [07:42<00:02,  1.26it/s]100%|█████████▉| 499/500 [07:42<00:00,  1.74it/s]100%|██████████| 500/500 [07:45<00:00,  1.07it/s]
Epoch:  477  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  478  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  479  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  480  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  481  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  482  	Training Loss: 0.13260148465633392
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  483  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  484  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  485  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  486  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  487  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  488  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  489  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  490  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  491  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  492  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  493  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  494  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510004222393036
Epoch:  495  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
Epoch:  496  	Training Loss: 0.13260149955749512
Test Loss:  0.16309306025505066
Valid Loss:  0.14510005712509155
Epoch:  497  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  498  	Training Loss: 0.13260149955749512
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
Epoch:  499  	Training Loss: 0.13260149955749512
Test Loss:  0.16309309005737305
Valid Loss:  0.14510005712509155
Epoch:  500  	Training Loss: 0.13260148465633392
Test Loss:  0.16309307515621185
Valid Loss:  0.14510005712509155
**************************************************learning rate decay**************************************************
seed is  5
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:02<24:35,  2.96s/it]  1%|          | 3/500 [00:03<06:44,  1.23it/s]  1%|          | 5/500 [00:03<03:32,  2.33it/s]  1%|▏         | 7/500 [00:03<02:15,  3.65it/s]  2%|▏         | 9/500 [00:03<01:35,  5.13it/s]  2%|▏         | 11/500 [00:06<05:26,  1.50it/s]  3%|▎         | 13/500 [00:06<03:47,  2.14it/s]  3%|▎         | 15/500 [00:09<06:30,  1.24it/s]  3%|▎         | 17/500 [00:09<04:35,  1.75it/s]  4%|▍         | 19/500 [00:09<03:18,  2.42it/s]  4%|▍         | 21/500 [00:15<09:31,  1.19s/it]  5%|▍         | 23/500 [00:16<06:45,  1.18it/s]  5%|▌         | 25/500 [00:19<08:23,  1.06s/it]  5%|▌         | 27/500 [00:19<05:59,  1.31it/s]  6%|▌         | 29/500 [00:19<04:19,  1.82it/s]  6%|▌         | 31/500 [00:25<10:01,  1.28s/it]  7%|▋         | 33/500 [00:25<07:08,  1.09it/s]  7%|▋         | 35/500 [00:28<08:34,  1.11s/it]  7%|▋         | 37/500 [00:28<06:07,  1.26it/s]  8%|▊         | 39/500 [00:28<04:24,  1.74it/s]  8%|▊         | 41/500 [00:34<10:00,  1.31s/it]  9%|▊         | 43/500 [00:34<07:06,  1.07it/s]  9%|▉         | 45/500 [00:38<08:27,  1.11s/it]  9%|▉         | 47/500 [00:38<06:02,  1.25it/s] 10%|▉         | 49/500 [00:38<04:21,  1.72it/s] 10%|█         | 51/500 [00:44<09:44,  1.30s/it] 11%|█         | 53/500 [00:44<06:56,  1.07it/s] 11%|█         | 55/500 [00:47<08:13,  1.11s/it] 11%|█▏        | 57/500 [00:47<05:52,  1.26it/s] 12%|█▏        | 59/500 [00:47<04:14,  1.74it/s] 12%|█▏        | 61/500 [00:53<09:40,  1.32s/it]Epoch:  1  	Training Loss: 0.10402712225914001
Test Loss:  52.65979766845703
Valid Loss:  51.15448760986328
Epoch:  2  	Training Loss: 50.42340087890625
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  3  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  4  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  5  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  6  	Training Loss: 0.09410630911588669
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  7  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  8  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  9  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  10  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  11  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  12  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  13  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  14  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  15  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  16  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  17  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  18  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  19  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  20  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  21  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  22  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  23  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  24  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  25  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  26  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  27  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  28  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  29  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  30  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  31  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  32  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  33  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  34  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  35  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  36  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  37  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  38  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  39  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  40  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  41  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  42  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  43  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  44  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  45  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  46  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  47  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  48  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  49  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  50  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  51  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  52  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  53  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  54  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  55  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  56  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  57  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  58  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  59  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  60  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  61  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  62  	Training Loss: 0.09410630911588669
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
 13%|█▎        | 63/500 [00:53<06:52,  1.06it/s] 13%|█▎        | 65/500 [00:57<08:07,  1.12s/it] 13%|█▎        | 67/500 [00:57<05:48,  1.24it/s] 14%|█▍        | 69/500 [00:57<04:11,  1.71it/s] 14%|█▍        | 71/500 [01:03<09:20,  1.31s/it] 15%|█▍        | 73/500 [01:03<06:39,  1.07it/s] 15%|█▌        | 75/500 [01:06<07:53,  1.11s/it] 15%|█▌        | 77/500 [01:06<05:38,  1.25it/s] 16%|█▌        | 79/500 [01:06<04:03,  1.73it/s] 16%|█▌        | 81/500 [01:12<09:03,  1.30s/it] 17%|█▋        | 83/500 [01:12<06:26,  1.08it/s] 17%|█▋        | 85/500 [01:15<07:40,  1.11s/it] 17%|█▋        | 87/500 [01:16<05:29,  1.25it/s] 18%|█▊        | 89/500 [01:16<03:57,  1.73it/s] 18%|█▊        | 91/500 [01:22<08:50,  1.30s/it] 19%|█▊        | 93/500 [01:22<06:17,  1.08it/s] 19%|█▉        | 95/500 [01:25<07:27,  1.11s/it] 19%|█▉        | 97/500 [01:25<05:19,  1.26it/s] 20%|█▉        | 99/500 [01:25<03:50,  1.74it/s] 20%|██        | 101/500 [01:31<08:36,  1.29s/it] 21%|██        | 103/500 [01:31<06:07,  1.08it/s] 21%|██        | 105/500 [01:34<07:17,  1.11s/it] 21%|██▏       | 107/500 [01:34<05:12,  1.26it/s] 22%|██▏       | 109/500 [01:34<03:45,  1.74it/s] 22%|██▏       | 111/500 [01:40<08:23,  1.30s/it] 23%|██▎       | 113/500 [01:41<05:58,  1.08it/s] 23%|██▎       | 115/500 [01:44<07:06,  1.11s/it] 23%|██▎       | 117/500 [01:44<05:04,  1.26it/s] 24%|██▍       | 119/500 [01:44<03:39,  1.74it/s] 24%|██▍       | 121/500 [01:50<08:11,  1.30s/it]Epoch:  63  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  64  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  65  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  66  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  67  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  68  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  69  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  70  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  71  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  72  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  73  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  74  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  75  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  76  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  77  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  78  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  79  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  80  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  81  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  82  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  83  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  84  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  85  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  86  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  87  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  88  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  89  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  90  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  91  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  92  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  93  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  94  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  95  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  96  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  97  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  98  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  99  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  100  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  101  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  102  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  103  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  104  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  105  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  106  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  107  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  108  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  109  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  110  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  111  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  112  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  113  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  114  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  115  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  116  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  117  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  118  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  119  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  120  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  121  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  122  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:   25%|██▍       | 123/500 [01:50<05:49,  1.08it/s] 25%|██▌       | 125/500 [01:53<06:55,  1.11s/it] 25%|██▌       | 127/500 [01:53<04:56,  1.26it/s] 26%|██▌       | 129/500 [01:53<03:33,  1.74it/s] 26%|██▌       | 131/500 [01:59<07:56,  1.29s/it] 27%|██▋       | 133/500 [01:59<05:39,  1.08it/s] 27%|██▋       | 135/500 [02:02<06:43,  1.11s/it] 27%|██▋       | 137/500 [02:03<04:48,  1.26it/s] 28%|██▊       | 139/500 [02:03<03:27,  1.74it/s] 28%|██▊       | 141/500 [02:09<07:44,  1.29s/it] 29%|██▊       | 143/500 [02:09<05:30,  1.08it/s] 29%|██▉       | 145/500 [02:12<06:31,  1.10s/it] 29%|██▉       | 147/500 [02:12<04:39,  1.26it/s] 30%|██▉       | 149/500 [02:12<03:21,  1.74it/s] 30%|███       | 151/500 [02:18<07:38,  1.31s/it] 31%|███       | 153/500 [02:18<05:26,  1.06it/s] 31%|███       | 155/500 [02:21<06:24,  1.11s/it] 31%|███▏      | 157/500 [02:21<04:34,  1.25it/s] 32%|███▏      | 159/500 [02:22<03:17,  1.73it/s] 32%|███▏      | 161/500 [02:27<07:20,  1.30s/it] 33%|███▎      | 163/500 [02:28<05:12,  1.08it/s] 33%|███▎      | 165/500 [02:31<06:15,  1.12s/it] 33%|███▎      | 167/500 [02:31<04:28,  1.24it/s] 34%|███▍      | 169/500 [02:31<03:13,  1.71it/s] 34%|███▍      | 171/500 [02:37<07:08,  1.30s/it] 35%|███▍      | 173/500 [02:37<05:05,  1.07it/s] 35%|███▌      | 175/500 [02:40<06:06,  1.13s/it] 35%|███▌      | 177/500 [02:40<04:21,  1.24it/s] 36%|███▌      | 179/500 [02:41<03:07,  1.71it/s] 36%|███▌      | 181/500 [02:47<06:59,  1.31s/it]0.12549248337745667
Epoch:  123  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  124  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  125  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  126  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  127  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  128  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  129  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  130  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  131  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549251317977905
Epoch:  132  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  133  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  134  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  135  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  136  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  137  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  138  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  139  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  140  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  141  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  142  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  143  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  144  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  145  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  146  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  147  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  148  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  149  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  150  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  151  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  152  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  153  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  154  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  155  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  156  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  157  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  158  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  159  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  160  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  161  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  162  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  163  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  164  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  165  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  166  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  167  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  168  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  169  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  170  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  171  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  172  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  173  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  174  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  175  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  176  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  177  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  178  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  179  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  180  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  181  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
 37%|███▋      | 183/500 [02:47<04:57,  1.06it/s] 37%|███▋      | 185/500 [02:50<05:50,  1.11s/it] 37%|███▋      | 187/500 [02:50<04:10,  1.25it/s] 38%|███▊      | 189/500 [02:50<03:00,  1.72it/s] 38%|███▊      | 191/500 [02:56<06:41,  1.30s/it] 39%|███▊      | 193/500 [02:56<04:45,  1.07it/s] 39%|███▉      | 195/500 [02:59<05:37,  1.11s/it] 39%|███▉      | 197/500 [02:59<04:00,  1.26it/s] 40%|███▉      | 199/500 [02:59<02:52,  1.74it/s] 40%|████      | 201/500 [03:05<06:28,  1.30s/it] 41%|████      | 203/500 [03:05<04:35,  1.08it/s] 41%|████      | 205/500 [03:09<05:25,  1.10s/it] 41%|████▏     | 207/500 [03:09<03:52,  1.26it/s] 42%|████▏     | 209/500 [03:09<02:47,  1.74it/s] 42%|████▏     | 211/500 [03:15<06:15,  1.30s/it] 43%|████▎     | 213/500 [03:15<04:26,  1.08it/s] 43%|████▎     | 215/500 [03:18<05:14,  1.10s/it] 43%|████▎     | 217/500 [03:18<03:43,  1.26it/s] 44%|████▍     | 219/500 [03:18<02:40,  1.75it/s] 44%|████▍     | 221/500 [03:24<06:06,  1.31s/it] 45%|████▍     | 223/500 [03:24<04:19,  1.07it/s] 45%|████▌     | 225/500 [03:28<05:10,  1.13s/it] 45%|████▌     | 227/500 [03:28<03:41,  1.23it/s] 46%|████▌     | 229/500 [03:28<02:38,  1.71it/s] 46%|████▌     | 231/500 [03:34<05:52,  1.31s/it] 47%|████▋     | 233/500 [03:34<04:10,  1.07it/s] 47%|████▋     | 235/500 [03:37<04:55,  1.11s/it] 47%|████▋     | 237/500 [03:37<03:30,  1.25it/s] 48%|████▊     | 239/500 [03:37<02:30,  1.73it/s]Epoch:  182  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  183  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  184  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  185  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  186  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  187  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  188  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  189  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  190  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  191  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  192  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  193  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  194  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  195  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  196  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  197  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  198  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  199  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  200  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  201  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  202  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  203  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  204  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  205  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  206  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  207  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  208  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  209  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  210  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  211  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  212  	Training Loss: 0.09410630911588669
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  213  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  214  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  215  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  216  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549251317977905
Epoch:  217  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  218  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  219  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  220  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  221  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  222  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  223  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  224  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  225  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  226  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  227  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  228  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  229  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  230  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  231  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  232  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  233  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  234  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  235  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  236  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  237  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  238  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  239  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  240  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
 48%|████▊     | 241/500 [03:43<05:36,  1.30s/it] 49%|████▊     | 243/500 [03:43<03:58,  1.08it/s] 49%|████▉     | 245/500 [03:46<04:42,  1.11s/it] 49%|████▉     | 247/500 [03:46<03:21,  1.26it/s] 50%|████▉     | 249/500 [03:47<02:24,  1.74it/s] 50%|█████     | 251/500 [03:53<05:25,  1.31s/it] 51%|█████     | 253/500 [03:53<03:50,  1.07it/s] 51%|█████     | 255/500 [03:56<04:31,  1.11s/it] 51%|█████▏    | 257/500 [03:56<03:13,  1.26it/s] 52%|█████▏    | 259/500 [03:56<02:18,  1.74it/s] 52%|█████▏    | 261/500 [04:02<05:09,  1.30s/it] 53%|█████▎    | 263/500 [04:02<03:39,  1.08it/s] 53%|█████▎    | 265/500 [04:05<04:19,  1.11s/it] 53%|█████▎    | 267/500 [04:05<03:05,  1.26it/s] 54%|█████▍    | 269/500 [04:05<02:13,  1.74it/s] 54%|█████▍    | 271/500 [04:11<04:56,  1.30s/it] 55%|█████▍    | 273/500 [04:12<03:30,  1.08it/s] 55%|█████▌    | 275/500 [04:15<04:08,  1.10s/it] 55%|█████▌    | 277/500 [04:15<02:56,  1.26it/s] 56%|█████▌    | 279/500 [04:15<02:06,  1.75it/s] 56%|█████▌    | 281/500 [04:21<04:43,  1.30s/it] 57%|█████▋    | 283/500 [04:21<03:20,  1.08it/s] 57%|█████▋    | 285/500 [04:24<03:57,  1.11s/it] 57%|█████▋    | 287/500 [04:24<02:49,  1.26it/s] 58%|█████▊    | 289/500 [04:24<02:01,  1.73it/s] 58%|█████▊    | 291/500 [04:30<04:30,  1.30s/it] 59%|█████▊    | 293/500 [04:30<03:12,  1.08it/s] 59%|█████▉    | 295/500 [04:33<03:46,  1.10s/it] 59%|█████▉    | 297/500 [04:33<02:40,  1.26it/s] 60%|█████▉    | 299/500 [04:34<01:55,  1.74it/s]Epoch:  241  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  242  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  243  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  244  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  245  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  246  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  247  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  248  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  249  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  250  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  251  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  252  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  253  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  254  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  255  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  256  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  257  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  258  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  259  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  260  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  261  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  262  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  263  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  264  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  265  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  266  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  267  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  268  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  269  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  270  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  271  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  272  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  273  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  274  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  275  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  276  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  277  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  278  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  279  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  280  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  281  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  282  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  283  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  284  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  285  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  286  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  287  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  288  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  289  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  290  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  291  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  292  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  293  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  294  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  295  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  296  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  297  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  298  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  299  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  300  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
 60%|██████    | 301/500 [04:40<04:17,  1.30s/it] 61%|██████    | 303/500 [04:40<03:02,  1.08it/s] 61%|██████    | 305/500 [04:43<03:35,  1.11s/it] 61%|██████▏   | 307/500 [04:43<02:33,  1.26it/s] 62%|██████▏   | 309/500 [04:43<01:50,  1.73it/s] 62%|██████▏   | 311/500 [04:49<04:05,  1.30s/it] 63%|██████▎   | 313/500 [04:49<02:53,  1.08it/s] 63%|██████▎   | 315/500 [04:52<03:24,  1.10s/it] 63%|██████▎   | 317/500 [04:52<02:25,  1.26it/s] 64%|██████▍   | 319/500 [04:52<01:43,  1.74it/s] 64%|██████▍   | 321/500 [04:58<03:52,  1.30s/it] 65%|██████▍   | 323/500 [04:59<02:44,  1.08it/s] 65%|██████▌   | 325/500 [05:02<03:13,  1.11s/it] 65%|██████▌   | 327/500 [05:02<02:17,  1.26it/s] 66%|██████▌   | 329/500 [05:02<01:38,  1.73it/s] 66%|██████▌   | 331/500 [05:08<03:39,  1.30s/it] 67%|██████▋   | 333/500 [05:08<02:35,  1.07it/s] 67%|██████▋   | 335/500 [05:11<03:02,  1.11s/it] 67%|██████▋   | 337/500 [05:11<02:09,  1.26it/s] 68%|██████▊   | 339/500 [05:11<01:32,  1.74it/s] 68%|██████▊   | 341/500 [05:17<03:26,  1.30s/it] 69%|██████▊   | 343/500 [05:17<02:25,  1.08it/s] 69%|██████▉   | 345/500 [05:20<02:51,  1.11s/it] 69%|██████▉   | 347/500 [05:21<02:01,  1.26it/s] 70%|██████▉   | 349/500 [05:21<01:26,  1.74it/s] 70%|███████   | 351/500 [05:27<03:12,  1.29s/it] 71%|███████   | 353/500 [05:27<02:16,  1.08it/s] 71%|███████   | 355/500 [05:30<02:39,  1.10s/it] 71%|███████▏  | 357/500 [05:30<01:53,  1.27it/s] 72%|███████▏  | 359/500 [05:30<01:20,  1.75it/s]**************************************************learning rate decay**************************************************
Epoch:  301  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  302  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  303  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  304  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  305  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  306  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  307  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  308  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  309  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  310  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  311  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  312  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  313  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  314  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  315  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  316  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  317  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  318  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  319  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  320  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  321  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  322  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  323  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  324  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  325  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  326  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  327  	Training Loss: 0.09410630166530609
Test Loss:  0.12960252165794373
Valid Loss:  0.12549249827861786
Epoch:  328  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  329  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  330  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  331  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  332  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  333  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  334  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  335  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  336  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  337  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  338  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  339  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  340  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  341  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  342  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  343  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  344  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  345  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  346  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  347  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  348  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  349  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  350  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  351  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  352  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  353  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  354  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  355  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  356  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  357  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  358  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  359  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  360  	Training Loss: 0.09410630166530609
 72%|███████▏  | 361/500 [05:36<02:59,  1.29s/it] 73%|███████▎  | 363/500 [05:36<02:06,  1.08it/s] 73%|███████▎  | 365/500 [05:39<02:29,  1.10s/it] 73%|███████▎  | 367/500 [05:39<01:45,  1.26it/s] 74%|███████▍  | 369/500 [05:39<01:15,  1.74it/s] 74%|███████▍  | 371/500 [05:45<02:46,  1.29s/it] 75%|███████▍  | 373/500 [05:45<01:57,  1.08it/s] 75%|███████▌  | 375/500 [05:49<02:17,  1.10s/it] 75%|███████▌  | 377/500 [05:49<01:37,  1.26it/s] 76%|███████▌  | 379/500 [05:49<01:09,  1.74it/s] 76%|███████▌  | 381/500 [05:55<02:35,  1.30s/it] 77%|███████▋  | 383/500 [05:55<01:48,  1.07it/s] 77%|███████▋  | 385/500 [05:58<02:07,  1.11s/it] 77%|███████▋  | 387/500 [05:58<01:29,  1.26it/s] 78%|███████▊  | 389/500 [05:58<01:03,  1.74it/s] 78%|███████▊  | 391/500 [06:04<02:22,  1.30s/it] 79%|███████▊  | 393/500 [06:04<01:39,  1.07it/s] 79%|███████▉  | 395/500 [06:07<01:57,  1.12s/it] 79%|███████▉  | 397/500 [06:08<01:22,  1.25it/s] 80%|███████▉  | 399/500 [06:08<00:58,  1.72it/s] 80%|████████  | 401/500 [06:14<02:08,  1.30s/it] 81%|████████  | 403/500 [06:14<01:29,  1.08it/s] 81%|████████  | 405/500 [06:17<01:45,  1.11s/it] 81%|████████▏ | 407/500 [06:17<01:13,  1.26it/s] 82%|████████▏ | 409/500 [06:17<00:52,  1.74it/s] 82%|████████▏ | 411/500 [06:23<01:55,  1.29s/it] 83%|████████▎ | 413/500 [06:23<01:20,  1.08it/s] 83%|████████▎ | 415/500 [06:26<01:33,  1.10s/it] 83%|████████▎ | 417/500 [06:26<01:05,  1.27it/s]Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  361  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  362  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  363  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  364  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  365  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  366  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  367  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  368  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  369  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  370  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  371  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  372  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  373  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  374  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  375  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  376  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  377  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  378  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  379  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  380  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  381  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  382  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  383  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  384  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  385  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  386  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  387  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  388  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  389  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  390  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  391  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  392  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  393  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  394  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  395  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  396  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  397  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  398  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  399  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  400  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  401  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  402  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  403  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  404  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  405  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  406  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  407  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  408  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  409  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  410  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  411  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  412  	Training Loss: 0.09410630911588669
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  413  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  414  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  415  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  416  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  417  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  418  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  419  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:   84%|████████▍ | 419/500 [06:26<00:46,  1.75it/s] 84%|████████▍ | 421/500 [06:32<01:42,  1.29s/it] 85%|████████▍ | 423/500 [06:33<01:11,  1.08it/s] 85%|████████▌ | 425/500 [06:36<01:23,  1.12s/it] 85%|████████▌ | 427/500 [06:36<00:58,  1.25it/s] 86%|████████▌ | 429/500 [06:36<00:41,  1.72it/s] 86%|████████▌ | 431/500 [06:42<01:29,  1.30s/it] 87%|████████▋ | 433/500 [06:42<01:02,  1.07it/s] 87%|████████▋ | 435/500 [06:45<01:12,  1.11s/it] 87%|████████▋ | 437/500 [06:45<00:50,  1.25it/s] 88%|████████▊ | 439/500 [06:45<00:35,  1.73it/s] 88%|████████▊ | 441/500 [06:51<01:16,  1.30s/it] 89%|████████▊ | 443/500 [06:51<00:53,  1.07it/s] 89%|████████▉ | 445/500 [06:55<01:01,  1.12s/it] 89%|████████▉ | 447/500 [06:55<00:42,  1.24it/s] 90%|████████▉ | 449/500 [06:55<00:29,  1.71it/s] 90%|█████████ | 451/500 [07:01<01:04,  1.31s/it] 91%|█████████ | 453/500 [07:01<00:43,  1.07it/s] 91%|█████████ | 455/500 [07:04<00:49,  1.11s/it] 91%|█████████▏| 457/500 [07:04<00:34,  1.26it/s] 92%|█████████▏| 459/500 [07:04<00:23,  1.74it/s] 92%|█████████▏| 461/500 [07:10<00:51,  1.33s/it] 93%|█████████▎| 463/500 [07:11<00:35,  1.05it/s] 93%|█████████▎| 465/500 [07:14<00:39,  1.13s/it] 93%|█████████▎| 467/500 [07:14<00:26,  1.23it/s] 94%|█████████▍| 469/500 [07:14<00:18,  1.70it/s] 94%|█████████▍| 471/500 [07:20<00:38,  1.33s/it] 95%|█████████▍| 473/500 [07:20<00:25,  1.05it/s] 95%|█████████▌| 475/500 [07:23<00:28,  1.12s/it] 95%|█████████▌| 477/500 [07:23<00:18,  1.24it/s]0.12549248337745667
Epoch:  420  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  421  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  422  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  423  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  424  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  425  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  426  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  427  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  428  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  429  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  430  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  431  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  432  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  433  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  434  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  435  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  436  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  437  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  438  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  439  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  440  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  441  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  442  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  443  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  444  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  445  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  446  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  447  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  448  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  449  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  450  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  451  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  452  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  453  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  454  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  455  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  456  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  457  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  458  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  459  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  460  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  461  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  462  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  463  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  464  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  465  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  466  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  467  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  468  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  469  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  470  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  471  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  472  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  473  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  474  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  475  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  476  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  477  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  478  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
 96%|█████████▌| 479/500 [07:24<00:12,  1.71it/s] 96%|█████████▌| 481/500 [07:30<00:24,  1.31s/it] 97%|█████████▋| 483/500 [07:30<00:15,  1.07it/s] 97%|█████████▋| 485/500 [07:33<00:16,  1.12s/it] 97%|█████████▋| 487/500 [07:33<00:10,  1.25it/s] 98%|█████████▊| 489/500 [07:33<00:06,  1.72it/s] 98%|█████████▊| 491/500 [07:39<00:11,  1.30s/it] 99%|█████████▊| 493/500 [07:39<00:06,  1.07it/s] 99%|█████████▉| 495/500 [07:42<00:05,  1.11s/it] 99%|█████████▉| 497/500 [07:42<00:02,  1.26it/s]100%|█████████▉| 499/500 [07:42<00:00,  1.73it/s]100%|██████████| 500/500 [07:45<00:00,  1.07it/s]
Epoch:  479  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  480  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  481  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  482  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  483  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  484  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  485  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
**************************************************learning rate decay**************************************************
Epoch:  486  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  487  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  488  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  489  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549249827861786
Epoch:  490  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  491  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  492  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549249827861786
Epoch:  493  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  494  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  495  	Training Loss: 0.09410630911588669
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
Epoch:  496  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
Epoch:  497  	Training Loss: 0.0941062942147255
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  498  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  499  	Training Loss: 0.09410630166530609
Test Loss:  0.1296025514602661
Valid Loss:  0.12549248337745667
Epoch:  500  	Training Loss: 0.09410630166530609
Test Loss:  0.12960253655910492
Valid Loss:  0.12549248337745667
**************************************************learning rate decay**************************************************
seed is  6
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:02<24:51,  2.99s/it]  1%|          | 3/500 [00:03<06:48,  1.22it/s]  1%|          | 5/500 [00:03<03:33,  2.31it/s]  1%|▏         | 7/500 [00:03<02:16,  3.62it/s]  2%|▏         | 9/500 [00:03<01:35,  5.12it/s]  2%|▏         | 11/500 [00:06<05:25,  1.50it/s]  3%|▎         | 13/500 [00:06<03:46,  2.15it/s]  3%|▎         | 15/500 [00:09<06:31,  1.24it/s]  3%|▎         | 17/500 [00:09<04:36,  1.75it/s]  4%|▍         | 19/500 [00:10<03:19,  2.41it/s]  4%|▍         | 21/500 [00:15<09:38,  1.21s/it]  5%|▍         | 23/500 [00:16<06:49,  1.16it/s]  5%|▌         | 25/500 [00:19<08:23,  1.06s/it]  5%|▌         | 27/500 [00:19<05:59,  1.32it/s]  6%|▌         | 29/500 [00:19<04:18,  1.82it/s]  6%|▌         | 31/500 [00:25<09:58,  1.28s/it]  7%|▋         | 33/500 [00:25<07:06,  1.09it/s]  7%|▋         | 35/500 [00:28<08:29,  1.09s/it]  7%|▋         | 37/500 [00:28<06:03,  1.27it/s]  8%|▊         | 39/500 [00:28<04:22,  1.76it/s]  8%|▊         | 41/500 [00:34<09:55,  1.30s/it]  9%|▊         | 43/500 [00:34<07:03,  1.08it/s]  9%|▉         | 45/500 [00:37<08:24,  1.11s/it]  9%|▉         | 47/500 [00:38<06:00,  1.26it/s] 10%|▉         | 49/500 [00:38<04:20,  1.73it/s] 10%|█         | 51/500 [00:44<09:43,  1.30s/it] 11%|█         | 53/500 [00:44<06:55,  1.08it/s] 11%|█         | 55/500 [00:47<08:11,  1.10s/it] 11%|█▏        | 57/500 [00:47<05:50,  1.26it/s] 12%|█▏        | 59/500 [00:47<04:12,  1.74it/s]Epoch:  1  	Training Loss: 0.0319158174097538
Test Loss:  2.6090831756591797
Valid Loss:  2.3772058486938477
Epoch:  2  	Training Loss: 2.8835673332214355
Test Loss:  144.9998779296875
Valid Loss:  139.1568145751953
Epoch:  3  	Training Loss: 146.61575317382812
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  4  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  5  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  6  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  7  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  8  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  9  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  10  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  11  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  12  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  13  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  14  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  15  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  16  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  17  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  18  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  19  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  20  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  21  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  22  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  23  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  24  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  25  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  26  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  27  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  28  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  29  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  30  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  31  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  32  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  33  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  34  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  35  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  36  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  37  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  38  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  39  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  40  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  41  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  42  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  43  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  44  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  45  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  46  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  47  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  48  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  49  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  50  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  51  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  52  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  53  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  54  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  55  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  56  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  57  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  58  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  59  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  60  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  61  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
 12%|█▏        | 61/500 [00:53<09:30,  1.30s/it] 13%|█▎        | 63/500 [00:53<06:45,  1.08it/s] 13%|█▎        | 65/500 [00:56<08:05,  1.12s/it] 13%|█▎        | 67/500 [00:56<05:47,  1.25it/s] 14%|█▍        | 69/500 [00:57<04:10,  1.72it/s] 14%|█▍        | 71/500 [01:03<09:17,  1.30s/it] 15%|█▍        | 73/500 [01:03<06:37,  1.08it/s] 15%|█▌        | 75/500 [01:06<07:51,  1.11s/it] 15%|█▌        | 77/500 [01:06<05:36,  1.26it/s] 16%|█▌        | 79/500 [01:06<04:02,  1.74it/s] 16%|█▌        | 81/500 [01:12<09:03,  1.30s/it] 17%|█▋        | 83/500 [01:12<06:26,  1.08it/s] 17%|█▋        | 85/500 [01:15<07:38,  1.10s/it] 17%|█▋        | 87/500 [01:15<05:27,  1.26it/s] 18%|█▊        | 89/500 [01:15<03:56,  1.74it/s] 18%|█▊        | 91/500 [01:21<08:50,  1.30s/it] 19%|█▊        | 93/500 [01:21<06:18,  1.08it/s] 19%|█▉        | 95/500 [01:25<07:30,  1.11s/it] 19%|█▉        | 97/500 [01:25<05:21,  1.25it/s] 20%|█▉        | 99/500 [01:25<03:51,  1.73it/s] 20%|██        | 101/500 [01:31<08:40,  1.31s/it] 21%|██        | 103/500 [01:31<06:10,  1.07it/s] 21%|██        | 105/500 [01:34<07:20,  1.11s/it] 21%|██▏       | 107/500 [01:34<05:14,  1.25it/s] 22%|██▏       | 109/500 [01:34<03:46,  1.72it/s] 22%|██▏       | 111/500 [01:40<08:30,  1.31s/it] 23%|██▎       | 113/500 [01:40<06:03,  1.06it/s] 23%|██▎       | 115/500 [01:44<07:11,  1.12s/it] 23%|██▎       | 117/500 [01:44<05:07,  1.24it/s] 24%|██▍       | 119/500 [01:44<03:41,  1.72it/s]Valid Loss:  0.022946085780858994
Epoch:  62  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  63  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  64  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  65  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  66  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  67  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  68  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  69  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  70  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  71  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  72  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  73  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  74  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  75  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  76  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  77  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  78  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  79  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  80  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  81  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  82  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  83  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  84  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  85  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  86  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  87  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  88  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  89  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  90  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  91  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  92  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  93  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  94  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  95  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  96  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  97  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  98  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  99  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  100  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  101  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  102  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  103  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  104  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  105  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  106  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  107  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  108  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  109  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  110  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  111  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  112  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  113  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  114  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  115  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  116  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  117  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  118  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  119  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  120  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
 24%|██▍       | 121/500 [01:50<08:18,  1.32s/it] 25%|██▍       | 123/500 [01:50<05:54,  1.06it/s] 25%|██▌       | 125/500 [01:50<04:14,  1.48it/s] 25%|██▌       | 127/500 [01:50<03:04,  2.03it/s] 26%|██▌       | 129/500 [01:50<02:15,  2.74it/s] 26%|██▌       | 131/500 [01:56<07:06,  1.15s/it] 27%|██▋       | 133/500 [01:57<05:03,  1.21it/s] 27%|██▋       | 135/500 [02:00<06:20,  1.04s/it] 27%|██▋       | 137/500 [02:00<04:32,  1.33it/s] 28%|██▊       | 139/500 [02:00<03:16,  1.84it/s] 28%|██▊       | 141/500 [02:06<07:40,  1.28s/it] 29%|██▊       | 143/500 [02:06<05:27,  1.09it/s] 29%|██▉       | 145/500 [02:09<06:32,  1.11s/it] 29%|██▉       | 147/500 [02:09<04:39,  1.26it/s] 30%|██▉       | 149/500 [02:09<03:21,  1.74it/s] 30%|███       | 151/500 [02:15<07:35,  1.31s/it] 31%|███       | 153/500 [02:16<05:23,  1.07it/s] 31%|███       | 155/500 [02:19<06:24,  1.12s/it] 31%|███▏      | 157/500 [02:19<04:34,  1.25it/s] 32%|███▏      | 159/500 [02:19<03:17,  1.72it/s] 32%|███▏      | 161/500 [02:25<07:20,  1.30s/it] 33%|███▎      | 163/500 [02:25<05:13,  1.07it/s] 33%|███▎      | 165/500 [02:28<06:15,  1.12s/it] 33%|███▎      | 167/500 [02:28<04:27,  1.24it/s] 34%|███▍      | 169/500 [02:28<03:12,  1.72it/s] 34%|███▍      | 171/500 [02:34<07:08,  1.30s/it] 35%|███▍      | 173/500 [02:34<05:04,  1.07it/s] 35%|███▌      | 175/500 [02:37<06:00,  1.11s/it] 35%|███▌      | 177/500 [02:38<04:18,  1.25it/s]**************************************************learning rate decay**************************************************
Epoch:  121  	Training Loss: 0.032413676381111145
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  122  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  123  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  124  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  125  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  126  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  127  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  128  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  129  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  130  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  131  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  132  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  133  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  134  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  135  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  136  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  137  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  138  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  139  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  140  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  141  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  142  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  143  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  144  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  145  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  146  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  147  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  148  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  149  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  150  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  151  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  152  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  153  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  154  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  155  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  156  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  157  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  158  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  159  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  160  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  161  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  162  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  163  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  164  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  165  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  166  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  167  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  168  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  169  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  170  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  171  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  172  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  173  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  174  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  175  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  176  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  177  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  178  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  179  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:   36%|███▌      | 179/500 [02:38<03:05,  1.73it/s] 36%|███▌      | 181/500 [02:44<07:01,  1.32s/it] 37%|███▋      | 183/500 [02:44<04:59,  1.06it/s] 37%|███▋      | 185/500 [02:47<05:51,  1.12s/it] 37%|███▋      | 187/500 [02:47<04:10,  1.25it/s] 38%|███▊      | 189/500 [02:47<03:00,  1.73it/s] 38%|███▊      | 191/500 [02:53<06:46,  1.31s/it] 39%|███▊      | 193/500 [02:53<04:48,  1.06it/s] 39%|███▉      | 195/500 [02:57<05:42,  1.12s/it] 39%|███▉      | 197/500 [02:57<04:04,  1.24it/s] 40%|███▉      | 199/500 [02:57<02:55,  1.71it/s] 40%|████      | 201/500 [03:03<06:35,  1.32s/it] 41%|████      | 203/500 [03:03<04:40,  1.06it/s] 41%|████      | 205/500 [03:06<05:31,  1.12s/it] 41%|████▏     | 207/500 [03:06<03:56,  1.24it/s] 42%|████▏     | 209/500 [03:06<02:49,  1.71it/s] 42%|████▏     | 211/500 [03:12<06:19,  1.31s/it] 43%|████▎     | 213/500 [03:13<04:29,  1.06it/s] 43%|████▎     | 215/500 [03:16<05:20,  1.12s/it] 43%|████▎     | 217/500 [03:16<03:48,  1.24it/s] 44%|████▍     | 219/500 [03:16<02:44,  1.71it/s] 44%|████▍     | 221/500 [03:22<06:04,  1.31s/it] 45%|████▍     | 223/500 [03:22<04:19,  1.07it/s] 45%|████▌     | 225/500 [03:25<05:05,  1.11s/it] 45%|████▌     | 227/500 [03:25<03:37,  1.25it/s] 46%|████▌     | 229/500 [03:25<02:36,  1.73it/s] 46%|████▌     | 231/500 [03:31<05:52,  1.31s/it] 47%|████▋     | 233/500 [03:32<04:10,  1.07it/s] 47%|████▋     | 235/500 [03:35<04:55,  1.11s/it]0.022946085780858994
Epoch:  180  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  181  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  182  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  183  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  184  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  185  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  186  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  187  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  188  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  189  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  190  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  191  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  192  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  193  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  194  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  195  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  196  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  197  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  198  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  199  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  200  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  201  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  202  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  203  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  204  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  205  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  206  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  207  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  208  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  209  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  210  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  211  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  212  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  213  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  214  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  215  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  216  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  217  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  218  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  219  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  220  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  221  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  222  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  223  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  224  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  225  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  226  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  227  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  228  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  229  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  230  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  231  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  232  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  233  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  234  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  235  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  236  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  237  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
 47%|████▋     | 237/500 [03:35<03:30,  1.25it/s] 48%|████▊     | 239/500 [03:35<02:31,  1.72it/s] 48%|████▊     | 241/500 [03:41<05:36,  1.30s/it] 49%|████▊     | 243/500 [03:41<03:58,  1.08it/s] 49%|████▉     | 245/500 [03:44<04:41,  1.10s/it] 49%|████▉     | 247/500 [03:44<03:20,  1.26it/s] 50%|████▉     | 249/500 [03:44<02:23,  1.75it/s] 50%|█████     | 251/500 [03:50<05:24,  1.30s/it] 51%|█████     | 253/500 [03:50<03:49,  1.07it/s] 51%|█████     | 255/500 [03:53<04:32,  1.11s/it] 51%|█████▏    | 257/500 [03:54<03:14,  1.25it/s] 52%|█████▏    | 259/500 [03:54<02:19,  1.73it/s] 52%|█████▏    | 261/500 [04:00<05:09,  1.30s/it] 53%|█████▎    | 263/500 [04:00<03:39,  1.08it/s] 53%|█████▎    | 265/500 [04:03<04:20,  1.11s/it] 53%|█████▎    | 267/500 [04:03<03:05,  1.26it/s] 54%|█████▍    | 269/500 [04:03<02:12,  1.74it/s] 54%|█████▍    | 271/500 [04:06<03:17,  1.16it/s] 55%|█████▍    | 273/500 [04:06<02:21,  1.60it/s] 55%|█████▌    | 275/500 [04:09<03:20,  1.12it/s] 55%|█████▌    | 277/500 [04:09<02:23,  1.56it/s] 56%|█████▌    | 279/500 [04:10<01:43,  2.14it/s] 56%|█████▌    | 281/500 [04:15<04:26,  1.22s/it] 57%|█████▋    | 283/500 [04:16<03:09,  1.15it/s] 57%|█████▋    | 285/500 [04:19<03:49,  1.07s/it] 57%|█████▋    | 287/500 [04:19<02:43,  1.30it/s] 58%|█████▊    | 289/500 [04:19<01:57,  1.80it/s] 58%|█████▊    | 291/500 [04:25<04:28,  1.28s/it] 59%|█████▊    | 293/500 [04:25<03:10,  1.09it/s] 59%|█████▉    | 295/500 [04:28<03:45,  1.10s/it]Valid Loss:  0.022946085780858994
Epoch:  238  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  239  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  240  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  241  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  242  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  243  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  244  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  245  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  246  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  247  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  248  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  249  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  250  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  251  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  252  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  253  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  254  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  255  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  256  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  257  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  258  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  259  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  260  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  261  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  262  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  263  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  264  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  265  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  266  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  267  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  268  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  269  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  270  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  271  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  272  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  273  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  274  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  275  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  276  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  277  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  278  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  279  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  280  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  281  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  282  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  283  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  284  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  285  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  286  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  287  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  288  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  289  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  290  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  291  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  292  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  293  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  294  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  295  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  296  	Training Loss: 0.032413680106401443
Test Loss:   59%|█████▉    | 297/500 [04:28<02:39,  1.27it/s] 60%|█████▉    | 299/500 [04:28<01:54,  1.75it/s] 60%|██████    | 301/500 [04:34<04:17,  1.29s/it] 61%|██████    | 303/500 [04:34<03:01,  1.08it/s] 61%|██████    | 305/500 [04:37<03:34,  1.10s/it] 61%|██████▏   | 307/500 [04:38<02:33,  1.26it/s] 62%|██████▏   | 309/500 [04:38<01:49,  1.74it/s] 62%|██████▏   | 311/500 [04:44<04:05,  1.30s/it] 63%|██████▎   | 313/500 [04:44<02:54,  1.07it/s] 63%|██████▎   | 315/500 [04:47<03:24,  1.11s/it] 63%|██████▎   | 317/500 [04:47<02:25,  1.26it/s] 64%|██████▍   | 319/500 [04:47<01:44,  1.74it/s] 64%|██████▍   | 321/500 [04:53<03:52,  1.30s/it] 65%|██████▍   | 323/500 [04:53<02:44,  1.08it/s] 65%|██████▌   | 325/500 [04:56<03:14,  1.11s/it] 65%|██████▌   | 327/500 [04:56<02:17,  1.25it/s] 66%|██████▌   | 329/500 [04:57<01:38,  1.73it/s] 66%|██████▌   | 331/500 [05:03<03:39,  1.30s/it] 67%|██████▋   | 333/500 [05:03<02:35,  1.08it/s] 67%|██████▋   | 335/500 [05:06<03:02,  1.11s/it] 67%|██████▋   | 337/500 [05:06<02:09,  1.26it/s] 68%|██████▊   | 339/500 [05:06<01:32,  1.74it/s] 68%|██████▊   | 341/500 [05:12<03:25,  1.30s/it] 69%|██████▊   | 343/500 [05:12<02:25,  1.08it/s] 69%|██████▉   | 345/500 [05:15<02:51,  1.11s/it] 69%|██████▉   | 347/500 [05:15<02:02,  1.25it/s] 70%|██████▉   | 349/500 [05:15<01:27,  1.73it/s] 70%|███████   | 351/500 [05:21<03:16,  1.32s/it] 71%|███████   | 353/500 [05:22<02:18,  1.06it/s]0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  297  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  298  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  299  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  300  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  301  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  302  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  303  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  304  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  305  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  306  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  307  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  308  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  309  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  310  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  311  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  312  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  313  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  314  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  315  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  316  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  317  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  318  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  319  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  320  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  321  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  322  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  323  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  324  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  325  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  326  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  327  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  328  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  329  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  330  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  331  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  332  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  333  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  334  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  335  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  336  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  337  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  338  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  339  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  340  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  341  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  342  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  343  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  344  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  345  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  346  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  347  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  348  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  349  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  350  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  351  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  352  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  353  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  354  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  355  	Training Loss: 0.03241368383169174
Test Loss:  71%|███████   | 355/500 [05:25<02:42,  1.12s/it] 71%|███████▏  | 357/500 [05:25<01:54,  1.24it/s] 72%|███████▏  | 359/500 [05:25<01:22,  1.72it/s] 72%|███████▏  | 361/500 [05:31<03:02,  1.31s/it] 73%|███████▎  | 363/500 [05:31<02:08,  1.07it/s] 73%|███████▎  | 365/500 [05:34<02:29,  1.11s/it] 73%|███████▎  | 367/500 [05:34<01:46,  1.25it/s] 74%|███████▍  | 369/500 [05:34<01:15,  1.73it/s] 74%|███████▍  | 371/500 [05:40<02:47,  1.30s/it] 75%|███████▍  | 373/500 [05:40<01:58,  1.08it/s] 75%|███████▌  | 375/500 [05:44<02:18,  1.11s/it] 75%|███████▌  | 377/500 [05:44<01:37,  1.26it/s] 76%|███████▌  | 379/500 [05:44<01:09,  1.73it/s] 76%|███████▌  | 381/500 [05:50<02:34,  1.30s/it] 77%|███████▋  | 383/500 [05:50<01:48,  1.08it/s] 77%|███████▋  | 385/500 [05:53<02:07,  1.11s/it] 77%|███████▋  | 387/500 [05:53<01:29,  1.26it/s] 78%|███████▊  | 389/500 [05:53<01:03,  1.74it/s] 78%|███████▊  | 391/500 [05:59<02:21,  1.30s/it] 79%|███████▊  | 393/500 [05:59<01:39,  1.08it/s] 79%|███████▉  | 395/500 [06:02<01:55,  1.10s/it] 79%|███████▉  | 397/500 [06:02<01:21,  1.27it/s] 80%|███████▉  | 399/500 [06:03<00:57,  1.75it/s] 80%|████████  | 401/500 [06:09<02:07,  1.29s/it] 81%|████████  | 403/500 [06:09<01:29,  1.08it/s] 81%|████████  | 405/500 [06:12<01:44,  1.10s/it] 81%|████████▏ | 407/500 [06:12<01:13,  1.26it/s] 82%|████████▏ | 409/500 [06:12<00:52,  1.74it/s] 82%|████████▏ | 411/500 [06:18<01:55,  1.30s/it] 0.014530927874147892
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  356  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  357  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  358  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  359  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  360  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  361  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  362  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  363  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  364  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  365  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  366  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  367  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  368  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  369  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  370  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  371  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  372  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  373  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  374  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  375  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  376  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  377  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  378  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  379  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  380  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  381  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  382  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  383  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  384  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  385  	Training Loss: 0.032413676381111145
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  386  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  387  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  388  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  389  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  390  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  391  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  392  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  393  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  394  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  395  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  396  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  397  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  398  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  399  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  400  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  401  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  402  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  403  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  404  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  405  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  406  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  407  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  408  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  409  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  410  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  411  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  412  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  413  	Training Loss: 0.03241368383169174
 83%|████████▎ | 413/500 [06:18<01:20,  1.08it/s] 83%|████████▎ | 415/500 [06:21<01:34,  1.11s/it] 83%|████████▎ | 417/500 [06:21<01:05,  1.26it/s] 84%|████████▍ | 419/500 [06:21<00:46,  1.74it/s] 84%|████████▍ | 421/500 [06:27<01:43,  1.30s/it] 85%|████████▍ | 423/500 [06:28<01:11,  1.07it/s] 85%|████████▌ | 425/500 [06:31<01:23,  1.11s/it] 85%|████████▌ | 427/500 [06:31<00:58,  1.25it/s] 86%|████████▌ | 429/500 [06:31<00:41,  1.73it/s] 86%|████████▌ | 431/500 [06:37<01:29,  1.30s/it] 87%|████████▋ | 433/500 [06:37<01:02,  1.08it/s] 87%|████████▋ | 435/500 [06:40<01:11,  1.10s/it] 87%|████████▋ | 437/500 [06:40<00:49,  1.26it/s] 88%|████████▊ | 439/500 [06:40<00:35,  1.74it/s] 88%|████████▊ | 441/500 [06:46<01:16,  1.29s/it] 89%|████████▊ | 443/500 [06:46<00:52,  1.08it/s] 89%|████████▉ | 445/500 [06:49<01:00,  1.11s/it] 89%|████████▉ | 447/500 [06:49<00:42,  1.26it/s] 90%|████████▉ | 449/500 [06:50<00:29,  1.74it/s] 90%|█████████ | 451/500 [06:56<01:03,  1.30s/it] 91%|█████████ | 453/500 [06:56<00:43,  1.07it/s] 91%|█████████ | 455/500 [06:59<00:50,  1.11s/it] 91%|█████████▏| 457/500 [06:59<00:34,  1.25it/s] 92%|█████████▏| 459/500 [06:59<00:23,  1.73it/s] 92%|█████████▏| 461/500 [07:05<00:50,  1.31s/it] 93%|█████████▎| 463/500 [07:05<00:34,  1.07it/s] 93%|█████████▎| 465/500 [07:08<00:39,  1.11s/it] 93%|█████████▎| 467/500 [07:08<00:26,  1.25it/s] 94%|█████████▍| 469/500 [07:09<00:17,  1.72it/s]Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  414  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  415  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  416  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  417  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  418  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  419  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  420  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  421  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  422  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  423  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  424  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  425  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  426  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  427  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  428  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  429  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  430  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  431  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  432  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  433  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  434  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  435  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  436  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  437  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  438  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  439  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  440  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  441  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  442  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  443  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  444  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  445  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  446  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  447  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  448  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  449  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  450  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  451  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  452  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  453  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  454  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  455  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
**************************************************learning rate decay**************************************************
Epoch:  456  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  457  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  458  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  459  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  460  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  461  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  462  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946087643504143
Epoch:  463  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  464  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  465  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  466  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  467  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  468  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  469  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  470  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
 94%|█████████▍| 471/500 [07:15<00:37,  1.30s/it] 95%|█████████▍| 473/500 [07:15<00:25,  1.08it/s] 95%|█████████▌| 475/500 [07:18<00:27,  1.11s/it] 95%|█████████▌| 477/500 [07:18<00:18,  1.26it/s] 96%|█████████▌| 479/500 [07:18<00:12,  1.74it/s] 96%|█████████▌| 481/500 [07:24<00:24,  1.30s/it] 97%|█████████▋| 483/500 [07:24<00:15,  1.08it/s] 97%|█████████▋| 485/500 [07:27<00:16,  1.11s/it] 97%|█████████▋| 487/500 [07:27<00:10,  1.26it/s] 98%|█████████▊| 489/500 [07:27<00:06,  1.73it/s] 98%|█████████▊| 491/500 [07:33<00:11,  1.30s/it] 99%|█████████▊| 493/500 [07:33<00:06,  1.08it/s] 99%|█████████▉| 495/500 [07:37<00:05,  1.11s/it] 99%|█████████▉| 497/500 [07:37<00:02,  1.26it/s]100%|█████████▉| 499/500 [07:37<00:00,  1.74it/s]100%|██████████| 500/500 [07:40<00:00,  1.09it/s]
Epoch:  471  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  472  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  473  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  474  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  475  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  476  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  477  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  478  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  479  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946087643504143
Epoch:  480  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  481  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  482  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
Epoch:  483  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  484  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  485  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  486  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  487  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  488  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  489  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  490  	Training Loss: 0.032413680106401443
Test Loss:  0.014530926942825317
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  491  	Training Loss: 0.032413680106401443
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  492  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  493  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  494  	Training Loss: 0.03241368383169174
Test Loss:  0.014530926942825317
Valid Loss:  0.022946087643504143
Epoch:  495  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
Epoch:  496  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  497  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  498  	Training Loss: 0.032413680106401443
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
Epoch:  499  	Training Loss: 0.03241368383169174
Test Loss:  0.014530928805470467
Valid Loss:  0.022946085780858994
Epoch:  500  	Training Loss: 0.03241368383169174
Test Loss:  0.014530927874147892
Valid Loss:  0.022946085780858994
**************************************************learning rate decay**************************************************
seed is  7
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:03<25:21,  3.05s/it]  1%|          | 3/500 [00:03<06:56,  1.19it/s]  1%|          | 5/500 [00:03<03:37,  2.27it/s]  1%|▏         | 7/500 [00:03<02:18,  3.57it/s]  2%|▏         | 9/500 [00:03<01:37,  5.03it/s]  2%|▏         | 11/500 [00:06<05:23,  1.51it/s]  3%|▎         | 13/500 [00:06<03:46,  2.15it/s]  3%|▎         | 15/500 [00:06<02:42,  2.98it/s]  3%|▎         | 17/500 [00:06<02:00,  4.01it/s]  4%|▍         | 19/500 [00:07<01:32,  5.21it/s]  4%|▍         | 21/500 [00:13<08:24,  1.05s/it]  5%|▍         | 23/500 [00:13<05:58,  1.33it/s]  5%|▌         | 25/500 [00:16<07:49,  1.01it/s]  5%|▌         | 27/500 [00:16<05:35,  1.41it/s]  6%|▌         | 29/500 [00:16<04:02,  1.95it/s]  6%|▌         | 31/500 [00:22<09:47,  1.25s/it]  7%|▋         | 33/500 [00:22<06:57,  1.12it/s]  7%|▋         | 35/500 [00:25<08:23,  1.08s/it]  7%|▋         | 37/500 [00:25<06:01,  1.28it/s]  8%|▊         | 39/500 [00:25<04:20,  1.77it/s]  8%|▊         | 41/500 [00:31<09:50,  1.29s/it]  9%|▊         | 43/500 [00:32<07:00,  1.09it/s]  9%|▉         | 45/500 [00:35<08:20,  1.10s/it]  9%|▉         | 47/500 [00:35<05:57,  1.27it/s] 10%|▉         | 49/500 [00:35<04:17,  1.75it/s] 10%|█         | 51/500 [00:41<09:50,  1.31s/it] 11%|█         | 53/500 [00:41<06:59,  1.06it/s] 11%|█         | 55/500 [00:44<08:16,  1.12s/it] 11%|█▏        | 57/500 [00:44<05:54,  1.25it/s] 12%|█▏        | 59/500 [00:44<04:15,  1.72it/s] 12%|█▏        | 61/500 [00:50<09:36,  1.31s/it] 13%|█▎        | 63/500 [00:51<06:50,  1.06it/s]Epoch:  1  	Training Loss: 0.1344495266675949
Test Loss:  361.2841491699219
Valid Loss:  348.29681396484375
Epoch:  2  	Training Loss: 348.98248291015625
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  3  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  4  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  5  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  6  	Training Loss: 0.15511159598827362
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  7  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  8  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  9  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  10  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  11  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  12  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  13  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  14  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  15  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  16  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  17  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  18  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  19  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  20  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  21  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  22  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  23  	Training Loss: 0.15511159598827362
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  24  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  25  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  26  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  27  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  28  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  29  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  30  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  31  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  32  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  33  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  34  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  35  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  36  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  37  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  38  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  39  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  40  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  41  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  42  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  43  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  44  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  45  	Training Loss: 0.15511159598827362
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  46  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  47  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  48  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  49  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  50  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  51  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  52  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  53  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  54  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  55  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  56  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  57  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  58  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  59  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  60  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  61  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  62  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  63  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
 13%|█▎        | 65/500 [00:54<08:02,  1.11s/it] 13%|█▎        | 67/500 [00:54<05:44,  1.26it/s] 14%|█▍        | 69/500 [00:54<04:08,  1.73it/s] 14%|█▍        | 71/500 [01:00<09:15,  1.29s/it] 15%|█▍        | 73/500 [01:00<06:35,  1.08it/s] 15%|█▌        | 75/500 [01:03<07:50,  1.11s/it] 15%|█▌        | 77/500 [01:03<05:36,  1.26it/s] 16%|█▌        | 79/500 [01:03<04:02,  1.74it/s] 16%|█▌        | 81/500 [01:09<09:04,  1.30s/it] 17%|█▋        | 83/500 [01:09<06:28,  1.07it/s] 17%|█▋        | 85/500 [01:12<07:40,  1.11s/it] 17%|█▋        | 87/500 [01:12<05:28,  1.26it/s] 18%|█▊        | 89/500 [01:13<03:56,  1.73it/s] 18%|█▊        | 91/500 [01:19<09:02,  1.33s/it] 19%|█▊        | 93/500 [01:19<06:25,  1.06it/s] 19%|█▉        | 95/500 [01:22<07:36,  1.13s/it] 19%|█▉        | 97/500 [01:22<05:26,  1.23it/s] 20%|█▉        | 99/500 [01:22<03:55,  1.70it/s] 20%|██        | 101/500 [01:28<08:45,  1.32s/it] 21%|██        | 103/500 [01:28<06:13,  1.06it/s] 21%|██        | 105/500 [01:32<07:22,  1.12s/it] 21%|██▏       | 107/500 [01:32<05:15,  1.24it/s] 22%|██▏       | 109/500 [01:32<03:47,  1.72it/s] 22%|██▏       | 111/500 [01:38<08:26,  1.30s/it] 23%|██▎       | 113/500 [01:38<06:00,  1.07it/s] 23%|██▎       | 115/500 [01:41<07:06,  1.11s/it] 23%|██▎       | 117/500 [01:41<05:05,  1.25it/s] 24%|██▍       | 119/500 [01:41<03:40,  1.73it/s] 24%|██▍       | 121/500 [01:47<08:13,  1.30s/it]Epoch:  64  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  65  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  66  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  67  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  68  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  69  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  70  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  71  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  72  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  73  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  74  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  75  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  76  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  77  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  78  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  79  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734654247760773
Epoch:  80  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  81  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  82  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  83  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  84  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  85  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  86  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  87  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  88  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  89  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  90  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  91  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  92  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  93  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  94  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  95  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  96  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  97  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  98  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  99  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  100  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  101  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  102  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  103  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  104  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  105  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  106  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  107  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  108  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  109  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  110  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  111  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  112  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  113  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  114  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  115  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  116  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  117  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  118  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  119  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  120  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  121  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  122  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  123  	Training Loss: 0.15511158108711243
Test Loss:   25%|██▍       | 123/500 [01:47<05:51,  1.07it/s] 25%|██▌       | 125/500 [01:50<06:56,  1.11s/it] 25%|██▌       | 127/500 [01:50<04:56,  1.26it/s] 26%|██▌       | 129/500 [01:51<03:33,  1.73it/s] 26%|██▌       | 131/500 [01:57<07:57,  1.29s/it] 27%|██▋       | 133/500 [01:57<05:39,  1.08it/s] 27%|██▋       | 135/500 [02:00<06:43,  1.11s/it] 27%|██▋       | 137/500 [02:00<04:48,  1.26it/s] 28%|██▊       | 139/500 [02:00<03:27,  1.74it/s] 28%|██▊       | 141/500 [02:06<07:45,  1.30s/it] 29%|██▊       | 143/500 [02:06<05:31,  1.08it/s] 29%|██▉       | 145/500 [02:09<06:34,  1.11s/it] 29%|██▉       | 147/500 [02:09<04:41,  1.25it/s] 30%|██▉       | 149/500 [02:09<03:22,  1.73it/s] 30%|███       | 151/500 [02:15<07:32,  1.30s/it] 31%|███       | 153/500 [02:16<05:21,  1.08it/s] 31%|███       | 155/500 [02:19<06:20,  1.10s/it] 31%|███▏      | 157/500 [02:19<04:32,  1.26it/s] 32%|███▏      | 159/500 [02:19<03:16,  1.74it/s] 32%|███▏      | 161/500 [02:25<07:20,  1.30s/it] 33%|███▎      | 163/500 [02:25<05:13,  1.08it/s] 33%|███▎      | 165/500 [02:28<06:09,  1.10s/it] 33%|███▎      | 167/500 [02:28<04:23,  1.26it/s] 34%|███▍      | 169/500 [02:28<03:09,  1.74it/s] 34%|███▍      | 171/500 [02:34<07:08,  1.30s/it] 35%|███▍      | 173/500 [02:34<05:04,  1.07it/s] 35%|███▌      | 175/500 [02:37<05:59,  1.11s/it] 35%|███▌      | 177/500 [02:37<04:17,  1.26it/s] 36%|███▌      | 179/500 [02:38<03:05,  1.73it/s] 36%|███▌      | 181/500 [02:44<06:52,  1.29s/it]0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  124  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  125  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  126  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  127  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734651267528534
Epoch:  128  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734654247760773
Epoch:  129  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  130  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  131  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  132  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  133  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  134  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  135  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  136  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  137  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  138  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  139  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  140  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  141  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  142  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  143  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  144  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  145  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  146  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  147  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  148  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  149  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  150  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  151  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  152  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  153  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  154  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  155  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  156  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  157  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  158  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  159  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  160  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  161  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  162  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  163  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  164  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  165  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  166  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  167  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  168  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  169  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  170  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  171  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  172  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  173  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  174  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  175  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  176  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  177  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  178  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  179  	Training Loss: 0.15511159598827362
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  180  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  181  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  182  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
 37%|███▋      | 183/500 [02:44<04:53,  1.08it/s] 37%|███▋      | 185/500 [02:47<05:47,  1.10s/it] 37%|███▋      | 187/500 [02:47<04:07,  1.26it/s] 38%|███▊      | 189/500 [02:47<02:58,  1.74it/s] 38%|███▊      | 191/500 [02:53<06:40,  1.30s/it] 39%|███▊      | 193/500 [02:53<04:45,  1.08it/s] 39%|███▉      | 195/500 [02:56<05:38,  1.11s/it] 39%|███▉      | 197/500 [02:56<04:01,  1.25it/s] 40%|███▉      | 199/500 [02:56<02:53,  1.73it/s] 40%|████      | 201/500 [03:02<06:27,  1.30s/it] 41%|████      | 203/500 [03:03<04:35,  1.08it/s] 41%|████      | 205/500 [03:06<05:26,  1.11s/it] 41%|████▏     | 207/500 [03:06<03:52,  1.26it/s] 42%|████▏     | 209/500 [03:06<02:47,  1.74it/s] 42%|████▏     | 211/500 [03:12<06:20,  1.32s/it] 43%|████▎     | 213/500 [03:12<04:29,  1.06it/s] 43%|████▎     | 215/500 [03:15<05:19,  1.12s/it] 43%|████▎     | 217/500 [03:15<03:47,  1.24it/s] 44%|████▍     | 219/500 [03:15<02:43,  1.72it/s] 44%|████▍     | 221/500 [03:21<06:01,  1.30s/it] 45%|████▍     | 223/500 [03:21<04:17,  1.08it/s] 45%|████▌     | 225/500 [03:24<05:03,  1.10s/it] 45%|████▌     | 227/500 [03:25<03:35,  1.26it/s] 46%|████▌     | 229/500 [03:25<02:35,  1.75it/s] 46%|████▌     | 231/500 [03:31<05:52,  1.31s/it] 47%|████▋     | 233/500 [03:31<04:10,  1.07it/s] 47%|████▋     | 235/500 [03:34<05:03,  1.15s/it] 47%|████▋     | 237/500 [03:34<03:36,  1.22it/s] 48%|████▊     | 239/500 [03:34<02:35,  1.68it/s]Valid Loss:  0.17734652757644653
Epoch:  183  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  184  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  185  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  186  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  187  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  188  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  189  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  190  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  191  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  192  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  193  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  194  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  195  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  196  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  197  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  198  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  199  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  200  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  201  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  202  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  203  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  204  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  205  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  206  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  207  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  208  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  209  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  210  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  211  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  212  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  213  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  214  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  215  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  216  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  217  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  218  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  219  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  220  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  221  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  222  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  223  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  224  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  225  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  226  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  227  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  228  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  229  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  230  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  231  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  232  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  233  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  234  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  235  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  236  	Training Loss: 0.15511159598827362
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  237  	Training Loss: 0.15511159598827362
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  238  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  239  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  240  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  241  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:   48%|████▊     | 241/500 [03:41<05:44,  1.33s/it] 49%|████▊     | 243/500 [03:41<04:04,  1.05it/s] 49%|████▉     | 245/500 [03:44<04:47,  1.13s/it] 49%|████▉     | 247/500 [03:44<03:24,  1.24it/s] 50%|████▉     | 249/500 [03:44<02:26,  1.71it/s] 50%|█████     | 251/500 [03:50<05:25,  1.31s/it] 51%|█████     | 253/500 [03:50<03:50,  1.07it/s] 51%|█████     | 255/500 [03:53<04:34,  1.12s/it] 51%|█████▏    | 257/500 [03:53<03:15,  1.24it/s] 52%|█████▏    | 259/500 [03:53<02:20,  1.72it/s] 52%|█████▏    | 261/500 [03:59<05:11,  1.30s/it] 53%|█████▎    | 263/500 [04:00<03:40,  1.07it/s] 53%|█████▎    | 265/500 [04:03<04:21,  1.11s/it] 53%|█████▎    | 267/500 [04:03<03:05,  1.25it/s] 54%|█████▍    | 269/500 [04:03<02:13,  1.73it/s] 54%|█████▍    | 271/500 [04:09<04:59,  1.31s/it] 55%|█████▍    | 273/500 [04:09<03:32,  1.07it/s] 55%|█████▌    | 275/500 [04:12<04:10,  1.11s/it] 55%|█████▌    | 277/500 [04:12<02:58,  1.25it/s] 56%|█████▌    | 279/500 [04:12<02:08,  1.72it/s] 56%|█████▌    | 281/500 [04:18<04:45,  1.30s/it] 57%|█████▋    | 283/500 [04:18<03:22,  1.07it/s] 57%|█████▋    | 285/500 [04:22<03:58,  1.11s/it] 57%|█████▋    | 287/500 [04:22<02:49,  1.26it/s] 58%|█████▊    | 289/500 [04:22<02:01,  1.74it/s] 58%|█████▊    | 291/500 [04:28<04:33,  1.31s/it] 59%|█████▊    | 293/500 [04:28<03:13,  1.07it/s] 59%|█████▉    | 295/500 [04:31<03:48,  1.11s/it] 59%|█████▉    | 297/500 [04:31<02:42,  1.25it/s] 60%|█████▉    | 299/500 [04:31<01:56,  1.73it/s]0.17734652757644653
Epoch:  242  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  243  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  244  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  245  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  246  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  247  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  248  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  249  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  250  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  251  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  252  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  253  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  254  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  255  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  256  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  257  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  258  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  259  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  260  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  261  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  262  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  263  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  264  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  265  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  266  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  267  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  268  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  269  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  270  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  271  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  272  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  273  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  274  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  275  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  276  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  277  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  278  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  279  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  280  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  281  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  282  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734651267528534
Epoch:  283  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  284  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  285  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  286  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  287  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  288  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  289  	Training Loss: 0.15511159598827362
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  290  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  291  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  292  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734654247760773
Epoch:  293  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734651267528534
Epoch:  294  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  295  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  296  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  297  	Training Loss: 0.15511159598827362
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  298  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  299  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  300  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
 60%|██████    | 301/500 [04:37<04:19,  1.30s/it] 61%|██████    | 303/500 [04:37<03:03,  1.07it/s] 61%|██████    | 305/500 [04:40<03:36,  1.11s/it] 61%|██████▏   | 307/500 [04:41<02:33,  1.26it/s] 62%|██████▏   | 309/500 [04:41<01:49,  1.74it/s] 62%|██████▏   | 311/500 [04:47<04:05,  1.30s/it] 63%|██████▎   | 313/500 [04:47<02:53,  1.08it/s] 63%|██████▎   | 315/500 [04:50<03:23,  1.10s/it] 63%|██████▎   | 317/500 [04:50<02:25,  1.26it/s] 64%|██████▍   | 319/500 [04:50<01:43,  1.74it/s] 64%|██████▍   | 321/500 [04:56<03:51,  1.30s/it] 65%|██████▍   | 323/500 [04:56<02:44,  1.08it/s] 65%|██████▌   | 325/500 [04:59<03:13,  1.11s/it] 65%|██████▌   | 327/500 [04:59<02:17,  1.26it/s] 66%|██████▌   | 329/500 [04:59<01:38,  1.74it/s] 66%|██████▌   | 331/500 [05:05<03:38,  1.30s/it] 67%|██████▋   | 333/500 [05:06<02:34,  1.08it/s] 67%|██████▋   | 335/500 [05:09<03:03,  1.11s/it] 67%|██████▋   | 337/500 [05:09<02:09,  1.25it/s] 68%|██████▊   | 339/500 [05:09<01:32,  1.73it/s] 68%|██████▊   | 341/500 [05:15<03:26,  1.30s/it] 69%|██████▊   | 343/500 [05:15<02:26,  1.08it/s] 69%|██████▉   | 345/500 [05:18<02:51,  1.11s/it] 69%|██████▉   | 347/500 [05:18<02:01,  1.26it/s] 70%|██████▉   | 349/500 [05:18<01:26,  1.74it/s] 70%|███████   | 351/500 [05:24<03:13,  1.30s/it] 71%|███████   | 353/500 [05:24<02:16,  1.08it/s] 71%|███████   | 355/500 [05:27<02:40,  1.11s/it] 71%|███████▏  | 357/500 [05:28<01:53,  1.26it/s] 72%|███████▏  | 359/500 [05:28<01:21,  1.73it/s]Epoch:  301  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  302  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  303  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  304  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  305  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  306  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  307  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  308  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  309  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  310  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  311  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  312  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  313  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  314  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  315  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  316  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  317  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  318  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  319  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  320  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734651267528534
**************************************************learning rate decay**************************************************
Epoch:  321  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  322  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  323  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  324  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  325  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  326  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  327  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  328  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  329  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  330  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  331  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  332  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  333  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  334  	Training Loss: 0.15511159598827362
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  335  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  336  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  337  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  338  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  339  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  340  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  341  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  342  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  343  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  344  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  345  	Training Loss: 0.15511159598827362
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  346  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  347  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  348  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  349  	Training Loss: 0.15511159598827362
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  350  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  351  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  352  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  353  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734654247760773
Epoch:  354  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  355  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  356  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  357  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  358  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  359  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  360  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
 72%|███████▏  | 361/500 [05:34<03:00,  1.30s/it] 73%|███████▎  | 363/500 [05:34<02:07,  1.07it/s] 73%|███████▎  | 365/500 [05:37<02:29,  1.11s/it] 73%|███████▎  | 367/500 [05:37<01:45,  1.26it/s] 74%|███████▍  | 369/500 [05:37<01:15,  1.74it/s] 74%|███████▍  | 371/500 [05:43<02:47,  1.30s/it] 75%|███████▍  | 373/500 [05:43<01:57,  1.08it/s] 75%|███████▌  | 375/500 [05:46<02:18,  1.11s/it] 75%|███████▌  | 377/500 [05:46<01:37,  1.26it/s] 76%|███████▌  | 379/500 [05:47<01:09,  1.74it/s] 76%|███████▌  | 381/500 [05:53<02:34,  1.30s/it] 77%|███████▋  | 383/500 [05:53<01:48,  1.07it/s] 77%|███████▋  | 385/500 [05:56<02:07,  1.11s/it] 77%|███████▋  | 387/500 [05:56<01:29,  1.26it/s] 78%|███████▊  | 389/500 [05:56<01:03,  1.74it/s] 78%|███████▊  | 391/500 [06:02<02:22,  1.30s/it] 79%|███████▊  | 393/500 [06:02<01:39,  1.07it/s] 79%|███████▉  | 395/500 [06:05<01:56,  1.11s/it] 79%|███████▉  | 397/500 [06:05<01:22,  1.26it/s] 80%|███████▉  | 399/500 [06:05<00:58,  1.73it/s] 80%|████████  | 401/500 [06:11<02:08,  1.30s/it] 81%|████████  | 403/500 [06:11<01:30,  1.08it/s] 81%|████████  | 405/500 [06:15<01:45,  1.11s/it] 81%|████████▏ | 407/500 [06:15<01:13,  1.26it/s] 82%|████████▏ | 409/500 [06:15<00:52,  1.74it/s] 82%|████████▏ | 411/500 [06:21<01:55,  1.30s/it] 83%|████████▎ | 413/500 [06:21<01:20,  1.08it/s] 83%|████████▎ | 415/500 [06:24<01:33,  1.10s/it] 83%|████████▎ | 417/500 [06:24<01:05,  1.26it/s] 84%|████████▍ | 419/500 [06:24<00:46,  1.74it/s]**************************************************learning rate decay**************************************************
Epoch:  361  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  362  	Training Loss: 0.15511159598827362
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  363  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  364  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  365  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  366  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  367  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  368  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  369  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  370  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  371  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  372  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  373  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  374  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  375  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  376  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  377  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  378  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  379  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  380  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  381  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  382  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  383  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  384  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  385  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  386  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  387  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  388  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  389  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  390  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  391  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  392  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  393  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  394  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  395  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  396  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  397  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  398  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  399  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  400  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  401  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  402  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  403  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  404  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  405  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  406  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  407  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  408  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  409  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  410  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  411  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  412  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  413  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  414  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  415  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  416  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  417  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  418  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  419  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
 84%|████████▍ | 421/500 [06:30<01:42,  1.30s/it] 85%|████████▍ | 423/500 [06:30<01:11,  1.08it/s] 85%|████████▌ | 425/500 [06:33<01:22,  1.11s/it] 85%|████████▌ | 427/500 [06:33<00:57,  1.26it/s] 86%|████████▌ | 429/500 [06:34<00:40,  1.74it/s] 86%|████████▌ | 431/500 [06:40<01:29,  1.30s/it] 87%|████████▋ | 433/500 [06:40<01:02,  1.08it/s] 87%|████████▋ | 435/500 [06:43<01:12,  1.11s/it] 87%|████████▋ | 437/500 [06:43<00:50,  1.26it/s] 88%|████████▊ | 439/500 [06:43<00:35,  1.73it/s] 88%|████████▊ | 441/500 [06:49<01:16,  1.30s/it] 89%|████████▊ | 443/500 [06:49<00:53,  1.07it/s] 89%|████████▉ | 445/500 [06:52<01:01,  1.11s/it] 89%|████████▉ | 447/500 [06:52<00:42,  1.25it/s] 90%|████████▉ | 449/500 [06:52<00:29,  1.73it/s] 90%|█████████ | 451/500 [06:58<01:04,  1.31s/it] 91%|█████████ | 453/500 [06:59<00:43,  1.07it/s] 91%|█████████ | 455/500 [07:02<00:50,  1.11s/it] 91%|█████████▏| 457/500 [07:02<00:34,  1.25it/s] 92%|█████████▏| 459/500 [07:02<00:23,  1.73it/s] 92%|█████████▏| 461/500 [07:08<00:50,  1.30s/it] 93%|█████████▎| 463/500 [07:08<00:34,  1.08it/s] 93%|█████████▎| 465/500 [07:11<00:38,  1.10s/it] 93%|█████████▎| 467/500 [07:11<00:26,  1.26it/s] 94%|█████████▍| 469/500 [07:11<00:17,  1.74it/s] 94%|█████████▍| 471/500 [07:17<00:37,  1.30s/it] 95%|█████████▍| 473/500 [07:17<00:25,  1.08it/s] 95%|█████████▌| 475/500 [07:20<00:27,  1.11s/it] 95%|█████████▌| 477/500 [07:21<00:18,  1.25it/s]Epoch:  420  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  421  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  422  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  423  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  424  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  425  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  426  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  427  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  428  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  429  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  430  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  431  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  432  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734651267528534
Epoch:  433  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  434  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  435  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  436  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  437  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  438  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  439  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  440  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  441  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734651267528534
Epoch:  442  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  443  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  444  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  445  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  446  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  447  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  448  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  449  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  450  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  451  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  452  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  453  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  454  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  455  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  456  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  457  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  458  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  459  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  460  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  461  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  462  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  463  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  464  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  465  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  466  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  467  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  468  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  469  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  470  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  471  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  472  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  473  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  474  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  475  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  476  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  477  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  478  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
 96%|█████████▌| 479/500 [07:21<00:12,  1.73it/s] 96%|█████████▌| 481/500 [07:27<00:24,  1.30s/it] 97%|█████████▋| 483/500 [07:27<00:15,  1.08it/s] 97%|█████████▋| 485/500 [07:30<00:16,  1.10s/it] 97%|█████████▋| 487/500 [07:30<00:10,  1.26it/s] 98%|█████████▊| 489/500 [07:30<00:06,  1.74it/s] 98%|█████████▊| 491/500 [07:36<00:11,  1.30s/it] 99%|█████████▊| 493/500 [07:36<00:06,  1.07it/s] 99%|█████████▉| 495/500 [07:39<00:05,  1.11s/it] 99%|█████████▉| 497/500 [07:39<00:02,  1.25it/s]100%|█████████▉| 499/500 [07:40<00:00,  1.73it/s]100%|██████████| 500/500 [07:43<00:00,  1.08it/s]
Epoch:  479  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  480  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  481  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  482  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  483  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  484  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  485  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  486  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  487  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  488  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  489  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  490  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  491  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  492  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  493  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  494  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  495  	Training Loss: 0.15511158108711243
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
Epoch:  496  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
Epoch:  497  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  498  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  499  	Training Loss: 0.15511158108711243
Test Loss:  0.19326306879520416
Valid Loss:  0.17734652757644653
Epoch:  500  	Training Loss: 0.15511159598827362
Test Loss:  0.19326308369636536
Valid Loss:  0.17734652757644653
**************************************************learning rate decay**************************************************
seed is  8
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:03<25:31,  3.07s/it]  1%|          | 3/500 [00:03<06:59,  1.19it/s]  1%|          | 5/500 [00:03<03:38,  2.26it/s]  1%|▏         | 7/500 [00:03<02:19,  3.55it/s]  2%|▏         | 9/500 [00:03<01:38,  5.01it/s]  2%|▏         | 11/500 [00:06<05:25,  1.50it/s]  3%|▎         | 13/500 [00:06<03:47,  2.14it/s]  3%|▎         | 15/500 [00:06<02:43,  2.97it/s]  3%|▎         | 17/500 [00:07<02:00,  3.99it/s]  4%|▍         | 19/500 [00:07<01:32,  5.19it/s]  4%|▍         | 21/500 [00:13<08:22,  1.05s/it]  5%|▍         | 23/500 [00:13<05:57,  1.33it/s]  5%|▌         | 25/500 [00:16<07:49,  1.01it/s]  5%|▌         | 27/500 [00:16<05:35,  1.41it/s]  6%|▌         | 29/500 [00:16<04:01,  1.95it/s]  6%|▌         | 31/500 [00:22<09:52,  1.26s/it]  7%|▋         | 33/500 [00:22<07:01,  1.11it/s]  7%|▋         | 35/500 [00:25<08:25,  1.09s/it]  7%|▋         | 37/500 [00:25<06:01,  1.28it/s]  8%|▊         | 39/500 [00:26<04:20,  1.77it/s]  8%|▊         | 41/500 [00:32<09:56,  1.30s/it]  9%|▊         | 43/500 [00:32<07:04,  1.08it/s]  9%|▉         | 45/500 [00:35<08:22,  1.10s/it]  9%|▉         | 47/500 [00:35<05:58,  1.26it/s] 10%|▉         | 49/500 [00:35<04:18,  1.74it/s] 10%|█         | 51/500 [00:41<09:43,  1.30s/it] 11%|█         | 53/500 [00:41<06:55,  1.08it/s] 11%|█         | 55/500 [00:44<08:14,  1.11s/it] 11%|█▏        | 57/500 [00:44<05:53,  1.25it/s] 12%|█▏        | 59/500 [00:44<04:15,  1.73it/s] 12%|█▏        | 61/500 [00:50<09:29,  1.30s/it] 13%|█▎        | 63/500 [00:50<06:45,  1.08it/s]Epoch:  1  	Training Loss: 0.22011888027191162
Test Loss:  325.066650390625
Valid Loss:  314.8958740234375
Epoch:  2  	Training Loss: 307.95074462890625
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  3  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  4  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  5  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  6  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  7  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  8  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  9  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  10  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  11  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  12  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  13  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  14  	Training Loss: 0.2451821267604828
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  15  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  16  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  17  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  18  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  19  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  20  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  21  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  22  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  23  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  24  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  25  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  26  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.32863849401474
Epoch:  27  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  28  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  29  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  30  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  31  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  32  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863855361938477
Epoch:  33  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  34  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  35  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  36  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  37  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  38  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  39  	Training Loss: 0.24518217146396637
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  40  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  41  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  42  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.3286385238170624
Epoch:  43  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  44  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  45  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  46  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863855361938477
Epoch:  47  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  48  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  49  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  50  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  51  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.3286385238170624
Epoch:  52  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  53  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  54  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  55  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  56  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  57  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.3286385238170624
Epoch:  58  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  59  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  60  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  61  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  62  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  63  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  64  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.32863849401474
Epoch:  65  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
 13%|█▎        | 65/500 [00:53<07:59,  1.10s/it] 13%|█▎        | 67/500 [00:54<05:42,  1.26it/s] 14%|█▍        | 69/500 [00:54<04:06,  1.75it/s] 14%|█▍        | 71/500 [01:00<09:15,  1.30s/it] 15%|█▍        | 73/500 [01:00<06:35,  1.08it/s] 15%|█▌        | 75/500 [01:03<07:51,  1.11s/it] 15%|█▌        | 77/500 [01:03<05:37,  1.25it/s] 16%|█▌        | 79/500 [01:03<04:03,  1.73it/s] 16%|█▌        | 81/500 [01:09<09:02,  1.30s/it] 17%|█▋        | 83/500 [01:09<06:26,  1.08it/s] 17%|█▋        | 85/500 [01:12<07:38,  1.11s/it] 17%|█▋        | 87/500 [01:12<05:27,  1.26it/s] 18%|█▊        | 89/500 [01:13<03:56,  1.74it/s] 18%|█▊        | 91/500 [01:19<08:51,  1.30s/it] 19%|█▊        | 93/500 [01:19<06:18,  1.08it/s] 19%|█▉        | 95/500 [01:22<07:27,  1.11s/it] 19%|█▉        | 97/500 [01:22<05:20,  1.26it/s] 20%|█▉        | 99/500 [01:22<03:50,  1.74it/s] 20%|██        | 101/500 [01:28<08:35,  1.29s/it] 21%|██        | 103/500 [01:28<06:07,  1.08it/s] 21%|██        | 105/500 [01:31<07:17,  1.11s/it] 21%|██▏       | 107/500 [01:31<05:12,  1.26it/s] 22%|██▏       | 109/500 [01:31<03:44,  1.74it/s] 22%|██▏       | 111/500 [01:37<08:22,  1.29s/it] 23%|██▎       | 113/500 [01:37<05:58,  1.08it/s] 23%|██▎       | 115/500 [01:41<07:09,  1.12s/it] 23%|██▎       | 117/500 [01:41<05:07,  1.25it/s] 24%|██▍       | 119/500 [01:41<03:41,  1.72it/s] 24%|██▍       | 121/500 [01:47<08:21,  1.32s/it] 25%|██▍       | 123/500 [01:47<05:56,  1.06it/s] 25%|██▌       | 125/500 [01:50<07:02,  1.13s/it]**************************************************learning rate decay**************************************************
Epoch:  66  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  67  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  68  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  69  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  70  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  71  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  72  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  73  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  74  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  75  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  76  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  77  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  78  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  79  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  80  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  81  	Training Loss: 0.24518214166164398
Test Loss:  0.3267739713191986
Valid Loss:  0.32863849401474
Epoch:  82  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.32863849401474
Epoch:  83  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  84  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  85  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  86  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  87  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  88  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  89  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  90  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  91  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  92  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  93  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  94  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  95  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  96  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  97  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  98  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  99  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  100  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  101  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  102  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  103  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  104  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  105  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  106  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  107  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  108  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  109  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  110  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  111  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  112  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  113  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  114  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  115  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  116  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  117  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  118  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  119  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  120  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  121  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  122  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  123  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  124  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  125  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  126  	Training Loss: 0.24518214166164398
 25%|██▌       | 127/500 [01:50<05:01,  1.24it/s] 26%|██▌       | 129/500 [01:50<03:36,  1.71it/s] 26%|██▌       | 131/500 [01:56<08:05,  1.31s/it] 27%|██▋       | 133/500 [01:57<05:44,  1.06it/s] 27%|██▋       | 135/500 [02:00<06:47,  1.12s/it] 27%|██▋       | 137/500 [02:00<04:51,  1.25it/s] 28%|██▊       | 139/500 [02:00<03:29,  1.72it/s] 28%|██▊       | 141/500 [02:06<07:47,  1.30s/it] 29%|██▊       | 143/500 [02:06<05:33,  1.07it/s] 29%|██▉       | 145/500 [02:09<06:33,  1.11s/it] 29%|██▉       | 147/500 [02:09<04:40,  1.26it/s] 30%|██▉       | 149/500 [02:09<03:22,  1.74it/s] 30%|███       | 151/500 [02:15<07:31,  1.29s/it] 31%|███       | 153/500 [02:15<05:20,  1.08it/s] 31%|███       | 155/500 [02:18<06:22,  1.11s/it] 31%|███▏      | 157/500 [02:19<04:33,  1.26it/s] 32%|███▏      | 159/500 [02:19<03:16,  1.73it/s] 32%|███▏      | 161/500 [02:25<07:20,  1.30s/it] 33%|███▎      | 163/500 [02:25<05:13,  1.08it/s] 33%|███▎      | 165/500 [02:28<06:10,  1.11s/it] 33%|███▎      | 167/500 [02:28<04:24,  1.26it/s] 34%|███▍      | 169/500 [02:28<03:10,  1.74it/s] 34%|███▍      | 171/500 [02:34<07:06,  1.30s/it] 35%|███▍      | 173/500 [02:34<05:02,  1.08it/s] 35%|███▌      | 175/500 [02:37<05:58,  1.10s/it] 35%|███▌      | 177/500 [02:37<04:16,  1.26it/s] 36%|███▌      | 179/500 [02:38<03:04,  1.74it/s] 36%|███▌      | 181/500 [02:43<06:52,  1.29s/it] 37%|███▋      | 183/500 [02:44<04:53,  1.08it/s] 37%|███▋      | 185/500 [02:47<05:46,  1.10s/it]Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  127  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  128  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  129  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  130  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  131  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  132  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  133  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  134  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  135  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  136  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  137  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  138  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  139  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  140  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  141  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.32863849401474
Epoch:  142  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  143  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  144  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  145  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  146  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  147  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  148  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  149  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  150  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  151  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  152  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  153  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  154  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  155  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  156  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  157  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  158  	Training Loss: 0.24518214166164398
Test Loss:  0.3267739713191986
Valid Loss:  0.3286385238170624
Epoch:  159  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.3286385238170624
Epoch:  160  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  161  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  162  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  163  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  164  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.3286385238170624
Epoch:  165  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  166  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  167  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  168  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  169  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.3286385238170624
Epoch:  170  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  171  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  172  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  173  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  174  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  175  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  176  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  177  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  178  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  179  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  180  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  181  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  182  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  183  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  184  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  185  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  186  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  187  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
 37%|███▋      | 187/500 [02:47<04:07,  1.27it/s] 38%|███▊      | 189/500 [02:47<02:57,  1.75it/s] 38%|███▊      | 191/500 [02:53<06:42,  1.30s/it] 39%|███▊      | 193/500 [02:53<04:45,  1.07it/s] 39%|███▉      | 195/500 [02:56<05:39,  1.11s/it] 39%|███▉      | 197/500 [02:56<04:02,  1.25it/s] 40%|███▉      | 199/500 [02:56<02:54,  1.73it/s] 40%|████      | 201/500 [03:02<06:28,  1.30s/it] 41%|████      | 203/500 [03:02<04:36,  1.08it/s] 41%|████      | 205/500 [03:05<05:26,  1.11s/it] 41%|████▏     | 207/500 [03:06<03:52,  1.26it/s] 42%|████▏     | 209/500 [03:06<02:47,  1.74it/s] 42%|████▏     | 211/500 [03:12<06:13,  1.29s/it] 43%|████▎     | 213/500 [03:12<04:25,  1.08it/s] 43%|████▎     | 215/500 [03:15<05:14,  1.10s/it] 43%|████▎     | 217/500 [03:15<03:44,  1.26it/s] 44%|████▍     | 219/500 [03:15<02:41,  1.74it/s] 44%|████▍     | 221/500 [03:21<06:01,  1.30s/it] 45%|████▍     | 223/500 [03:21<04:17,  1.08it/s] 45%|████▌     | 225/500 [03:24<05:03,  1.10s/it] 45%|████▌     | 227/500 [03:24<03:36,  1.26it/s] 46%|████▌     | 229/500 [03:24<02:35,  1.75it/s] 46%|████▌     | 231/500 [03:30<05:47,  1.29s/it] 47%|████▋     | 233/500 [03:31<04:06,  1.08it/s] 47%|████▋     | 235/500 [03:34<04:51,  1.10s/it] 47%|████▋     | 237/500 [03:34<03:28,  1.26it/s] 48%|████▊     | 239/500 [03:34<02:29,  1.74it/s] 48%|████▊     | 241/500 [03:37<03:41,  1.17it/s] 49%|████▊     | 243/500 [03:37<02:38,  1.62it/s] 49%|████▉     | 245/500 [03:40<03:47,  1.12it/s] 49%|████▉     | 247/500 [03:40<02:43,  1.55it/s] 50%|████▉     | 249/500 [03:40<01:58,  2.12it/s]Valid Loss:  0.32863849401474
Epoch:  188  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  189  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  190  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  191  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  192  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  193  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  194  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  195  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  196  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  197  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  198  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.32863849401474
Epoch:  199  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  200  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  201  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  202  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  203  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  204  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  205  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  206  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  207  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  208  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  209  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  210  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  211  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  212  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  213  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  214  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  215  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  216  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  217  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  218  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  219  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  220  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863855361938477
**************************************************learning rate decay**************************************************
Epoch:  221  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  222  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  223  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.3286385238170624
Epoch:  224  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.3286385238170624
Epoch:  225  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  226  	Training Loss: 0.24518214166164398
Test Loss:  0.3267739713191986
Valid Loss:  0.3286385238170624
Epoch:  227  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  228  	Training Loss: 0.2451821267604828
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  229  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  230  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  231  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  232  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  233  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  234  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  235  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  236  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  237  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  238  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  239  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  240  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  241  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  242  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863855361938477
Epoch:  243  	Training Loss: 0.24518214166164398
Test Loss:  0.3267739713191986
Valid Loss:  0.32863849401474
Epoch:  244  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  245  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  246  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  247  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  248  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  249  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
 50%|█████     | 251/500 [03:46<05:05,  1.23s/it] 51%|█████     | 253/500 [03:46<03:37,  1.14it/s] 51%|█████     | 255/500 [03:49<04:22,  1.07s/it] 51%|█████▏    | 257/500 [03:50<03:07,  1.30it/s] 52%|█████▏    | 259/500 [03:50<02:14,  1.79it/s] 52%|█████▏    | 261/500 [03:56<05:07,  1.29s/it] 53%|█████▎    | 263/500 [03:56<03:37,  1.09it/s] 53%|█████▎    | 265/500 [03:59<04:17,  1.10s/it] 53%|█████▎    | 267/500 [03:59<03:03,  1.27it/s] 54%|█████▍    | 269/500 [03:59<02:12,  1.75it/s] 54%|█████▍    | 271/500 [04:05<04:57,  1.30s/it] 55%|█████▍    | 273/500 [04:05<03:31,  1.07it/s] 55%|█████▌    | 275/500 [04:08<04:08,  1.10s/it] 55%|█████▌    | 277/500 [04:08<02:56,  1.26it/s] 56%|█████▌    | 279/500 [04:09<02:06,  1.74it/s] 56%|█████▌    | 281/500 [04:15<04:44,  1.30s/it] 57%|█████▋    | 283/500 [04:15<03:21,  1.08it/s] 57%|█████▋    | 285/500 [04:18<03:58,  1.11s/it] 57%|█████▋    | 287/500 [04:18<02:50,  1.25it/s] 58%|█████▊    | 289/500 [04:18<02:02,  1.73it/s] 58%|█████▊    | 291/500 [04:24<04:31,  1.30s/it] 59%|█████▊    | 293/500 [04:24<03:12,  1.08it/s] 59%|█████▉    | 295/500 [04:27<03:45,  1.10s/it] 59%|█████▉    | 297/500 [04:27<02:40,  1.27it/s] 60%|█████▉    | 299/500 [04:27<01:54,  1.75it/s] 60%|██████    | 301/500 [04:33<04:20,  1.31s/it] 61%|██████    | 303/500 [04:34<03:04,  1.07it/s] 61%|██████    | 305/500 [04:37<03:36,  1.11s/it] 61%|██████▏   | 307/500 [04:37<02:33,  1.25it/s] 62%|██████▏   | 309/500 [04:37<01:50,  1.73it/s]Epoch:  250  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  251  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  252  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  253  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  254  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  255  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  256  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  257  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  258  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  259  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  260  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  261  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  262  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  263  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  264  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  265  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  266  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  267  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  268  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  269  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  270  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  271  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  272  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  273  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  274  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  275  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  276  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  277  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  278  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  279  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  280  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  281  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  282  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  283  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  284  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  285  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  286  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  287  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863855361938477
Epoch:  288  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  289  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  290  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  291  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.3286385238170624
Epoch:  292  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  293  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  294  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  295  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  296  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  297  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  298  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  299  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  300  	Training Loss: 0.24518217146396637
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  301  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  302  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  303  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  304  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  305  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  306  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  307  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  308  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  309  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  310  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
 62%|██████▏   | 311/500 [04:40<02:44,  1.15it/s] 63%|██████▎   | 313/500 [04:40<01:57,  1.59it/s] 63%|██████▎   | 315/500 [04:43<02:47,  1.11it/s] 63%|██████▎   | 317/500 [04:43<01:59,  1.53it/s] 64%|██████▍   | 319/500 [04:43<01:26,  2.09it/s] 64%|██████▍   | 321/500 [04:49<03:40,  1.23s/it] 65%|██████▍   | 323/500 [04:50<02:36,  1.13it/s] 65%|██████▌   | 325/500 [04:53<03:10,  1.09s/it] 65%|██████▌   | 327/500 [04:53<02:15,  1.28it/s] 66%|██████▌   | 329/500 [04:53<01:36,  1.77it/s] 66%|██████▌   | 331/500 [04:59<03:40,  1.30s/it] 67%|██████▋   | 333/500 [04:59<02:35,  1.07it/s] 67%|██████▋   | 335/500 [05:02<03:03,  1.11s/it] 67%|██████▋   | 337/500 [05:02<02:10,  1.25it/s] 68%|██████▊   | 339/500 [05:02<01:33,  1.72it/s] 68%|██████▊   | 341/500 [05:08<03:27,  1.30s/it] 69%|██████▊   | 343/500 [05:09<02:26,  1.07it/s] 69%|██████▉   | 345/500 [05:12<02:51,  1.11s/it] 69%|██████▉   | 347/500 [05:12<02:01,  1.26it/s] 70%|██████▉   | 349/500 [05:12<01:27,  1.73it/s] 70%|███████   | 351/500 [05:18<03:14,  1.30s/it] 71%|███████   | 353/500 [05:18<02:16,  1.07it/s] 71%|███████   | 355/500 [05:21<02:40,  1.11s/it] 71%|███████▏  | 357/500 [05:21<01:53,  1.26it/s] 72%|███████▏  | 359/500 [05:21<01:21,  1.73it/s] 72%|███████▏  | 361/500 [05:27<03:00,  1.30s/it] 73%|███████▎  | 363/500 [05:27<02:07,  1.07it/s] 73%|███████▎  | 365/500 [05:30<02:30,  1.11s/it] 73%|███████▎  | 367/500 [05:31<01:46,  1.25it/s] 74%|███████▍  | 369/500 [05:31<01:15,  1.73it/s] 74%|███████▍  | 371/500 [05:37<02:47,  1.30s/it]Epoch:  311  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  312  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  313  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  314  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  315  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  316  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  317  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.32863849401474
Epoch:  318  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  319  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  320  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  321  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  322  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.3286385238170624
Epoch:  323  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  324  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  325  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  326  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  327  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  328  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  329  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  330  	Training Loss: 0.24518214166164398
Test Loss:  0.3267739713191986
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  331  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  332  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  333  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  334  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  335  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  336  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  337  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  338  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  339  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  340  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  341  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  342  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  343  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  344  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  345  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  346  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  347  	Training Loss: 0.24518217146396637
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  348  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  349  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  350  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  351  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  352  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  353  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  354  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  355  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  356  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  357  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  358  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  359  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  360  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  361  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  362  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  363  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  364  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  365  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  366  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  367  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  368  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  369  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  370  	Training Loss: 0.2451821267604828
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  371  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
 75%|███████▍  | 373/500 [05:37<01:58,  1.08it/s] 75%|███████▌  | 375/500 [05:40<02:18,  1.11s/it] 75%|███████▌  | 377/500 [05:40<01:37,  1.26it/s] 76%|███████▌  | 379/500 [05:40<01:09,  1.73it/s] 76%|███████▌  | 381/500 [05:46<02:35,  1.31s/it] 77%|███████▋  | 383/500 [05:46<01:49,  1.07it/s] 77%|███████▋  | 385/500 [05:49<02:07,  1.11s/it] 77%|███████▋  | 387/500 [05:49<01:30,  1.26it/s] 78%|███████▊  | 389/500 [05:50<01:04,  1.73it/s] 78%|███████▊  | 391/500 [05:56<02:22,  1.30s/it] 79%|███████▊  | 393/500 [05:56<01:39,  1.07it/s] 79%|███████▉  | 395/500 [05:59<01:56,  1.11s/it] 79%|███████▉  | 397/500 [05:59<01:22,  1.25it/s] 80%|███████▉  | 399/500 [05:59<00:58,  1.73it/s] 80%|████████  | 401/500 [06:05<02:09,  1.30s/it] 81%|████████  | 403/500 [06:05<01:30,  1.07it/s] 81%|████████  | 405/500 [06:08<01:45,  1.11s/it] 81%|████████▏ | 407/500 [06:08<01:13,  1.26it/s] 82%|████████▏ | 409/500 [06:08<00:52,  1.74it/s] 82%|████████▏ | 411/500 [06:15<01:56,  1.31s/it] 83%|████████▎ | 413/500 [06:15<01:21,  1.07it/s] 83%|████████▎ | 415/500 [06:18<01:34,  1.11s/it] 83%|████████▎ | 417/500 [06:18<01:06,  1.25it/s] 84%|████████▍ | 419/500 [06:18<00:46,  1.72it/s] 84%|████████▍ | 421/500 [06:24<01:42,  1.30s/it] 85%|████████▍ | 423/500 [06:24<01:11,  1.07it/s] 85%|████████▌ | 425/500 [06:27<01:23,  1.11s/it] 85%|████████▌ | 427/500 [06:27<00:58,  1.25it/s] 86%|████████▌ | 429/500 [06:27<00:41,  1.73it/s] 86%|████████▌ | 431/500 [06:33<01:29,  1.30s/it]Epoch:  372  	Training Loss: 0.24518214166164398
Test Loss:  0.3267739713191986
Valid Loss:  0.32863849401474
Epoch:  373  	Training Loss: 0.2451821267604828
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  374  	Training Loss: 0.24518214166164398
Test Loss:  0.3267739713191986
Valid Loss:  0.32863849401474
Epoch:  375  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  376  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  377  	Training Loss: 0.24518214166164398
Test Loss:  0.3267739713191986
Valid Loss:  0.3286385238170624
Epoch:  378  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  379  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  380  	Training Loss: 0.24518217146396637
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  381  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  382  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.32863849401474
Epoch:  383  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  384  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  385  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  386  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  387  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  388  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  389  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  390  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  391  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  392  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  393  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  394  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  395  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  396  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  397  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863855361938477
Epoch:  398  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  399  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  400  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  401  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  402  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  403  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  404  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  405  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  406  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  407  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  408  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  409  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  410  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  411  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  412  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  413  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  414  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  415  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  416  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  417  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  418  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  419  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  420  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  421  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  422  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  423  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  424  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  425  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  426  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  427  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  428  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863855361938477
Epoch:  429  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  430  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  431  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  432  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
 87%|████████▋ | 433/500 [06:34<01:02,  1.07it/s] 87%|████████▋ | 435/500 [06:37<01:12,  1.11s/it] 87%|████████▋ | 437/500 [06:37<00:50,  1.25it/s] 88%|████████▊ | 439/500 [06:37<00:35,  1.72it/s] 88%|████████▊ | 441/500 [06:43<01:17,  1.32s/it] 89%|████████▊ | 443/500 [06:43<00:53,  1.06it/s] 89%|████████▉ | 445/500 [06:46<01:01,  1.11s/it] 89%|████████▉ | 447/500 [06:46<00:42,  1.25it/s] 90%|████████▉ | 449/500 [06:46<00:29,  1.73it/s] 90%|█████████ | 451/500 [06:52<01:03,  1.30s/it] 91%|█████████ | 453/500 [06:52<00:43,  1.08it/s] 91%|█████████ | 455/500 [06:56<00:50,  1.11s/it] 91%|█████████▏| 457/500 [06:56<00:34,  1.25it/s] 92%|█████████▏| 459/500 [06:56<00:23,  1.73it/s] 92%|█████████▏| 461/500 [07:02<00:50,  1.29s/it] 93%|█████████▎| 463/500 [07:02<00:34,  1.08it/s] 93%|█████████▎| 465/500 [07:02<00:23,  1.49it/s] 93%|█████████▎| 467/500 [07:02<00:16,  2.05it/s] 94%|█████████▍| 469/500 [07:02<00:11,  2.77it/s] 94%|█████████▍| 471/500 [07:08<00:33,  1.17s/it] 95%|█████████▍| 473/500 [07:09<00:22,  1.19it/s] 95%|█████████▌| 475/500 [07:12<00:26,  1.05s/it] 95%|█████████▌| 477/500 [07:12<00:17,  1.33it/s] 96%|█████████▌| 479/500 [07:12<00:11,  1.83it/s] 96%|█████████▌| 481/500 [07:18<00:24,  1.28s/it] 97%|█████████▋| 483/500 [07:18<00:15,  1.09it/s] 97%|█████████▋| 485/500 [07:21<00:16,  1.10s/it] 97%|█████████▋| 487/500 [07:21<00:10,  1.26it/s] 98%|█████████▊| 489/500 [07:21<00:06,  1.74it/s] 98%|█████████▊| 491/500 [07:27<00:11,  1.30s/it] 99%|█████████▊| 493/500 [07:27<00:06,  1.07it/s]Epoch:  433  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  434  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  435  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  436  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  437  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  438  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  439  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  440  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  441  	Training Loss: 0.24518214166164398
Test Loss:  0.3267739713191986
Valid Loss:  0.32863849401474
Epoch:  442  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  443  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.32863849401474
Epoch:  444  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  445  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  446  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  447  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  448  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  449  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.3286385238170624
Epoch:  450  	Training Loss: 0.24518215656280518
Test Loss:  0.3267739713191986
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  451  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  452  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863855361938477
Epoch:  453  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  454  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  455  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  456  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  457  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  458  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  459  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  460  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  461  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863855361938477
Epoch:  462  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  463  	Training Loss: 0.24518217146396637
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  464  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  465  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  466  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  467  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  468  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  469  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  470  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  471  	Training Loss: 0.24518214166164398
Test Loss:  0.3267739713191986
Valid Loss:  0.32863849401474
Epoch:  472  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  473  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  474  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  475  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  476  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  477  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  478  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  479  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  480  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  481  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  482  	Training Loss: 0.2451821267604828
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  483  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  484  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  485  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
Epoch:  486  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  487  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  488  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  489  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  490  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  491  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  492  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  493  	Training Loss: 0.24518214166164398
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  494  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  495  	Training Loss: 0.24518214166164398
 99%|█████████▉| 495/500 [07:31<00:05,  1.11s/it] 99%|█████████▉| 497/500 [07:31<00:02,  1.25it/s]100%|█████████▉| 499/500 [07:31<00:00,  1.73it/s]100%|██████████| 500/500 [07:34<00:00,  1.10it/s]
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
**************************************************learning rate decay**************************************************
Epoch:  496  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  497  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
Epoch:  498  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  499  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.32863849401474
Epoch:  500  	Training Loss: 0.24518215656280518
Test Loss:  0.326774001121521
Valid Loss:  0.3286385238170624
**************************************************learning rate decay**************************************************
seed is  9
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:02<24:45,  2.98s/it]  1%|          | 3/500 [00:03<06:46,  1.22it/s]  1%|          | 5/500 [00:03<03:32,  2.33it/s]  1%|▏         | 7/500 [00:03<02:15,  3.64it/s]  2%|▏         | 9/500 [00:03<01:35,  5.13it/s]  2%|▏         | 11/500 [00:06<05:26,  1.50it/s]  3%|▎         | 13/500 [00:06<03:47,  2.14it/s]  3%|▎         | 15/500 [00:06<02:43,  2.96it/s]  3%|▎         | 17/500 [00:06<02:01,  3.98it/s]  4%|▍         | 19/500 [00:07<01:32,  5.18it/s]  4%|▍         | 21/500 [00:13<08:29,  1.06s/it]  5%|▍         | 23/500 [00:13<06:02,  1.31it/s]  5%|▌         | 25/500 [00:16<07:53,  1.00it/s]  5%|▌         | 27/500 [00:16<05:38,  1.40it/s]  6%|▌         | 29/500 [00:16<04:04,  1.93it/s]  6%|▌         | 31/500 [00:22<09:50,  1.26s/it]  7%|▋         | 33/500 [00:22<06:59,  1.11it/s]  7%|▋         | 35/500 [00:25<08:28,  1.09s/it]  7%|▋         | 37/500 [00:25<06:03,  1.27it/s]  8%|▊         | 39/500 [00:26<04:22,  1.76it/s]  8%|▊         | 41/500 [00:32<09:53,  1.29s/it]  9%|▊         | 43/500 [00:32<07:02,  1.08it/s]  9%|▉         | 45/500 [00:35<08:21,  1.10s/it]  9%|▉         | 47/500 [00:35<05:58,  1.26it/s] 10%|▉         | 49/500 [00:35<04:18,  1.75it/s] 10%|█         | 51/500 [00:41<09:40,  1.29s/it] 11%|█         | 53/500 [00:41<06:53,  1.08it/s] 11%|█         | 55/500 [00:44<08:11,  1.11s/it] 11%|█▏        | 57/500 [00:44<05:51,  1.26it/s] 12%|█▏        | 59/500 [00:44<04:13,  1.74it/s] 12%|█▏        | 61/500 [00:50<09:34,  1.31s/it] 13%|█▎        | 63/500 [00:51<06:48,  1.07it/s]Epoch:  1  	Training Loss: 0.45221614837646484
Test Loss:  74.32869720458984
Valid Loss:  71.6448745727539
Epoch:  2  	Training Loss: 73.3187255859375
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  3  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  4  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  5  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  6  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  7  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.49766305088996887
Epoch:  8  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  9  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  10  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  11  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  12  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  13  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  14  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  15  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  16  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  17  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.49766305088996887
Epoch:  18  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  19  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  20  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  21  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  22  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  23  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  24  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  25  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  26  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  27  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  28  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  29  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  30  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  31  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  32  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  33  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  34  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  35  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  36  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  37  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  38  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  39  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  40  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  41  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  42  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  43  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  44  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  45  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  46  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  47  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  48  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  49  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  50  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  51  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  52  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  53  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  54  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  55  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  56  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  57  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  58  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.49766305088996887
Epoch:  59  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  60  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  61  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  62  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.49766305088996887
Epoch:  63  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  64  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  65  	Training Loss: 0.45748376846313477 13%|█▎        | 65/500 [00:54<08:06,  1.12s/it] 13%|█▎        | 67/500 [00:54<05:47,  1.25it/s] 14%|█▍        | 69/500 [00:54<04:10,  1.72it/s] 14%|█▍        | 71/500 [01:00<09:16,  1.30s/it] 15%|█▍        | 73/500 [01:00<06:35,  1.08it/s] 15%|█▌        | 75/500 [01:03<07:50,  1.11s/it] 15%|█▌        | 77/500 [01:03<05:36,  1.26it/s] 16%|█▌        | 79/500 [01:03<04:02,  1.74it/s] 16%|█▌        | 81/500 [01:09<09:02,  1.30s/it] 17%|█▋        | 83/500 [01:09<06:26,  1.08it/s] 17%|█▋        | 85/500 [01:12<07:40,  1.11s/it] 17%|█▋        | 87/500 [01:13<05:29,  1.25it/s] 18%|█▊        | 89/500 [01:13<03:57,  1.73it/s] 18%|█▊        | 91/500 [01:19<08:49,  1.29s/it] 19%|█▊        | 93/500 [01:19<06:16,  1.08it/s] 19%|█▉        | 95/500 [01:22<07:27,  1.10s/it] 19%|█▉        | 97/500 [01:22<05:19,  1.26it/s] 20%|█▉        | 99/500 [01:22<03:50,  1.74it/s] 20%|██        | 101/500 [01:28<08:34,  1.29s/it] 21%|██        | 103/500 [01:28<06:06,  1.08it/s] 21%|██        | 105/500 [01:31<07:14,  1.10s/it] 21%|██▏       | 107/500 [01:31<05:09,  1.27it/s] 22%|██▏       | 109/500 [01:31<03:43,  1.75it/s] 22%|██▏       | 111/500 [01:37<08:21,  1.29s/it] 23%|██▎       | 113/500 [01:37<05:56,  1.09it/s] 23%|██▎       | 115/500 [01:40<07:04,  1.10s/it] 23%|██▎       | 117/500 [01:41<05:03,  1.26it/s] 24%|██▍       | 119/500 [01:41<03:38,  1.74it/s] 24%|██▍       | 121/500 [01:47<08:11,  1.30s/it] 25%|██▍       | 123/500 [01:47<05:49,  1.08it/s]
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  66  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  67  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  68  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  69  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  70  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  71  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  72  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  73  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  74  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  75  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  76  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  77  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  78  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  79  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  80  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  81  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  82  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  83  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  84  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  85  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  86  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  87  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  88  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  89  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  90  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  91  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  92  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  93  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  94  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  95  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  96  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  97  	Training Loss: 0.45748379826545715
Test Loss:  0.5392844080924988
Valid Loss:  0.49766305088996887
Epoch:  98  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  99  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  100  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  101  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  102  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  103  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  104  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  105  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  106  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  107  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.49766305088996887
Epoch:  108  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  109  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  110  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  111  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  112  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  113  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  114  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  115  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  116  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  117  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  118  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  119  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  120  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  121  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  122  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  123  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  124  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  125  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
 25%|██▌       | 125/500 [01:50<06:54,  1.11s/it] 25%|██▌       | 127/500 [01:50<04:56,  1.26it/s] 26%|██▌       | 129/500 [01:50<03:33,  1.74it/s] 26%|██▌       | 131/500 [01:56<07:56,  1.29s/it] 27%|██▋       | 133/500 [01:56<05:38,  1.08it/s] 27%|██▋       | 135/500 [01:59<06:43,  1.11s/it] 27%|██▋       | 137/500 [01:59<04:48,  1.26it/s] 28%|██▊       | 139/500 [02:00<03:27,  1.74it/s] 28%|██▊       | 141/500 [02:05<07:43,  1.29s/it] 29%|██▊       | 143/500 [02:06<05:29,  1.08it/s] 29%|██▉       | 145/500 [02:09<06:30,  1.10s/it] 29%|██▉       | 147/500 [02:09<04:38,  1.27it/s] 30%|██▉       | 149/500 [02:09<03:20,  1.75it/s] 30%|███       | 151/500 [02:15<07:28,  1.29s/it] 31%|███       | 153/500 [02:15<05:19,  1.09it/s] 31%|███       | 155/500 [02:18<06:18,  1.10s/it] 31%|███▏      | 157/500 [02:18<04:30,  1.27it/s] 32%|███▏      | 159/500 [02:18<03:14,  1.75it/s] 32%|███▏      | 161/500 [02:24<07:17,  1.29s/it] 33%|███▎      | 163/500 [02:24<05:11,  1.08it/s] 33%|███▎      | 165/500 [02:27<06:09,  1.10s/it] 33%|███▎      | 167/500 [02:27<04:23,  1.26it/s] 34%|███▍      | 169/500 [02:28<03:09,  1.74it/s] 34%|███▍      | 171/500 [02:33<07:03,  1.29s/it] 35%|███▍      | 173/500 [02:34<05:01,  1.09it/s] 35%|███▌      | 175/500 [02:37<05:58,  1.10s/it] 35%|███▌      | 177/500 [02:37<04:16,  1.26it/s] 36%|███▌      | 179/500 [02:37<03:04,  1.74it/s] 36%|███▌      | 181/500 [02:43<06:53,  1.29s/it] 37%|███▋      | 183/500 [02:43<04:53,  1.08it/s]**************************************************learning rate decay**************************************************
Epoch:  126  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  127  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  128  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.4976630210876465
Epoch:  129  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  130  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
**************************************************learning rate decay**************************************************
Epoch:  131  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  132  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  133  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  134  	Training Loss: 0.45748382806777954
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  135  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  136  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  137  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  138  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  139  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  140  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  141  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  142  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  143  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  144  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  145  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  146  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  147  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  148  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  149  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  150  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  151  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  152  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  153  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  154  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  155  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  156  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  157  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  158  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  159  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  160  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  161  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  162  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  163  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  164  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  165  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  166  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  167  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  168  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  169  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  170  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  171  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  172  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  173  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  174  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  175  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  176  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  177  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  178  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  179  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  180  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  181  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  182  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  183  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  184  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  185  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
 37%|███▋      | 185/500 [02:46<05:47,  1.10s/it] 37%|███▋      | 187/500 [02:46<04:07,  1.27it/s] 38%|███▊      | 189/500 [02:46<02:58,  1.75it/s] 38%|███▊      | 191/500 [02:52<06:40,  1.29s/it] 39%|███▊      | 193/500 [02:52<04:44,  1.08it/s] 39%|███▉      | 195/500 [02:55<05:37,  1.11s/it] 39%|███▉      | 197/500 [02:56<04:00,  1.26it/s] 40%|███▉      | 199/500 [02:56<02:53,  1.74it/s] 40%|████      | 201/500 [03:02<06:26,  1.29s/it] 41%|████      | 203/500 [03:02<04:34,  1.08it/s] 41%|████      | 205/500 [03:05<05:24,  1.10s/it] 41%|████▏     | 207/500 [03:05<03:51,  1.27it/s] 42%|████▏     | 209/500 [03:05<02:46,  1.75it/s] 42%|████▏     | 211/500 [03:11<06:14,  1.30s/it] 43%|████▎     | 213/500 [03:11<04:25,  1.08it/s] 43%|████▎     | 215/500 [03:14<05:14,  1.10s/it] 43%|████▎     | 217/500 [03:14<03:44,  1.26it/s] 44%|████▍     | 219/500 [03:14<02:41,  1.74it/s] 44%|████▍     | 221/500 [03:20<06:00,  1.29s/it] 45%|████▍     | 223/500 [03:20<04:16,  1.08it/s] 45%|████▌     | 225/500 [03:24<05:03,  1.10s/it] 45%|████▌     | 227/500 [03:24<03:36,  1.26it/s] 46%|████▌     | 229/500 [03:24<02:35,  1.74it/s] 46%|████▌     | 231/500 [03:30<05:48,  1.30s/it] 47%|████▋     | 233/500 [03:30<04:07,  1.08it/s] 47%|████▋     | 235/500 [03:33<04:53,  1.11s/it] 47%|████▋     | 237/500 [03:33<03:29,  1.26it/s] 48%|████▊     | 239/500 [03:33<02:30,  1.74it/s] 48%|████▊     | 241/500 [03:39<05:35,  1.29s/it] 49%|████▊     | 243/500 [03:39<03:58,  1.08it/s]**************************************************learning rate decay**************************************************
Epoch:  186  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  187  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  188  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  189  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  190  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  191  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  192  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  193  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  194  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  195  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
**************************************************learning rate decay**************************************************
Epoch:  196  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  197  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  198  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  199  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  200  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  201  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  202  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  203  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  204  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  205  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  206  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  207  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  208  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  209  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  210  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  211  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  212  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  213  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  214  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  215  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  216  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  217  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  218  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  219  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  220  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  221  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  222  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  223  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  224  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  225  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  226  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  227  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  228  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  229  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  230  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  231  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  232  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  233  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  234  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  235  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  236  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  237  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  238  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  239  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  240  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  241  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  242  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  243  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  244  	Training Loss: 0.45748379826545715
Test Loss:  0.5392844080924988
Valid Loss:  0.4976630210876465
Epoch:  245  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.49766305088996887
 49%|████▉     | 245/500 [03:42<04:42,  1.11s/it] 49%|████▉     | 247/500 [03:42<03:21,  1.26it/s] 50%|████▉     | 249/500 [03:43<02:24,  1.73it/s] 50%|█████     | 251/500 [03:49<05:21,  1.29s/it] 51%|█████     | 253/500 [03:49<03:47,  1.08it/s] 51%|█████     | 255/500 [03:52<04:29,  1.10s/it] 51%|█████▏    | 257/500 [03:52<03:12,  1.26it/s] 52%|█████▏    | 259/500 [03:52<02:18,  1.75it/s] 52%|█████▏    | 261/500 [03:58<05:08,  1.29s/it] 53%|█████▎    | 263/500 [03:58<03:39,  1.08it/s] 53%|█████▎    | 265/500 [04:01<04:19,  1.10s/it] 53%|█████▎    | 267/500 [04:01<03:04,  1.26it/s] 54%|█████▍    | 269/500 [04:01<02:12,  1.75it/s] 54%|█████▍    | 271/500 [04:07<04:55,  1.29s/it] 55%|█████▍    | 273/500 [04:07<03:29,  1.08it/s] 55%|█████▌    | 275/500 [04:10<04:08,  1.10s/it] 55%|█████▌    | 277/500 [04:11<02:56,  1.26it/s] 56%|█████▌    | 279/500 [04:11<02:06,  1.74it/s] 56%|█████▌    | 281/500 [04:17<04:44,  1.30s/it] 57%|█████▋    | 283/500 [04:17<03:21,  1.08it/s] 57%|█████▋    | 285/500 [04:20<03:56,  1.10s/it] 57%|█████▋    | 287/500 [04:20<02:48,  1.26it/s] 58%|█████▊    | 289/500 [04:20<02:00,  1.75it/s] 58%|█████▊    | 291/500 [04:26<04:31,  1.30s/it] 59%|█████▊    | 293/500 [04:26<03:11,  1.08it/s] 59%|█████▉    | 295/500 [04:29<03:46,  1.11s/it] 59%|█████▉    | 297/500 [04:29<02:41,  1.26it/s] 60%|█████▉    | 299/500 [04:29<01:55,  1.74it/s] 60%|██████    | 301/500 [04:36<04:23,  1.33s/it] 61%|██████    | 303/500 [04:36<03:06,  1.05it/s]**************************************************learning rate decay**************************************************
Epoch:  246  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  247  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  248  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  249  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  250  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  251  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  252  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  253  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  254  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  255  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  256  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  257  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  258  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  259  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  260  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  261  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  262  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  263  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.49766308069229126
Epoch:  264  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  265  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  266  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  267  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  268  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  269  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  270  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  271  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  272  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  273  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  274  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  275  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  276  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  277  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  278  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  279  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  280  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  281  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  282  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  283  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  284  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  285  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  286  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  287  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  288  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  289  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  290  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  291  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  292  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  293  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  294  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  295  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  296  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  297  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  298  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  299  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  300  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  301  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  302  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  303  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.4976630210876465
Epoch:  304  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  305  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
 61%|██████    | 305/500 [04:39<03:37,  1.12s/it] 61%|██████▏   | 307/500 [04:39<02:34,  1.25it/s] 62%|██████▏   | 309/500 [04:39<01:50,  1.73it/s] 62%|██████▏   | 311/500 [04:45<04:05,  1.30s/it] 63%|██████▎   | 313/500 [04:45<02:54,  1.07it/s] 63%|██████▎   | 315/500 [04:48<03:25,  1.11s/it] 63%|██████▎   | 317/500 [04:48<02:26,  1.25it/s] 64%|██████▍   | 319/500 [04:48<01:44,  1.73it/s] 64%|██████▍   | 321/500 [04:54<03:50,  1.29s/it] 65%|██████▍   | 323/500 [04:54<02:43,  1.09it/s] 65%|██████▌   | 325/500 [04:58<03:12,  1.10s/it] 65%|██████▌   | 327/500 [04:58<02:16,  1.27it/s] 66%|██████▌   | 329/500 [04:58<01:37,  1.75it/s] 66%|██████▌   | 331/500 [05:04<03:41,  1.31s/it] 67%|██████▋   | 333/500 [05:04<02:36,  1.07it/s] 67%|██████▋   | 335/500 [05:07<03:03,  1.11s/it] 67%|██████▋   | 337/500 [05:07<02:10,  1.25it/s] 68%|██████▊   | 339/500 [05:07<01:33,  1.72it/s] 68%|██████▊   | 341/500 [05:13<03:26,  1.30s/it] 69%|██████▊   | 343/500 [05:13<02:25,  1.08it/s] 69%|██████▉   | 345/500 [05:16<02:51,  1.10s/it] 69%|██████▉   | 347/500 [05:17<02:01,  1.26it/s] 70%|██████▉   | 349/500 [05:17<01:26,  1.74it/s] 70%|███████   | 351/500 [05:23<03:12,  1.29s/it] 71%|███████   | 353/500 [05:23<02:15,  1.08it/s] 71%|███████   | 355/500 [05:26<02:40,  1.11s/it] 71%|███████▏  | 357/500 [05:26<01:53,  1.26it/s] 72%|███████▏  | 359/500 [05:26<01:21,  1.74it/s] 72%|███████▏  | 361/500 [05:32<02:59,  1.29s/it] 73%|███████▎  | 363/500 [05:32<02:06,  1.08it/s]**************************************************learning rate decay**************************************************
Epoch:  306  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  307  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.4976630210876465
Epoch:  308  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.49766305088996887
Epoch:  309  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  310  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  311  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  312  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  313  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  314  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  315  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  316  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  317  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  318  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  319  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  320  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  321  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  322  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  323  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  324  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  325  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  326  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  327  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  328  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  329  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  330  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  331  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  332  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.49766305088996887
Epoch:  333  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  334  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  335  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  336  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  337  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  338  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  339  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  340  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  341  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  342  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  343  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  344  	Training Loss: 0.45748379826545715
Test Loss:  0.5392844080924988
Valid Loss:  0.4976630210876465
Epoch:  345  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  346  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  347  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  348  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  349  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.49766305088996887
Epoch:  350  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  351  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  352  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  353  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  354  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  355  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.49766308069229126
**************************************************learning rate decay**************************************************
Epoch:  356  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  357  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  358  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  359  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  360  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  361  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  362  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  363  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  364  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  365  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
 73%|███████▎  | 365/500 [05:35<02:29,  1.10s/it] 73%|███████▎  | 367/500 [05:35<01:45,  1.26it/s] 74%|███████▍  | 369/500 [05:35<01:15,  1.74it/s] 74%|███████▍  | 371/500 [05:41<02:47,  1.30s/it] 75%|███████▍  | 373/500 [05:42<01:57,  1.08it/s] 75%|███████▌  | 375/500 [05:45<02:19,  1.12s/it] 75%|███████▌  | 377/500 [05:45<01:39,  1.24it/s] 76%|███████▌  | 379/500 [05:45<01:10,  1.71it/s] 76%|███████▌  | 381/500 [05:51<02:36,  1.32s/it] 77%|███████▋  | 383/500 [05:51<01:50,  1.06it/s] 77%|███████▋  | 385/500 [05:54<02:07,  1.11s/it] 77%|███████▋  | 387/500 [05:54<01:30,  1.25it/s] 78%|███████▊  | 389/500 [05:54<01:04,  1.73it/s] 78%|███████▊  | 391/500 [06:00<02:23,  1.32s/it] 79%|███████▊  | 393/500 [06:01<01:40,  1.06it/s] 79%|███████▉  | 395/500 [06:04<01:57,  1.12s/it] 79%|███████▉  | 397/500 [06:04<01:23,  1.24it/s] 80%|███████▉  | 399/500 [06:04<00:58,  1.71it/s] 80%|████████  | 401/500 [06:10<02:10,  1.31s/it] 81%|████████  | 403/500 [06:10<01:31,  1.06it/s] 81%|████████  | 405/500 [06:13<01:46,  1.12s/it] 81%|████████▏ | 407/500 [06:13<01:14,  1.25it/s] 82%|████████▏ | 409/500 [06:13<00:52,  1.72it/s] 82%|████████▏ | 411/500 [06:19<01:55,  1.30s/it] 83%|████████▎ | 413/500 [06:20<01:21,  1.07it/s] 83%|████████▎ | 415/500 [06:23<01:34,  1.11s/it] 83%|████████▎ | 417/500 [06:23<01:05,  1.26it/s] 84%|████████▍ | 419/500 [06:23<00:46,  1.74it/s] 84%|████████▍ | 421/500 [06:29<01:42,  1.30s/it] 85%|████████▍ | 423/500 [06:29<01:11,  1.08it/s]**************************************************learning rate decay**************************************************
Epoch:  366  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  367  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  368  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  369  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  370  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  371  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  372  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  373  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  374  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  375  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  376  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  377  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  378  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  379  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  380  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  381  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.4976630210876465
Epoch:  382  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  383  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.4976630210876465
Epoch:  384  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  385  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
**************************************************learning rate decay**************************************************
Epoch:  386  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  387  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  388  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  389  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  390  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  391  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  392  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  393  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  394  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  395  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  396  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  397  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  398  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  399  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  400  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  401  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  402  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  403  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  404  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  405  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  406  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  407  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  408  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  409  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  410  	Training Loss: 0.45748379826545715
Test Loss:  0.5392844080924988
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  411  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  412  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  413  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  414  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  415  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  416  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  417  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  418  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  419  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  420  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  421  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  422  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  423  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  424  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  425  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
 85%|████████▌ | 425/500 [06:32<01:23,  1.11s/it] 85%|████████▌ | 427/500 [06:32<00:58,  1.25it/s] 86%|████████▌ | 429/500 [06:32<00:40,  1.73it/s] 86%|████████▌ | 431/500 [06:38<01:30,  1.31s/it] 87%|████████▋ | 433/500 [06:38<01:02,  1.07it/s] 87%|████████▋ | 435/500 [06:39<00:43,  1.49it/s] 87%|████████▋ | 437/500 [06:39<00:30,  2.04it/s] 88%|████████▊ | 439/500 [06:39<00:22,  2.77it/s] 88%|████████▊ | 441/500 [06:45<01:07,  1.14s/it] 89%|████████▊ | 443/500 [06:45<00:46,  1.22it/s] 89%|████████▉ | 445/500 [06:48<00:56,  1.03s/it] 89%|████████▉ | 447/500 [06:48<00:39,  1.35it/s] 90%|████████▉ | 449/500 [06:48<00:27,  1.86it/s] 90%|█████████ | 451/500 [06:54<01:02,  1.27s/it] 91%|█████████ | 453/500 [06:54<00:42,  1.10it/s] 91%|█████████ | 455/500 [06:57<00:49,  1.10s/it] 91%|█████████▏| 457/500 [06:57<00:33,  1.27it/s] 92%|█████████▏| 459/500 [06:58<00:23,  1.76it/s] 92%|█████████▏| 461/500 [07:04<00:50,  1.29s/it] 93%|█████████▎| 463/500 [07:04<00:34,  1.08it/s] 93%|█████████▎| 465/500 [07:07<00:38,  1.11s/it] 93%|█████████▎| 467/500 [07:07<00:26,  1.26it/s] 94%|█████████▍| 469/500 [07:07<00:17,  1.74it/s] 94%|█████████▍| 471/500 [07:13<00:37,  1.29s/it] 95%|█████████▍| 473/500 [07:13<00:24,  1.08it/s] 95%|█████████▌| 475/500 [07:16<00:27,  1.10s/it] 95%|█████████▌| 477/500 [07:16<00:18,  1.27it/s] 96%|█████████▌| 479/500 [07:16<00:12,  1.75it/s] 96%|█████████▌| 481/500 [07:22<00:24,  1.30s/it] 97%|█████████▋| 483/500 [07:22<00:15,  1.08it/s] 97%|█████████▋| 485/500 [07:26<00:16,  1.11s/it]**************************************************learning rate decay**************************************************
Epoch:  426  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  427  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  428  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  429  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  430  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  431  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  432  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  433  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  434  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  435  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  436  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  437  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  438  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  439  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  440  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  441  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  442  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  443  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  444  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  445  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  446  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  447  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  448  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  449  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  450  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  451  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  452  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  453  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  454  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  455  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  456  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.49766305088996887
Epoch:  457  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  458  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  459  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  460  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  461  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  462  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  463  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  464  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  465  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  466  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  467  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  468  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  469  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.4976630210876465
Epoch:  470  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  471  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  472  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  473  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  474  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  475  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  476  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  477  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  478  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  479  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  480  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
Epoch:  481  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  482  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  483  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  484  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
Epoch:  485  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  486  	Training Loss: 0.45748376846313477
 97%|█████████▋| 487/500 [07:26<00:10,  1.25it/s] 98%|█████████▊| 489/500 [07:26<00:06,  1.73it/s] 98%|█████████▊| 491/500 [07:32<00:11,  1.30s/it] 99%|█████████▊| 493/500 [07:32<00:06,  1.07it/s] 99%|█████████▉| 495/500 [07:35<00:05,  1.11s/it] 99%|█████████▉| 497/500 [07:35<00:02,  1.26it/s]100%|█████████▉| 499/500 [07:35<00:00,  1.74it/s]100%|██████████| 500/500 [07:38<00:00,  1.09it/s]
Test Loss:  0.5392844080924988
Valid Loss:  0.4976630210876465
Epoch:  487  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  488  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.49766305088996887
Epoch:  489  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  490  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
**************************************************learning rate decay**************************************************
Epoch:  491  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  492  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  493  	Training Loss: 0.45748376846313477
Test Loss:  0.5392844080924988
Valid Loss:  0.49766305088996887
Epoch:  494  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  495  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766308069229126
**************************************************learning rate decay**************************************************
Epoch:  496  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
Epoch:  497  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  498  	Training Loss: 0.45748379826545715
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  499  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.49766305088996887
Epoch:  500  	Training Loss: 0.45748376846313477
Test Loss:  0.539284348487854
Valid Loss:  0.4976630210876465
**************************************************learning rate decay**************************************************
seed is  10
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:03<25:03,  3.01s/it]  1%|          | 3/500 [00:03<06:51,  1.21it/s]  1%|          | 5/500 [00:03<03:35,  2.30it/s]  1%|▏         | 7/500 [00:03<02:17,  3.59it/s]  2%|▏         | 9/500 [00:03<01:37,  5.06it/s]  2%|▏         | 11/500 [00:06<05:26,  1.50it/s]  3%|▎         | 13/500 [00:06<03:47,  2.14it/s]  3%|▎         | 15/500 [00:09<06:30,  1.24it/s]  3%|▎         | 17/500 [00:09<04:35,  1.75it/s]  4%|▍         | 19/500 [00:10<03:18,  2.42it/s]  4%|▍         | 21/500 [00:15<09:36,  1.20s/it]  5%|▍         | 23/500 [00:16<06:48,  1.17it/s]  5%|▌         | 25/500 [00:19<08:21,  1.06s/it]  5%|▌         | 27/500 [00:19<05:57,  1.32it/s]  6%|▌         | 29/500 [00:19<04:18,  1.83it/s]  6%|▌         | 31/500 [00:25<10:00,  1.28s/it]  7%|▋         | 33/500 [00:25<07:07,  1.09it/s]  7%|▋         | 35/500 [00:28<08:28,  1.09s/it]  7%|▋         | 37/500 [00:28<06:03,  1.27it/s]  8%|▊         | 39/500 [00:28<04:21,  1.76it/s]  8%|▊         | 41/500 [00:34<09:55,  1.30s/it]  9%|▊         | 43/500 [00:34<07:03,  1.08it/s]  9%|▉         | 45/500 [00:37<08:22,  1.11s/it]  9%|▉         | 47/500 [00:38<05:59,  1.26it/s] 10%|▉         | 49/500 [00:38<04:19,  1.74it/s] 10%|█         | 51/500 [00:44<09:42,  1.30s/it] 11%|█         | 53/500 [00:44<06:55,  1.08it/s] 11%|█         | 55/500 [00:47<08:12,  1.11s/it] 11%|█▏        | 57/500 [00:47<05:51,  1.26it/s] 12%|█▏        | 59/500 [00:47<04:13,  1.74it/s] 12%|█▏        | 61/500 [00:53<09:28,  1.29s/it]Epoch:  1  	Training Loss: 0.10188984870910645
Test Loss:  16.603065490722656
Valid Loss:  16.34926414489746
Epoch:  2  	Training Loss: 15.566265106201172
Test Loss:  51.88837432861328
Valid Loss:  49.64196014404297
Epoch:  3  	Training Loss: 49.228965759277344
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  4  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  5  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  6  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  7  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  8  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  9  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  10  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  11  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  12  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
Epoch:  13  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  14  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  15  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  16  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  17  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  18  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
Epoch:  19  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053980469703674
Epoch:  20  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  21  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  22  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  23  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  24  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  25  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  26  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  27  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  28  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  29  	Training Loss: 0.09359350055456161
Test Loss:  0.11147359013557434
Valid Loss:  0.13053981959819794
Epoch:  30  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  31  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  32  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  33  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  34  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  35  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  36  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  37  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  38  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  39  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  40  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  41  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  42  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  43  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  44  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  45  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  46  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  47  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  48  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  49  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  50  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  51  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  52  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  53  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  54  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053980469703674
Epoch:  55  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  56  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  57  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  58  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  59  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  60  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  61  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  62  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
 13%|█▎        | 63/500 [00:53<06:44,  1.08it/s] 13%|█▎        | 65/500 [00:56<07:59,  1.10s/it] 13%|█▎        | 67/500 [00:56<05:42,  1.26it/s] 14%|█▍        | 69/500 [00:56<04:06,  1.75it/s] 14%|█▍        | 71/500 [01:02<09:13,  1.29s/it] 15%|█▍        | 73/500 [01:03<06:33,  1.08it/s] 15%|█▌        | 75/500 [01:06<07:47,  1.10s/it] 15%|█▌        | 77/500 [01:06<05:34,  1.27it/s] 16%|█▌        | 79/500 [01:06<04:00,  1.75it/s] 16%|█▌        | 81/500 [01:12<08:59,  1.29s/it] 17%|█▋        | 83/500 [01:12<06:23,  1.09it/s] 17%|█▋        | 85/500 [01:15<07:35,  1.10s/it] 17%|█▋        | 87/500 [01:15<05:25,  1.27it/s] 18%|█▊        | 89/500 [01:15<03:54,  1.75it/s] 18%|█▊        | 91/500 [01:21<08:52,  1.30s/it] 19%|█▊        | 93/500 [01:21<06:18,  1.07it/s] 19%|█▉        | 95/500 [01:24<07:27,  1.11s/it] 19%|█▉        | 97/500 [01:24<05:19,  1.26it/s] 20%|█▉        | 99/500 [01:25<03:50,  1.74it/s] 20%|██        | 101/500 [01:31<08:39,  1.30s/it] 21%|██        | 103/500 [01:31<06:09,  1.08it/s] 21%|██        | 105/500 [01:34<07:16,  1.11s/it] 21%|██▏       | 107/500 [01:34<05:12,  1.26it/s] 22%|██▏       | 109/500 [01:34<03:44,  1.74it/s] 22%|██▏       | 111/500 [01:40<08:23,  1.30s/it] 23%|██▎       | 113/500 [01:40<05:59,  1.08it/s] 23%|██▎       | 115/500 [01:43<07:05,  1.11s/it] 23%|██▎       | 117/500 [01:43<05:03,  1.26it/s] 24%|██▍       | 119/500 [01:43<03:38,  1.74it/s] 24%|██▍       | 121/500 [01:49<08:13,  1.30s/it]Epoch:  63  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  64  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
Epoch:  65  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053980469703674
**************************************************learning rate decay**************************************************
Epoch:  66  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  67  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  68  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  69  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  70  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  71  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  72  	Training Loss: 0.09359349310398102
Test Loss:  0.11147356778383255
Valid Loss:  0.13053981959819794
Epoch:  73  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  74  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  75  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  76  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  77  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  78  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  79  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  80  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  81  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  82  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  83  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  84  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  85  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  86  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  87  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053980469703674
Epoch:  88  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  89  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  90  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  91  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  92  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  93  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  94  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  95  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  96  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  97  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  98  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  99  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  100  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  101  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  102  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  103  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  104  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  105  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  106  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  107  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  108  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053980469703674
Epoch:  109  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  110  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  111  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  112  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
Epoch:  113  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  114  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  115  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  116  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  117  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  118  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
Epoch:  119  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  120  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  121  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  122  	Training Loss: 0.09359349310398102
Test Loss:   25%|██▍       | 123/500 [01:50<05:51,  1.07it/s] 25%|██▌       | 125/500 [01:53<06:56,  1.11s/it] 25%|██▌       | 127/500 [01:53<04:57,  1.25it/s] 26%|██▌       | 129/500 [01:53<03:34,  1.73it/s] 26%|██▌       | 131/500 [01:59<08:01,  1.30s/it] 27%|██▋       | 133/500 [01:59<05:42,  1.07it/s] 27%|██▋       | 135/500 [02:02<06:45,  1.11s/it] 27%|██▋       | 137/500 [02:02<04:49,  1.26it/s] 28%|██▊       | 139/500 [02:02<03:28,  1.74it/s] 28%|██▊       | 141/500 [02:08<07:48,  1.31s/it] 29%|██▊       | 143/500 [02:08<05:33,  1.07it/s] 29%|██▉       | 145/500 [02:11<06:33,  1.11s/it] 29%|██▉       | 147/500 [02:12<04:41,  1.26it/s] 30%|██▉       | 149/500 [02:12<03:22,  1.73it/s] 30%|███       | 151/500 [02:18<07:33,  1.30s/it] 31%|███       | 153/500 [02:18<05:22,  1.07it/s] 31%|███       | 155/500 [02:21<06:23,  1.11s/it] 31%|███▏      | 157/500 [02:21<04:33,  1.25it/s] 32%|███▏      | 159/500 [02:21<03:16,  1.73it/s] 32%|███▏      | 161/500 [02:27<07:35,  1.34s/it] 33%|███▎      | 163/500 [02:28<05:24,  1.04it/s] 33%|███▎      | 165/500 [02:31<06:31,  1.17s/it] 33%|███▎      | 167/500 [02:31<04:39,  1.19it/s] 34%|███▍      | 169/500 [02:31<03:20,  1.65it/s] 34%|███▍      | 171/500 [02:37<07:14,  1.32s/it] 35%|███▍      | 173/500 [02:37<05:08,  1.06it/s] 35%|███▌      | 175/500 [02:40<06:02,  1.11s/it] 35%|███▌      | 177/500 [02:40<04:18,  1.25it/s] 36%|███▌      | 179/500 [02:41<03:05,  1.73it/s]0.11147357523441315
Valid Loss:  0.13053980469703674
Epoch:  123  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  124  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  125  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  126  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  127  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  128  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  129  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  130  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  131  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  132  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  133  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  134  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  135  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  136  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  137  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  138  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  139  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  140  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  141  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  142  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  143  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  144  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
Epoch:  145  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  146  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  147  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053980469703674
Epoch:  148  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  149  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  150  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  151  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  152  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  153  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
Epoch:  154  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  155  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  156  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  157  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  158  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
Epoch:  159  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053980469703674
Epoch:  160  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  161  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  162  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  163  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053980469703674
Epoch:  164  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  165  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  166  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053980469703674
Epoch:  167  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  168  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  169  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  170  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  171  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053980469703674
Epoch:  172  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  173  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  174  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  175  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  176  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  177  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  178  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  179  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  180  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  181  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
 36%|███▌      | 181/500 [02:46<06:55,  1.30s/it] 37%|███▋      | 183/500 [02:47<04:55,  1.07it/s] 37%|███▋      | 185/500 [02:50<05:56,  1.13s/it] 37%|███▋      | 187/500 [02:50<04:14,  1.23it/s] 38%|███▊      | 189/500 [02:50<03:03,  1.70it/s] 38%|███▊      | 191/500 [02:56<06:54,  1.34s/it] 39%|███▊      | 193/500 [02:56<04:54,  1.04it/s] 39%|███▉      | 195/500 [03:00<05:48,  1.14s/it] 39%|███▉      | 197/500 [03:00<04:08,  1.22it/s] 40%|███▉      | 199/500 [03:00<02:58,  1.69it/s] 40%|████      | 201/500 [03:06<06:38,  1.33s/it] 41%|████      | 203/500 [03:06<04:43,  1.05it/s] 41%|████      | 205/500 [03:09<05:34,  1.13s/it] 41%|████▏     | 207/500 [03:09<03:58,  1.23it/s] 42%|████▏     | 209/500 [03:09<02:51,  1.70it/s] 42%|████▏     | 211/500 [03:15<06:16,  1.30s/it] 43%|████▎     | 213/500 [03:16<04:27,  1.07it/s] 43%|████▎     | 215/500 [03:19<05:14,  1.10s/it] 43%|████▎     | 217/500 [03:19<03:43,  1.26it/s] 44%|████▍     | 219/500 [03:19<02:40,  1.75it/s] 44%|████▍     | 221/500 [03:25<06:03,  1.30s/it] 45%|████▍     | 223/500 [03:25<04:18,  1.07it/s] 45%|████▌     | 225/500 [03:28<05:08,  1.12s/it] 45%|████▌     | 227/500 [03:28<03:39,  1.24it/s] 46%|████▌     | 229/500 [03:28<02:37,  1.72it/s] 46%|████▌     | 231/500 [03:35<06:07,  1.36s/it] 47%|████▋     | 233/500 [03:35<04:20,  1.02it/s] 47%|████▋     | 235/500 [03:38<05:13,  1.18s/it] 47%|████▋     | 237/500 [03:38<03:43,  1.18it/s] 48%|████▊     | 239/500 [03:38<02:40,  1.63it/s]Valid Loss:  0.13053981959819794
Epoch:  182  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  183  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  184  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  185  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  186  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  187  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  188  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  189  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  190  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  191  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  192  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  193  	Training Loss: 0.09359349310398102
Test Loss:  0.11147359013557434
Valid Loss:  0.13053981959819794
Epoch:  194  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  195  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  196  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  197  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  198  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  199  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053980469703674
Epoch:  200  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  201  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  202  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  203  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  204  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  205  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  206  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  207  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  208  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  209  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  210  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  211  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  212  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  213  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  214  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  215  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  216  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  217  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053980469703674
Epoch:  218  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  219  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  220  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  221  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  222  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  223  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  224  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  225  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  226  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
Epoch:  227  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  228  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  229  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  230  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  231  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  232  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  233  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  234  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  235  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  236  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  237  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053980469703674
Epoch:  238  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  239  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  240  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
 48%|████▊     | 241/500 [03:44<05:45,  1.33s/it] 49%|████▊     | 243/500 [03:45<04:04,  1.05it/s] 49%|████▉     | 245/500 [03:48<04:45,  1.12s/it] 49%|████▉     | 247/500 [03:48<03:23,  1.24it/s] 50%|████▉     | 249/500 [03:48<02:26,  1.72it/s] 50%|█████     | 251/500 [03:54<05:26,  1.31s/it] 51%|█████     | 253/500 [03:54<03:51,  1.07it/s] 51%|█████     | 255/500 [03:57<04:33,  1.12s/it] 51%|█████▏    | 257/500 [03:57<03:14,  1.25it/s] 52%|█████▏    | 259/500 [03:57<02:19,  1.73it/s] 52%|█████▏    | 261/500 [04:03<05:13,  1.31s/it] 53%|█████▎    | 263/500 [04:04<03:42,  1.07it/s] 53%|█████▎    | 265/500 [04:07<04:22,  1.12s/it] 53%|█████▎    | 267/500 [04:07<03:06,  1.25it/s] 54%|█████▍    | 269/500 [04:07<02:14,  1.72it/s] 54%|█████▍    | 271/500 [04:13<05:05,  1.33s/it] 55%|█████▍    | 273/500 [04:13<03:36,  1.05it/s] 55%|█████▌    | 275/500 [04:16<04:11,  1.12s/it] 55%|█████▌    | 277/500 [04:16<02:58,  1.25it/s] 56%|█████▌    | 279/500 [04:16<02:08,  1.72it/s] 56%|█████▌    | 281/500 [04:22<04:48,  1.32s/it] 57%|█████▋    | 283/500 [04:23<03:24,  1.06it/s] 57%|█████▋    | 285/500 [04:26<04:00,  1.12s/it] 57%|█████▋    | 287/500 [04:26<02:50,  1.25it/s] 58%|█████▊    | 289/500 [04:26<02:02,  1.72it/s] 58%|█████▊    | 291/500 [04:32<04:36,  1.32s/it] 59%|█████▊    | 293/500 [04:32<03:16,  1.05it/s] 59%|█████▉    | 295/500 [04:35<03:48,  1.12s/it] 59%|█████▉    | 297/500 [04:35<02:42,  1.25it/s] 60%|█████▉    | 299/500 [04:35<01:56,  1.73it/s]**************************************************learning rate decay**************************************************
Epoch:  241  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  242  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  243  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  244  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  245  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053980469703674
**************************************************learning rate decay**************************************************
Epoch:  246  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  247  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  248  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
Epoch:  249  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  250  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  251  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  252  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  253  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  254  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
Epoch:  255  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  256  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  257  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  258  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  259  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  260  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  261  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  262  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  263  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  264  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  265  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  266  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  267  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  268  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  269  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  270  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  271  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  272  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  273  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  274  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  275  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  276  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
Epoch:  277  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  278  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  279  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  280  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  281  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  282  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  283  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  284  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  285  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  286  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  287  	Training Loss: 0.09359350800514221
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  288  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  289  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  290  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  291  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  292  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  293  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  294  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  295  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  296  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  297  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  298  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  299  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
 60%|██████    | 301/500 [04:42<04:20,  1.31s/it] 61%|██████    | 303/500 [04:42<03:04,  1.07it/s] 61%|██████    | 305/500 [04:45<03:37,  1.11s/it] 61%|██████▏   | 307/500 [04:45<02:34,  1.25it/s] 62%|██████▏   | 309/500 [04:45<01:50,  1.73it/s] 62%|██████▏   | 311/500 [04:51<04:06,  1.30s/it] 63%|██████▎   | 313/500 [04:51<02:54,  1.07it/s] 63%|██████▎   | 315/500 [04:54<03:24,  1.11s/it] 63%|██████▎   | 317/500 [04:54<02:25,  1.26it/s] 64%|██████▍   | 319/500 [04:54<01:43,  1.74it/s] 64%|██████▍   | 321/500 [05:00<03:52,  1.30s/it] 65%|██████▍   | 323/500 [05:00<02:44,  1.08it/s] 65%|██████▌   | 325/500 [05:03<03:13,  1.10s/it] 65%|██████▌   | 327/500 [05:04<02:17,  1.26it/s] 66%|██████▌   | 329/500 [05:04<01:38,  1.74it/s] 66%|██████▌   | 331/500 [05:10<03:46,  1.34s/it] 67%|██████▋   | 333/500 [05:10<02:40,  1.04it/s] 67%|██████▋   | 335/500 [05:13<03:05,  1.12s/it] 67%|██████▋   | 337/500 [05:13<02:11,  1.24it/s] 68%|██████▊   | 339/500 [05:13<01:33,  1.72it/s] 68%|██████▊   | 341/500 [05:19<03:26,  1.30s/it] 69%|██████▊   | 343/500 [05:19<02:25,  1.08it/s] 69%|██████▉   | 345/500 [05:23<02:54,  1.12s/it] 69%|██████▉   | 347/500 [05:23<02:03,  1.24it/s] 70%|██████▉   | 349/500 [05:23<01:28,  1.71it/s] 70%|███████   | 351/500 [05:29<03:25,  1.38s/it] 71%|███████   | 353/500 [05:29<02:24,  1.01it/s] 71%|███████   | 355/500 [05:33<02:47,  1.15s/it] 71%|███████▏  | 357/500 [05:33<01:58,  1.21it/s]Epoch:  300  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  301  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  302  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
Epoch:  303  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  304  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  305  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  306  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  307  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  308  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  309  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  310  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  311  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  312  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  313  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  314  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  315  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  316  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
Epoch:  317  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  318  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  319  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  320  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  321  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  322  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  323  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  324  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  325  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  326  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  327  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  328  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  329  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  330  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  331  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  332  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  333  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  334  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  335  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  336  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  337  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  338  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  339  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  340  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  341  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  342  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
Epoch:  343  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  344  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  345  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  346  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  347  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  348  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  349  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  350  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  351  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  352  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  353  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  354  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  355  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  356  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  357  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  358  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
 72%|███████▏  | 359/500 [05:33<01:24,  1.67it/s] 72%|███████▏  | 361/500 [05:39<03:03,  1.32s/it] 73%|███████▎  | 363/500 [05:39<02:09,  1.06it/s] 73%|███████▎  | 365/500 [05:42<02:31,  1.12s/it] 73%|███████▎  | 367/500 [05:42<01:47,  1.24it/s] 74%|███████▍  | 369/500 [05:42<01:16,  1.72it/s] 74%|███████▍  | 371/500 [05:48<02:47,  1.30s/it] 75%|███████▍  | 373/500 [05:48<01:57,  1.08it/s] 75%|███████▌  | 375/500 [05:51<02:18,  1.11s/it] 75%|███████▌  | 377/500 [05:52<01:37,  1.26it/s] 76%|███████▌  | 379/500 [05:52<01:09,  1.74it/s] 76%|███████▌  | 381/500 [05:58<02:35,  1.31s/it] 77%|███████▋  | 383/500 [05:58<01:49,  1.07it/s] 77%|███████▋  | 385/500 [06:01<02:08,  1.12s/it] 77%|███████▋  | 387/500 [06:01<01:30,  1.25it/s] 78%|███████▊  | 389/500 [06:01<01:04,  1.73it/s] 78%|███████▊  | 391/500 [06:07<02:21,  1.30s/it] 79%|███████▊  | 393/500 [06:07<01:39,  1.08it/s] 79%|███████▉  | 395/500 [06:10<01:55,  1.10s/it] 79%|███████▉  | 397/500 [06:10<01:21,  1.26it/s] 80%|███████▉  | 399/500 [06:11<00:57,  1.74it/s] 80%|████████  | 401/500 [06:17<02:08,  1.30s/it] 81%|████████  | 403/500 [06:17<01:29,  1.08it/s] 81%|████████  | 405/500 [06:20<01:45,  1.11s/it] 81%|████████▏ | 407/500 [06:20<01:14,  1.25it/s] 82%|████████▏ | 409/500 [06:20<00:52,  1.73it/s] 82%|████████▏ | 411/500 [06:26<01:55,  1.30s/it] 83%|████████▎ | 413/500 [06:26<01:20,  1.08it/s] 83%|████████▎ | 415/500 [06:29<01:34,  1.11s/it] 83%|████████▎ | 417/500 [06:29<01:05,  1.26it/s]Epoch:  359  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  360  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  361  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  362  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  363  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  364  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  365  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  366  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  367  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  368  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  369  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  370  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  371  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  372  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  373  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  374  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
Epoch:  375  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  376  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  377  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  378  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  379  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  380  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  381  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  382  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  383  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  384  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  385  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  386  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  387  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  388  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  389  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053980469703674
Epoch:  390  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  391  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  392  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  393  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  394  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  395  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  396  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  397  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  398  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  399  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  400  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  401  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  402  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  403  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  404  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  405  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053980469703674
**************************************************learning rate decay**************************************************
Epoch:  406  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  407  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  408  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  409  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053980469703674
Epoch:  410  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  411  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  412  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  413  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  414  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  415  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  416  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  417  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
 84%|████████▍ | 419/500 [06:29<00:46,  1.74it/s] 84%|████████▍ | 421/500 [06:35<01:43,  1.32s/it] 85%|████████▍ | 423/500 [06:36<01:12,  1.06it/s] 85%|████████▌ | 425/500 [06:39<01:23,  1.12s/it] 85%|████████▌ | 427/500 [06:39<00:58,  1.24it/s] 86%|████████▌ | 429/500 [06:39<00:41,  1.72it/s] 86%|████████▌ | 431/500 [06:45<01:31,  1.32s/it] 87%|████████▋ | 433/500 [06:45<01:03,  1.06it/s] 87%|████████▋ | 435/500 [06:48<01:13,  1.13s/it] 87%|████████▋ | 437/500 [06:48<00:51,  1.23it/s] 88%|████████▊ | 439/500 [06:49<00:35,  1.70it/s] 88%|████████▊ | 441/500 [06:55<01:16,  1.30s/it] 89%|████████▊ | 443/500 [06:55<00:53,  1.07it/s] 89%|████████▉ | 445/500 [06:58<01:01,  1.12s/it] 89%|████████▉ | 447/500 [06:58<00:42,  1.25it/s] 90%|████████▉ | 449/500 [06:58<00:29,  1.72it/s] 90%|█████████ | 451/500 [07:04<01:03,  1.30s/it] 91%|█████████ | 453/500 [07:04<00:43,  1.07it/s] 91%|█████████ | 455/500 [07:04<00:30,  1.49it/s] 91%|█████████▏| 457/500 [07:04<00:21,  2.05it/s] 92%|█████████▏| 459/500 [07:04<00:14,  2.77it/s] 92%|█████████▏| 461/500 [07:10<00:44,  1.14s/it] 93%|█████████▎| 463/500 [07:11<00:30,  1.22it/s] 93%|█████████▎| 465/500 [07:14<00:35,  1.03s/it] 93%|█████████▎| 467/500 [07:14<00:24,  1.35it/s] 94%|█████████▍| 469/500 [07:14<00:16,  1.87it/s] 94%|█████████▍| 471/500 [07:20<00:36,  1.26s/it] 95%|█████████▍| 473/500 [07:20<00:24,  1.11it/s] 95%|█████████▌| 475/500 [07:23<00:27,  1.09s/it] 95%|█████████▌| 477/500 [07:23<00:17,  1.28it/s]Epoch:  418  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  419  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  420  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  421  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  422  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  423  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  424  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053980469703674
Epoch:  425  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  426  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  427  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  428  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  429  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  430  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  431  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  432  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  433  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  434  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053980469703674
Epoch:  435  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  436  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
Epoch:  437  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  438  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  439  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  440  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  441  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
Epoch:  442  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  443  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  444  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  445  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  446  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  447  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  448  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  449  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  450  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  451  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  452  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  453  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  454  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  455  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  456  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  457  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  458  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  459  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  460  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  461  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  462  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  463  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  464  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  465  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053980469703674
**************************************************learning rate decay**************************************************
Epoch:  466  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  467  	Training Loss: 0.09359350800514221
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  468  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  469  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  470  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  471  	Training Loss: 0.09359349310398102
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
Epoch:  472  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  473  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  474  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  475  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  476  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  477  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053983449935913
 96%|█████████▌| 479/500 [07:23<00:11,  1.77it/s] 96%|█████████▌| 481/500 [07:29<00:24,  1.29s/it] 97%|█████████▋| 483/500 [07:29<00:15,  1.08it/s] 97%|█████████▋| 485/500 [07:32<00:16,  1.11s/it] 97%|█████████▋| 487/500 [07:32<00:10,  1.26it/s] 98%|█████████▊| 489/500 [07:33<00:06,  1.74it/s] 98%|█████████▊| 491/500 [07:39<00:11,  1.30s/it] 99%|█████████▊| 493/500 [07:39<00:06,  1.08it/s] 99%|█████████▉| 495/500 [07:42<00:05,  1.11s/it] 99%|█████████▉| 497/500 [07:42<00:02,  1.25it/s]100%|█████████▉| 499/500 [07:42<00:00,  1.73it/s]100%|██████████| 500/500 [07:45<00:00,  1.07it/s]
Epoch:  478  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  479  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  480  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  481  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  482  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  483  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  484  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  485  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
**************************************************learning rate decay**************************************************
Epoch:  486  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  487  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  488  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  489  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  490  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  491  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  492  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  493  	Training Loss: 0.09359350055456161
Test Loss:  0.11147358268499374
Valid Loss:  0.13053981959819794
Epoch:  494  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  495  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
Epoch:  496  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053980469703674
Epoch:  497  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053983449935913
Epoch:  498  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  499  	Training Loss: 0.09359349310398102
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
Epoch:  500  	Training Loss: 0.09359350055456161
Test Loss:  0.11147357523441315
Valid Loss:  0.13053981959819794
**************************************************learning rate decay**************************************************
seed is  11
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:02<24:55,  3.00s/it]  1%|          | 3/500 [00:03<06:49,  1.21it/s]  1%|          | 5/500 [00:03<03:34,  2.31it/s]  1%|▏         | 7/500 [00:03<02:16,  3.61it/s]  2%|▏         | 9/500 [00:03<01:36,  5.10it/s]  2%|▏         | 11/500 [00:06<05:23,  1.51it/s]  3%|▎         | 13/500 [00:06<03:45,  2.16it/s]  3%|▎         | 15/500 [00:09<06:29,  1.25it/s]  3%|▎         | 17/500 [00:09<04:35,  1.76it/s]  4%|▍         | 19/500 [00:09<03:18,  2.43it/s]  4%|▍         | 21/500 [00:15<09:37,  1.21s/it]  5%|▍         | 23/500 [00:16<06:49,  1.17it/s]  5%|▌         | 25/500 [00:19<08:22,  1.06s/it]  5%|▌         | 27/500 [00:19<05:58,  1.32it/s]  6%|▌         | 29/500 [00:19<04:18,  1.82it/s]  6%|▌         | 31/500 [00:25<10:02,  1.28s/it]  7%|▋         | 33/500 [00:25<07:08,  1.09it/s]  7%|▋         | 35/500 [00:28<08:32,  1.10s/it]  7%|▋         | 37/500 [00:28<06:05,  1.27it/s]  8%|▊         | 39/500 [00:28<04:23,  1.75it/s]  8%|▊         | 41/500 [00:34<09:55,  1.30s/it]  9%|▊         | 43/500 [00:34<07:03,  1.08it/s]  9%|▉         | 45/500 [00:37<08:24,  1.11s/it]  9%|▉         | 47/500 [00:38<06:00,  1.26it/s] 10%|▉         | 49/500 [00:38<04:19,  1.73it/s] 10%|█         | 51/500 [00:44<09:42,  1.30s/it] 11%|█         | 53/500 [00:44<06:55,  1.08it/s] 11%|█         | 55/500 [00:47<08:11,  1.10s/it] 11%|█▏        | 57/500 [00:47<05:50,  1.26it/s] 12%|█▏        | 59/500 [00:47<04:12,  1.74it/s] 12%|█▏        | 61/500 [00:53<09:31,  1.30s/it]Epoch:  1  	Training Loss: 0.10591993480920792
Test Loss:  211.88253784179688
Valid Loss:  204.689453125
Epoch:  2  	Training Loss: 203.284912109375
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  3  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  4  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  5  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  6  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  7  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  8  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  9  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  10  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  11  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  12  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  13  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  14  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  15  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  16  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  17  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  18  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  19  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  20  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  21  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  22  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173267900943756
Epoch:  23  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  24  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  25  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  26  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  27  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  28  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  29  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  30  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  31  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  32  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  33  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  34  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  35  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  36  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  37  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  38  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  39  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  40  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  41  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  42  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  43  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  44  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  45  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  46  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  47  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  48  	Training Loss: 0.1300297975540161
Test Loss:  0.17489862442016602
Valid Loss:  0.17173269391059875
Epoch:  49  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173267900943756
Epoch:  50  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  51  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  52  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  53  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  54  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  55  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  56  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  57  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  58  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  59  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  60  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  61  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  62  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  63  	Training Loss: 0.1300297975540161
Test Loss:   13%|█▎        | 63/500 [00:53<06:46,  1.08it/s] 13%|█▎        | 65/500 [00:56<08:04,  1.11s/it] 13%|█▎        | 67/500 [00:56<05:47,  1.25it/s] 14%|█▍        | 69/500 [00:57<04:10,  1.72it/s] 14%|█▍        | 71/500 [01:03<09:16,  1.30s/it] 15%|█▍        | 73/500 [01:03<06:36,  1.08it/s] 15%|█▌        | 75/500 [01:06<07:50,  1.11s/it] 15%|█▌        | 77/500 [01:06<05:36,  1.26it/s] 16%|█▌        | 79/500 [01:06<04:02,  1.74it/s] 16%|█▌        | 81/500 [01:12<09:23,  1.35s/it] 17%|█▋        | 83/500 [01:12<06:40,  1.04it/s] 17%|█▋        | 85/500 [01:15<07:46,  1.12s/it] 17%|█▋        | 87/500 [01:16<05:33,  1.24it/s] 18%|█▊        | 89/500 [01:16<04:00,  1.71it/s] 18%|█▊        | 91/500 [01:22<08:51,  1.30s/it] 19%|█▊        | 93/500 [01:22<06:21,  1.07it/s] 19%|█▉        | 95/500 [01:25<07:29,  1.11s/it] 19%|█▉        | 97/500 [01:25<05:21,  1.26it/s] 20%|█▉        | 99/500 [01:25<03:51,  1.73it/s] 20%|██        | 101/500 [01:31<08:38,  1.30s/it] 21%|██        | 103/500 [01:31<06:08,  1.08it/s] 21%|██        | 105/500 [01:34<07:20,  1.12s/it] 21%|██▏       | 107/500 [01:34<05:14,  1.25it/s] 22%|██▏       | 109/500 [01:35<03:46,  1.72it/s] 22%|██▏       | 111/500 [01:40<08:24,  1.30s/it] 23%|██▎       | 113/500 [01:41<05:59,  1.08it/s] 23%|██▎       | 115/500 [01:44<07:07,  1.11s/it] 23%|██▎       | 117/500 [01:44<05:04,  1.26it/s] 24%|██▍       | 119/500 [01:44<03:39,  1.74it/s] 24%|██▍       | 121/500 [01:50<08:11,  1.30s/it]0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  64  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  65  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  66  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  67  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  68  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  69  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173267900943756
Epoch:  70  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  71  	Training Loss: 0.1300297975540161
Test Loss:  0.17489862442016602
Valid Loss:  0.17173269391059875
Epoch:  72  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173267900943756
Epoch:  73  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  74  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  75  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  76  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  77  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  78  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  79  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  80  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  81  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  82  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173267900943756
Epoch:  83  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  84  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  85  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  86  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  87  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  88  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  89  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  90  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  91  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  92  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  93  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  94  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  95  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  96  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  97  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  98  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  99  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  100  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  101  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  102  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  103  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  104  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  105  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  106  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  107  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  108  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  109  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  110  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  111  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  112  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173267900943756
Epoch:  113  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  114  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  115  	Training Loss: 0.1300298124551773
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  116  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  117  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  118  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  119  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  120  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  121  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  122  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  123  	Training Loss: 0.1300298124551773
Test Loss:   25%|██▍       | 123/500 [01:50<05:49,  1.08it/s] 25%|██▌       | 125/500 [01:53<06:57,  1.11s/it] 25%|██▌       | 127/500 [01:53<04:58,  1.25it/s] 26%|██▌       | 129/500 [01:53<03:35,  1.72it/s] 26%|██▌       | 131/500 [01:59<08:03,  1.31s/it] 27%|██▋       | 133/500 [02:00<05:44,  1.07it/s] 27%|██▋       | 135/500 [02:03<06:47,  1.12s/it] 27%|██▋       | 137/500 [02:03<04:51,  1.25it/s] 28%|██▊       | 139/500 [02:03<03:29,  1.72it/s] 28%|██▊       | 141/500 [02:09<07:51,  1.31s/it] 29%|██▊       | 143/500 [02:09<05:35,  1.07it/s] 29%|██▉       | 145/500 [02:13<07:00,  1.18s/it] 29%|██▉       | 147/500 [02:13<04:59,  1.18it/s] 30%|██▉       | 149/500 [02:13<03:35,  1.63it/s] 30%|███       | 151/500 [02:19<07:40,  1.32s/it] 31%|███       | 153/500 [02:19<05:27,  1.06it/s] 31%|███       | 155/500 [02:22<06:24,  1.11s/it] 31%|███▏      | 157/500 [02:22<04:34,  1.25it/s] 32%|███▏      | 159/500 [02:22<03:17,  1.73it/s] 32%|███▏      | 161/500 [02:28<07:18,  1.29s/it] 33%|███▎      | 163/500 [02:28<05:11,  1.08it/s] 33%|███▎      | 165/500 [02:31<06:07,  1.10s/it] 33%|███▎      | 167/500 [02:31<04:22,  1.27it/s] 34%|███▍      | 169/500 [02:32<03:09,  1.75it/s] 34%|███▍      | 171/500 [02:37<07:04,  1.29s/it] 35%|███▍      | 173/500 [02:38<05:01,  1.08it/s] 35%|███▌      | 175/500 [02:41<05:57,  1.10s/it] 35%|███▌      | 177/500 [02:41<04:14,  1.27it/s] 36%|███▌      | 179/500 [02:41<03:03,  1.75it/s] 36%|███▌      | 181/500 [02:47<07:07,  1.34s/it]0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  124  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  125  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  126  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  127  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  128  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  129  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173267900943756
Epoch:  130  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  131  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  132  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  133  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173267900943756
Epoch:  134  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  135  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  136  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  137  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  138  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  139  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  140  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  141  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  142  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  143  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  144  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  145  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  146  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  147  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  148  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173267900943756
Epoch:  149  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  150  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  151  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  152  	Training Loss: 0.1300297975540161
Test Loss:  0.17489862442016602
Valid Loss:  0.17173269391059875
Epoch:  153  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  154  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  155  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  156  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  157  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  158  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  159  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  160  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  161  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  162  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  163  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  164  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  165  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  166  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  167  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  168  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  169  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  170  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  171  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  172  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  173  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  174  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  175  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  176  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  177  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  178  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  179  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  180  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  181  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  182  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
 37%|███▋      | 183/500 [02:47<05:03,  1.04it/s] 37%|███▋      | 185/500 [02:50<05:56,  1.13s/it] 37%|███▋      | 187/500 [02:50<04:14,  1.23it/s] 38%|███▊      | 189/500 [02:51<03:02,  1.70it/s] 38%|███▊      | 191/500 [02:57<06:43,  1.31s/it] 39%|███▊      | 193/500 [02:57<04:46,  1.07it/s] 39%|███▉      | 195/500 [03:00<05:36,  1.10s/it] 39%|███▉      | 197/500 [03:00<04:00,  1.26it/s] 40%|███▉      | 199/500 [03:00<02:52,  1.74it/s] 40%|████      | 201/500 [03:06<06:32,  1.31s/it] 41%|████      | 203/500 [03:06<04:38,  1.07it/s] 41%|████      | 205/500 [03:09<05:30,  1.12s/it] 41%|████▏     | 207/500 [03:09<03:56,  1.24it/s] 42%|████▏     | 209/500 [03:10<02:49,  1.71it/s] 42%|████▏     | 211/500 [03:15<06:17,  1.30s/it] 43%|████▎     | 213/500 [03:16<04:28,  1.07it/s] 43%|████▎     | 215/500 [03:19<05:15,  1.11s/it] 43%|████▎     | 217/500 [03:19<03:44,  1.26it/s] 44%|████▍     | 219/500 [03:19<02:41,  1.74it/s] 44%|████▍     | 221/500 [03:25<06:10,  1.33s/it] 45%|████▍     | 223/500 [03:25<04:22,  1.05it/s] 45%|████▌     | 225/500 [03:28<05:17,  1.15s/it] 45%|████▌     | 227/500 [03:29<03:46,  1.21it/s] 46%|████▌     | 229/500 [03:29<02:42,  1.66it/s] 46%|████▌     | 231/500 [03:35<06:05,  1.36s/it] 47%|████▋     | 233/500 [03:35<04:19,  1.03it/s] 47%|████▋     | 235/500 [03:38<04:59,  1.13s/it] 47%|████▋     | 237/500 [03:38<03:33,  1.23it/s] 48%|████▊     | 239/500 [03:38<02:33,  1.70it/s] 48%|████▊     | 241/500 [03:44<05:41,  1.32s/it]Epoch:  183  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  184  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  185  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  186  	Training Loss: 0.1300298124551773
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  187  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  188  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  189  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  190  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  191  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  192  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  193  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  194  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  195  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  196  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  197  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  198  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  199  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  200  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  201  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  202  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  203  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  204  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  205  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  206  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  207  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  208  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  209  	Training Loss: 0.1300298124551773
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  210  	Training Loss: 0.1300297975540161
Test Loss:  0.17489862442016602
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  211  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  212  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  213  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  214  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  215  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173267900943756
**************************************************learning rate decay**************************************************
Epoch:  216  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  217  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  218  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  219  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  220  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  221  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  222  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  223  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  224  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  225  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  226  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  227  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  228  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  229  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  230  	Training Loss: 0.1300298124551773
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  231  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  232  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  233  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  234  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  235  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  236  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  237  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  238  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  239  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  240  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  241  	Training Loss: 0.1300298124551773
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  242  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
 49%|████▊     | 243/500 [03:45<04:02,  1.06it/s] 49%|████▉     | 245/500 [03:48<04:44,  1.12s/it] 49%|████▉     | 247/500 [03:48<03:22,  1.25it/s] 50%|████▉     | 249/500 [03:48<02:25,  1.72it/s] 50%|█████     | 251/500 [03:54<05:25,  1.31s/it] 51%|█████     | 253/500 [03:54<03:51,  1.07it/s] 51%|█████     | 255/500 [03:57<04:33,  1.12s/it] 51%|█████▏    | 257/500 [03:57<03:14,  1.25it/s] 52%|█████▏    | 259/500 [03:57<02:19,  1.72it/s] 52%|█████▏    | 261/500 [04:03<05:10,  1.30s/it] 53%|█████▎    | 263/500 [04:03<03:40,  1.08it/s] 53%|█████▎    | 265/500 [04:07<04:20,  1.11s/it] 53%|█████▎    | 267/500 [04:07<03:05,  1.26it/s] 54%|█████▍    | 269/500 [04:07<02:13,  1.73it/s] 54%|█████▍    | 271/500 [04:13<04:57,  1.30s/it] 55%|█████▍    | 273/500 [04:13<03:30,  1.08it/s] 55%|█████▌    | 275/500 [04:16<04:08,  1.11s/it] 55%|█████▌    | 277/500 [04:16<02:56,  1.26it/s] 56%|█████▌    | 279/500 [04:16<02:06,  1.74it/s] 56%|█████▌    | 281/500 [04:22<04:44,  1.30s/it] 57%|█████▋    | 283/500 [04:22<03:21,  1.08it/s] 57%|█████▋    | 285/500 [04:25<03:58,  1.11s/it] 57%|█████▋    | 287/500 [04:25<02:49,  1.26it/s] 58%|█████▊    | 289/500 [04:26<02:01,  1.73it/s] 58%|█████▊    | 291/500 [04:32<04:30,  1.30s/it] 59%|█████▊    | 293/500 [04:32<03:11,  1.08it/s] 59%|█████▉    | 295/500 [04:35<03:47,  1.11s/it] 59%|█████▉    | 297/500 [04:35<02:41,  1.26it/s] 60%|█████▉    | 299/500 [04:35<01:55,  1.74it/s] 60%|██████    | 301/500 [04:41<04:19,  1.31s/it]Valid Loss:  0.17173269391059875
Epoch:  243  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  244  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  245  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  246  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  247  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  248  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  249  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  250  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  251  	Training Loss: 0.1300298124551773
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  252  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173267900943756
Epoch:  253  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  254  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  255  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  256  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  257  	Training Loss: 0.1300297975540161
Test Loss:  0.17489862442016602
Valid Loss:  0.17173269391059875
Epoch:  258  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  259  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  260  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  261  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  262  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  263  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173267900943756
Epoch:  264  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  265  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  266  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  267  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  268  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  269  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  270  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  271  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  272  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  273  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  274  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  275  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173267900943756
**************************************************learning rate decay**************************************************
Epoch:  276  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  277  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  278  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  279  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  280  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  281  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  282  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  283  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  284  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  285  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  286  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  287  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  288  	Training Loss: 0.1300297975540161
Test Loss:  0.17489862442016602
Valid Loss:  0.17173269391059875
Epoch:  289  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  290  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  291  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  292  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  293  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  294  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  295  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  296  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  297  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  298  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  299  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  300  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  301  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  302  	Training Loss: 0.1300297975540161
 61%|██████    | 303/500 [04:41<03:03,  1.07it/s] 61%|██████    | 305/500 [04:44<03:36,  1.11s/it] 61%|██████▏   | 307/500 [04:44<02:33,  1.26it/s] 62%|██████▏   | 309/500 [04:44<01:50,  1.73it/s] 62%|██████▏   | 311/500 [04:50<04:05,  1.30s/it] 63%|██████▎   | 313/500 [04:51<02:53,  1.08it/s] 63%|██████▎   | 315/500 [04:54<03:24,  1.11s/it] 63%|██████▎   | 317/500 [04:54<02:25,  1.26it/s] 64%|██████▍   | 319/500 [04:54<01:44,  1.74it/s] 64%|██████▍   | 321/500 [05:00<03:52,  1.30s/it] 65%|██████▍   | 323/500 [05:00<02:44,  1.07it/s] 65%|██████▌   | 325/500 [05:03<03:13,  1.10s/it] 65%|██████▌   | 327/500 [05:03<02:17,  1.26it/s] 66%|██████▌   | 329/500 [05:03<01:38,  1.74it/s] 66%|██████▌   | 331/500 [05:09<03:38,  1.29s/it] 67%|██████▋   | 333/500 [05:09<02:34,  1.08it/s] 67%|██████▋   | 335/500 [05:12<03:01,  1.10s/it] 67%|██████▋   | 337/500 [05:12<02:08,  1.27it/s] 68%|██████▊   | 339/500 [05:13<01:32,  1.75it/s] 68%|██████▊   | 341/500 [05:19<03:24,  1.29s/it] 69%|██████▊   | 343/500 [05:19<02:24,  1.09it/s] 69%|██████▉   | 345/500 [05:22<02:50,  1.10s/it] 69%|██████▉   | 347/500 [05:22<02:00,  1.27it/s] 70%|██████▉   | 349/500 [05:22<01:26,  1.75it/s] 70%|███████   | 351/500 [05:28<03:11,  1.29s/it] 71%|███████   | 353/500 [05:28<02:15,  1.09it/s] 71%|███████   | 355/500 [05:31<02:39,  1.10s/it] 71%|███████▏  | 357/500 [05:31<01:52,  1.27it/s] 72%|███████▏  | 359/500 [05:31<01:20,  1.75it/s] 72%|███████▏  | 361/500 [05:37<02:59,  1.29s/it]Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  303  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  304  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  305  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  306  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  307  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  308  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  309  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  310  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  311  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  312  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  313  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  314  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  315  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  316  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  317  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  318  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173267900943756
Epoch:  319  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  320  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  321  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  322  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  323  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  324  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173267900943756
Epoch:  325  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  326  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  327  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  328  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  329  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  330  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  331  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  332  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  333  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  334  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  335  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  336  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173267900943756
Epoch:  337  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  338  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  339  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  340  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  341  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  342  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  343  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  344  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  345  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  346  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  347  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  348  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  349  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  350  	Training Loss: 0.1300297975540161
Test Loss:  0.17489862442016602
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  351  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  352  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  353  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  354  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173267900943756
Epoch:  355  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  356  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  357  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  358  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  359  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  360  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  361  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
 73%|███████▎  | 363/500 [05:37<02:06,  1.08it/s] 73%|███████▎  | 365/500 [05:41<02:33,  1.14s/it] 73%|███████▎  | 367/500 [05:41<01:48,  1.22it/s] 74%|███████▍  | 369/500 [05:41<01:17,  1.69it/s] 74%|███████▍  | 371/500 [05:47<02:48,  1.31s/it] 75%|███████▍  | 373/500 [05:47<01:58,  1.07it/s] 75%|███████▌  | 375/500 [05:50<02:18,  1.11s/it] 75%|███████▌  | 377/500 [05:50<01:37,  1.26it/s] 76%|███████▌  | 379/500 [05:50<01:09,  1.74it/s] 76%|███████▌  | 381/500 [05:56<02:33,  1.29s/it] 77%|███████▋  | 383/500 [05:56<01:47,  1.09it/s] 77%|███████▋  | 385/500 [05:59<02:06,  1.10s/it] 77%|███████▋  | 387/500 [05:59<01:29,  1.27it/s] 78%|███████▊  | 389/500 [06:00<01:03,  1.75it/s] 78%|███████▊  | 391/500 [06:06<02:21,  1.29s/it] 79%|███████▊  | 393/500 [06:06<01:39,  1.08it/s] 79%|███████▉  | 395/500 [06:09<01:55,  1.10s/it] 79%|███████▉  | 397/500 [06:09<01:21,  1.27it/s] 80%|███████▉  | 399/500 [06:09<00:57,  1.75it/s] 80%|████████  | 401/500 [06:15<02:11,  1.33s/it] 81%|████████  | 403/500 [06:15<01:32,  1.05it/s] 81%|████████  | 405/500 [06:18<01:46,  1.12s/it] 81%|████████▏ | 407/500 [06:18<01:15,  1.24it/s] 82%|████████▏ | 409/500 [06:19<00:53,  1.71it/s] 82%|████████▏ | 411/500 [06:25<01:55,  1.30s/it] 83%|████████▎ | 413/500 [06:25<01:20,  1.07it/s] 83%|████████▎ | 415/500 [06:28<01:34,  1.11s/it] 83%|████████▎ | 417/500 [06:28<01:05,  1.26it/s] 84%|████████▍ | 419/500 [06:28<00:46,  1.74it/s]Epoch:  362  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  363  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173267900943756
Epoch:  364  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  365  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  366  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  367  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  368  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  369  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173267900943756
Epoch:  370  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  371  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173267900943756
Epoch:  372  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  373  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  374  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  375  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  376  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  377  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  378  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  379  	Training Loss: 0.1300297975540161
Test Loss:  0.17489862442016602
Valid Loss:  0.17173269391059875
Epoch:  380  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  381  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  382  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  383  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  384  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  385  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  386  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  387  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  388  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  389  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  390  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  391  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  392  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  393  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  394  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  395  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  396  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  397  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  398  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  399  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  400  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  401  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  402  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  403  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  404  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173267900943756
Epoch:  405  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173267900943756
**************************************************learning rate decay**************************************************
Epoch:  406  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  407  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  408  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  409  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  410  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  411  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  412  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  413  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  414  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  415  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  416  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  417  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  418  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  419  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  420  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  421  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
 84%|████████▍ | 421/500 [06:34<01:42,  1.30s/it] 85%|████████▍ | 423/500 [06:34<01:11,  1.08it/s] 85%|████████▌ | 425/500 [06:37<01:22,  1.11s/it] 85%|████████▌ | 427/500 [06:37<00:58,  1.26it/s] 86%|████████▌ | 429/500 [06:37<00:40,  1.74it/s] 86%|████████▌ | 431/500 [06:43<01:30,  1.31s/it] 87%|████████▋ | 433/500 [06:44<01:02,  1.07it/s] 87%|████████▋ | 435/500 [06:47<01:12,  1.12s/it] 87%|████████▋ | 437/500 [06:47<00:50,  1.24it/s] 88%|████████▊ | 439/500 [06:47<00:35,  1.72it/s] 88%|████████▊ | 441/500 [06:53<01:16,  1.30s/it] 89%|████████▊ | 443/500 [06:53<00:53,  1.07it/s] 89%|████████▉ | 445/500 [06:56<01:01,  1.11s/it] 89%|████████▉ | 447/500 [06:56<00:42,  1.25it/s] 90%|████████▉ | 449/500 [06:56<00:29,  1.73it/s] 90%|█████████ | 451/500 [07:02<01:03,  1.30s/it] 91%|█████████ | 453/500 [07:02<00:43,  1.07it/s] 91%|█████████ | 455/500 [07:06<00:49,  1.11s/it] 91%|█████████▏| 457/500 [07:06<00:34,  1.26it/s] 92%|█████████▏| 459/500 [07:06<00:23,  1.74it/s] 92%|█████████▏| 461/500 [07:12<00:51,  1.32s/it] 93%|█████████▎| 463/500 [07:12<00:34,  1.06it/s] 93%|█████████▎| 465/500 [07:15<00:39,  1.11s/it] 93%|█████████▎| 467/500 [07:15<00:26,  1.25it/s] 94%|█████████▍| 469/500 [07:15<00:17,  1.73it/s] 94%|█████████▍| 471/500 [07:21<00:37,  1.31s/it] 95%|█████████▍| 473/500 [07:21<00:25,  1.07it/s] 95%|█████████▌| 475/500 [07:24<00:27,  1.11s/it] 95%|█████████▌| 477/500 [07:25<00:18,  1.26it/s] 96%|█████████▌| 479/500 [07:25<00:12,  1.74it/s]Valid Loss:  0.17173269391059875
Epoch:  422  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  423  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  424  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  425  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  426  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173267900943756
Epoch:  427  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173267900943756
Epoch:  428  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  429  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  430  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  431  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  432  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  433  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173267900943756
Epoch:  434  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  435  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  436  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  437  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  438  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  439  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  440  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  441  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  442  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  443  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  444  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  445  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  446  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  447  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  448  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  449  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  450  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  451  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  452  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  453  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  454  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  455  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  456  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  457  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173267900943756
Epoch:  458  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  459  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173267900943756
Epoch:  460  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  461  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  462  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  463  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  464  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  465  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  466  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  467  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  468  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  469  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  470  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173267900943756
**************************************************learning rate decay**************************************************
Epoch:  471  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173267900943756
Epoch:  472  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  473  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  474  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  475  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  476  	Training Loss: 0.1300297975540161
Test Loss:  0.17489862442016602
Valid Loss:  0.17173269391059875
Epoch:  477  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  478  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  479  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  480  	Training Loss: 0.1300298124551773
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  481  	Training Loss: 0.1300297975540161
 96%|█████████▌| 481/500 [07:31<00:24,  1.30s/it] 97%|█████████▋| 483/500 [07:31<00:15,  1.08it/s] 97%|█████████▋| 485/500 [07:34<00:16,  1.11s/it] 97%|█████████▋| 487/500 [07:34<00:10,  1.26it/s] 98%|█████████▊| 489/500 [07:34<00:06,  1.74it/s] 98%|█████████▊| 491/500 [07:40<00:11,  1.30s/it] 99%|█████████▊| 493/500 [07:40<00:06,  1.07it/s] 99%|█████████▉| 495/500 [07:43<00:05,  1.11s/it] 99%|█████████▉| 497/500 [07:43<00:02,  1.25it/s]100%|█████████▉| 499/500 [07:44<00:00,  1.73it/s]100%|██████████| 500/500 [07:47<00:00,  1.07it/s]
Test Loss:  0.17489859461784363
Valid Loss:  0.17173267900943756
Epoch:  482  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  483  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  484  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  485  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  486  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  487  	Training Loss: 0.1300298124551773
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  488  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  489  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  490  	Training Loss: 0.1300297975540161
Test Loss:  0.17489862442016602
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  491  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  492  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  493  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  494  	Training Loss: 0.1300298124551773
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  495  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
Epoch:  496  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  497  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173269391059875
Epoch:  498  	Training Loss: 0.1300297975540161
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
Epoch:  499  	Training Loss: 0.1300297975540161
Test Loss:  0.17489860951900482
Valid Loss:  0.17173267900943756
Epoch:  500  	Training Loss: 0.1300298124551773
Test Loss:  0.17489859461784363
Valid Loss:  0.17173269391059875
**************************************************learning rate decay**************************************************
seed is  12
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:03<26:15,  3.16s/it]  1%|          | 3/500 [00:03<07:10,  1.15it/s]  1%|          | 5/500 [00:03<03:45,  2.20it/s]  1%|▏         | 7/500 [00:03<02:22,  3.45it/s]  2%|▏         | 9/500 [00:03<01:40,  4.86it/s]  2%|▏         | 11/500 [00:06<05:42,  1.43it/s]  3%|▎         | 13/500 [00:07<03:58,  2.04it/s]  3%|▎         | 15/500 [00:10<06:37,  1.22it/s]  3%|▎         | 17/500 [00:10<04:40,  1.72it/s]  4%|▍         | 19/500 [00:10<03:22,  2.38it/s]  4%|▍         | 21/500 [00:16<09:34,  1.20s/it]  5%|▍         | 23/500 [00:16<06:47,  1.17it/s]  5%|▌         | 25/500 [00:19<08:19,  1.05s/it]  5%|▌         | 27/500 [00:19<05:57,  1.32it/s]  6%|▌         | 29/500 [00:19<04:17,  1.83it/s]  6%|▌         | 31/500 [00:25<10:15,  1.31s/it]  7%|▋         | 33/500 [00:25<07:17,  1.07it/s]  7%|▋         | 35/500 [00:29<08:37,  1.11s/it]  7%|▋         | 37/500 [00:29<06:09,  1.25it/s]  8%|▊         | 39/500 [00:29<04:26,  1.73it/s]  8%|▊         | 41/500 [00:35<10:05,  1.32s/it]  9%|▊         | 43/500 [00:35<07:10,  1.06it/s]  9%|▉         | 45/500 [00:38<08:32,  1.13s/it]  9%|▉         | 47/500 [00:38<06:06,  1.24it/s] 10%|▉         | 49/500 [00:38<04:24,  1.71it/s] 10%|█         | 51/500 [00:44<09:50,  1.32s/it] 11%|█         | 53/500 [00:45<07:00,  1.06it/s] 11%|█         | 55/500 [00:48<08:14,  1.11s/it] 11%|█▏        | 57/500 [00:48<05:52,  1.26it/s] 12%|█▏        | 59/500 [00:48<04:14,  1.73it/s] 12%|█▏        | 61/500 [00:54<09:28,  1.30s/it] 13%|█▎        | 63/500 [00:54<06:44,  1.08it/s]Epoch:  1  	Training Loss: 0.3922201991081238
Test Loss:  1377.62939453125
Valid Loss:  1324.635986328125
Epoch:  2  	Training Loss: 1339.91552734375
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  3  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  4  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  5  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  6  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  7  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  8  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  9  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  10  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  11  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  12  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  13  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  14  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  15  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  16  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  17  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  18  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  19  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  20  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  21  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  22  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  23  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  24  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  25  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  26  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  27  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  28  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  29  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  30  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  31  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  32  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  33  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  34  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  35  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  36  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  37  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  38  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  39  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  40  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  41  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  42  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  43  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  44  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  45  	Training Loss: 0.7467443943023682
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  46  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  47  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  48  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  49  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  50  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  51  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  52  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  53  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  54  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  55  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  56  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  57  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  58  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  59  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  60  	Training Loss: 0.7467442750930786
Test Loss:  0.879029393196106
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  61  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  62  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  63  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  64  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164 13%|█▎        | 65/500 [00:57<08:01,  1.11s/it] 13%|█▎        | 67/500 [00:57<05:45,  1.25it/s] 14%|█▍        | 69/500 [00:57<04:09,  1.73it/s] 14%|█▍        | 71/500 [01:03<09:20,  1.31s/it] 15%|█▍        | 73/500 [01:03<06:39,  1.07it/s] 15%|█▌        | 75/500 [01:06<07:50,  1.11s/it] 15%|█▌        | 77/500 [01:07<05:36,  1.26it/s] 16%|█▌        | 79/500 [01:07<04:02,  1.74it/s] 16%|█▌        | 81/500 [01:13<09:00,  1.29s/it] 17%|█▋        | 83/500 [01:13<06:24,  1.09it/s] 17%|█▋        | 85/500 [01:16<07:38,  1.10s/it] 17%|█▋        | 87/500 [01:16<05:29,  1.25it/s] 18%|█▊        | 89/500 [01:16<03:57,  1.73it/s] 18%|█▊        | 91/500 [01:22<08:58,  1.32s/it] 19%|█▊        | 93/500 [01:22<06:23,  1.06it/s] 19%|█▉        | 95/500 [01:25<07:32,  1.12s/it] 19%|█▉        | 97/500 [01:25<05:22,  1.25it/s] 20%|█▉        | 99/500 [01:26<03:52,  1.73it/s] 20%|██        | 101/500 [01:31<08:35,  1.29s/it] 21%|██        | 103/500 [01:32<06:06,  1.08it/s] 21%|██        | 105/500 [01:35<07:16,  1.10s/it] 21%|██▏       | 107/500 [01:35<05:12,  1.26it/s] 22%|██▏       | 109/500 [01:35<03:44,  1.74it/s] 22%|██▏       | 111/500 [01:41<08:20,  1.29s/it] 23%|██▎       | 113/500 [01:41<05:56,  1.09it/s] 23%|██▎       | 115/500 [01:44<07:05,  1.11s/it] 23%|██▎       | 117/500 [01:44<05:04,  1.26it/s] 24%|██▍       | 119/500 [01:44<03:38,  1.74it/s] 24%|██▍       | 121/500 [01:50<08:09,  1.29s/it] 25%|██▍       | 123/500 [01:50<05:48,  1.08it/s]
Valid Loss:  0.8397847414016724
Epoch:  65  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  66  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  67  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  68  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  69  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  70  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  71  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  72  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  73  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  74  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  75  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  76  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  77  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  78  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  79  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  80  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  81  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  82  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  83  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  84  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  85  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  86  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  87  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  88  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  89  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  90  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  91  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  92  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  93  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  94  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  95  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  96  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  97  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  98  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  99  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  100  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  101  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  102  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  103  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  104  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  105  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  106  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  107  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  108  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  109  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  110  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  111  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  112  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  113  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  114  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  115  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  116  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  117  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  118  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  119  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  120  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  121  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  122  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  123  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  124  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  125  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:   25%|██▌       | 125/500 [01:53<06:55,  1.11s/it] 25%|██▌       | 127/500 [01:54<04:57,  1.26it/s] 26%|██▌       | 129/500 [01:54<03:33,  1.73it/s] 26%|██▌       | 131/500 [02:00<07:56,  1.29s/it] 27%|██▋       | 133/500 [02:00<05:39,  1.08it/s] 27%|██▋       | 135/500 [02:03<06:43,  1.11s/it] 27%|██▋       | 137/500 [02:03<04:48,  1.26it/s] 28%|██▊       | 139/500 [02:03<03:27,  1.74it/s] 28%|██▊       | 141/500 [02:09<07:46,  1.30s/it] 29%|██▊       | 143/500 [02:09<05:31,  1.08it/s] 29%|██▉       | 145/500 [02:13<06:50,  1.16s/it] 29%|██▉       | 147/500 [02:13<04:53,  1.20it/s] 30%|██▉       | 149/500 [02:13<03:30,  1.66it/s] 30%|███       | 151/500 [02:19<07:56,  1.37s/it] 31%|███       | 153/500 [02:19<05:38,  1.02it/s] 31%|███       | 155/500 [02:22<06:33,  1.14s/it] 31%|███▏      | 157/500 [02:22<04:40,  1.22it/s] 32%|███▏      | 159/500 [02:23<03:21,  1.69it/s] 32%|███▏      | 161/500 [02:28<07:20,  1.30s/it] 33%|███▎      | 163/500 [02:29<05:13,  1.08it/s] 33%|███▎      | 165/500 [02:32<06:12,  1.11s/it] 33%|███▎      | 167/500 [02:32<04:26,  1.25it/s] 34%|███▍      | 169/500 [02:32<03:11,  1.73it/s] 34%|███▍      | 171/500 [02:38<07:05,  1.29s/it] 35%|███▍      | 173/500 [02:38<05:02,  1.08it/s] 35%|███▌      | 175/500 [02:41<05:57,  1.10s/it] 35%|███▌      | 177/500 [02:41<04:15,  1.27it/s] 36%|███▌      | 179/500 [02:41<03:03,  1.75it/s] 36%|███▌      | 181/500 [02:47<06:52,  1.29s/it] 37%|███▋      | 183/500 [02:47<04:53,  1.08it/s]0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  126  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  127  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  128  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  129  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  130  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  131  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  132  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  133  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  134  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  135  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  136  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  137  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  138  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  139  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  140  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  141  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  142  	Training Loss: 0.7467443943023682
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  143  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  144  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  145  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  146  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  147  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  148  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  149  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  150  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  151  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  152  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  153  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  154  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  155  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  156  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  157  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  158  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  159  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  160  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  161  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  162  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  163  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  164  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  165  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  166  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  167  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  168  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  169  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  170  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  171  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  172  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  173  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  174  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  175  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  176  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  177  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  178  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  179  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  180  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  181  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  182  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  183  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  184  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  185  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
 37%|███▋      | 185/500 [02:50<05:47,  1.10s/it] 37%|███▋      | 187/500 [02:51<04:08,  1.26it/s] 38%|███▊      | 189/500 [02:51<02:58,  1.74it/s] 38%|███▊      | 191/500 [02:57<06:39,  1.29s/it] 39%|███▊      | 193/500 [02:57<04:43,  1.08it/s] 39%|███▉      | 195/500 [03:00<05:35,  1.10s/it] 39%|███▉      | 197/500 [03:00<03:59,  1.27it/s] 40%|███▉      | 199/500 [03:00<02:51,  1.75it/s] 40%|████      | 201/500 [03:06<06:28,  1.30s/it] 41%|████      | 203/500 [03:06<04:36,  1.08it/s] 41%|████      | 205/500 [03:09<05:27,  1.11s/it] 41%|████▏     | 207/500 [03:09<03:53,  1.25it/s] 42%|████▏     | 209/500 [03:09<02:48,  1.73it/s] 42%|████▏     | 211/500 [03:15<06:14,  1.30s/it] 43%|████▎     | 213/500 [03:16<04:26,  1.08it/s] 43%|████▎     | 215/500 [03:19<05:15,  1.11s/it] 43%|████▎     | 217/500 [03:19<03:46,  1.25it/s] 44%|████▍     | 219/500 [03:19<02:42,  1.73it/s] 44%|████▍     | 221/500 [03:25<06:01,  1.30s/it] 45%|████▍     | 223/500 [03:25<04:16,  1.08it/s] 45%|████▌     | 225/500 [03:28<05:03,  1.10s/it] 45%|████▌     | 227/500 [03:28<03:36,  1.26it/s] 46%|████▌     | 229/500 [03:28<02:35,  1.74it/s] 46%|████▌     | 231/500 [03:34<05:47,  1.29s/it] 47%|████▋     | 233/500 [03:34<04:06,  1.08it/s] 47%|████▋     | 235/500 [03:37<04:51,  1.10s/it] 47%|████▋     | 237/500 [03:37<03:27,  1.27it/s] 48%|████▊     | 239/500 [03:38<02:29,  1.75it/s] 48%|████▊     | 241/500 [03:44<05:36,  1.30s/it] 49%|████▊     | 243/500 [03:44<03:58,  1.08it/s]**************************************************learning rate decay**************************************************
Epoch:  186  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  187  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  188  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  189  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  190  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  191  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  192  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  193  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  194  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  195  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  196  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  197  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  198  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  199  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  200  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  201  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  202  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  203  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  204  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  205  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  206  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  207  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  208  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  209  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  210  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  211  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  212  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  213  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  214  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  215  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  216  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  217  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  218  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  219  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  220  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  221  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  222  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  223  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  224  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  225  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  226  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  227  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  228  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  229  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  230  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  231  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  232  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  233  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  234  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  235  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  236  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  237  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  238  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  239  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  240  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  241  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  242  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  243  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  244  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  245  	Training Loss: 0.7467442750930786
Test Loss:  0.879029393196106
Valid Loss:  0.8397847414016724
 49%|████▉     | 245/500 [03:47<04:42,  1.11s/it] 49%|████▉     | 247/500 [03:47<03:21,  1.26it/s] 50%|████▉     | 249/500 [03:47<02:24,  1.74it/s] 50%|█████     | 251/500 [03:53<05:23,  1.30s/it] 51%|█████     | 253/500 [03:53<03:49,  1.08it/s] 51%|█████     | 255/500 [03:56<04:29,  1.10s/it] 51%|█████▏    | 257/500 [03:56<03:11,  1.27it/s] 52%|█████▏    | 259/500 [03:56<02:17,  1.75it/s] 52%|█████▏    | 261/500 [04:02<05:08,  1.29s/it] 53%|█████▎    | 263/500 [04:02<03:38,  1.08it/s] 53%|█████▎    | 265/500 [04:05<04:18,  1.10s/it] 53%|█████▎    | 267/500 [04:06<03:04,  1.27it/s] 54%|█████▍    | 269/500 [04:06<02:12,  1.75it/s] 54%|█████▍    | 271/500 [04:12<04:56,  1.29s/it] 55%|█████▍    | 273/500 [04:12<03:30,  1.08it/s] 55%|█████▌    | 275/500 [04:15<04:07,  1.10s/it] 55%|█████▌    | 277/500 [04:15<02:55,  1.27it/s] 56%|█████▌    | 279/500 [04:15<02:06,  1.75it/s] 56%|█████▌    | 281/500 [04:21<04:46,  1.31s/it] 57%|█████▋    | 283/500 [04:21<03:23,  1.07it/s] 57%|█████▋    | 285/500 [04:24<04:00,  1.12s/it] 57%|█████▋    | 287/500 [04:24<02:50,  1.25it/s] 58%|█████▊    | 289/500 [04:25<02:02,  1.72it/s] 58%|█████▊    | 291/500 [04:31<04:30,  1.30s/it] 59%|█████▊    | 293/500 [04:31<03:11,  1.08it/s] 59%|█████▉    | 295/500 [04:34<03:45,  1.10s/it] 59%|█████▉    | 297/500 [04:34<02:40,  1.27it/s] 60%|█████▉    | 299/500 [04:34<01:54,  1.75it/s] 60%|██████    | 301/500 [04:40<04:16,  1.29s/it] 61%|██████    | 303/500 [04:40<03:01,  1.09it/s]**************************************************learning rate decay**************************************************
Epoch:  246  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  247  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  248  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  249  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  250  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  251  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  252  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  253  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  254  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  255  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  256  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  257  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  258  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  259  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  260  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  261  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  262  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  263  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  264  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  265  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  266  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  267  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  268  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  269  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  270  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  271  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  272  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  273  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  274  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  275  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  276  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  277  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  278  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  279  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  280  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  281  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  282  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  283  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  284  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  285  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  286  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  287  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  288  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  289  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  290  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  291  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  292  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  293  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  294  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  295  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  296  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  297  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  298  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  299  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  300  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  301  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  302  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  303  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  304  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  305  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
 61%|██████    | 305/500 [04:43<03:35,  1.10s/it] 61%|██████▏   | 307/500 [04:43<02:33,  1.26it/s] 62%|██████▏   | 309/500 [04:43<01:49,  1.74it/s] 62%|██████▏   | 311/500 [04:49<04:04,  1.29s/it] 63%|██████▎   | 313/500 [04:49<02:53,  1.08it/s] 63%|██████▎   | 315/500 [04:52<03:23,  1.10s/it] 63%|██████▎   | 317/500 [04:53<02:24,  1.27it/s] 64%|██████▍   | 319/500 [04:53<01:43,  1.75it/s] 64%|██████▍   | 321/500 [04:59<03:51,  1.29s/it] 65%|██████▍   | 323/500 [04:59<02:43,  1.08it/s] 65%|██████▌   | 325/500 [05:02<03:12,  1.10s/it] 65%|██████▌   | 327/500 [05:02<02:16,  1.26it/s] 66%|██████▌   | 329/500 [05:02<01:38,  1.74it/s] 66%|██████▌   | 331/500 [05:08<03:37,  1.29s/it] 67%|██████▋   | 333/500 [05:08<02:34,  1.08it/s] 67%|██████▋   | 335/500 [05:11<03:01,  1.10s/it] 67%|██████▋   | 337/500 [05:11<02:08,  1.27it/s] 68%|██████▊   | 339/500 [05:11<01:32,  1.75it/s] 68%|██████▊   | 341/500 [05:17<03:25,  1.29s/it] 69%|██████▊   | 343/500 [05:17<02:25,  1.08it/s] 69%|██████▉   | 345/500 [05:21<02:51,  1.11s/it] 69%|██████▉   | 347/500 [05:21<02:01,  1.26it/s] 70%|██████▉   | 349/500 [05:21<01:26,  1.74it/s] 70%|███████   | 351/500 [05:27<03:13,  1.30s/it] 71%|███████   | 353/500 [05:27<02:16,  1.08it/s] 71%|███████   | 355/500 [05:30<02:40,  1.10s/it] 71%|███████▏  | 357/500 [05:30<01:53,  1.26it/s] 72%|███████▏  | 359/500 [05:30<01:20,  1.74it/s] 72%|███████▏  | 361/500 [05:36<03:01,  1.31s/it] 73%|███████▎  | 363/500 [05:36<02:07,  1.07it/s]**************************************************learning rate decay**************************************************
Epoch:  306  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  307  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  308  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  309  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  310  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  311  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  312  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  313  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  314  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  315  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  316  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  317  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  318  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  319  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  320  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  321  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  322  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  323  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  324  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  325  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  326  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  327  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  328  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  329  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  330  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  331  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  332  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  333  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  334  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  335  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  336  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  337  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  338  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  339  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  340  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  341  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  342  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  343  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  344  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  345  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  346  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  347  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  348  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  349  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  350  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  351  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  352  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  353  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  354  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  355  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  356  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  357  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  358  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  359  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  360  	Training Loss: 0.7467442750930786
Test Loss:  0.879029393196106
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  361  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  362  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  363  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  364  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  365  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
 73%|███████▎  | 365/500 [05:39<02:30,  1.11s/it] 73%|███████▎  | 367/500 [05:40<01:46,  1.25it/s] 74%|███████▍  | 369/500 [05:40<01:16,  1.72it/s] 74%|███████▍  | 371/500 [05:46<02:47,  1.30s/it] 75%|███████▍  | 373/500 [05:46<01:57,  1.08it/s] 75%|███████▌  | 375/500 [05:49<02:19,  1.12s/it] 75%|███████▌  | 377/500 [05:49<01:38,  1.24it/s] 76%|███████▌  | 379/500 [05:49<01:10,  1.72it/s] 76%|███████▌  | 381/500 [05:55<02:39,  1.34s/it] 77%|███████▋  | 383/500 [05:55<01:51,  1.04it/s] 77%|███████▋  | 385/500 [05:59<02:10,  1.13s/it] 77%|███████▋  | 387/500 [05:59<01:32,  1.23it/s] 78%|███████▊  | 389/500 [05:59<01:05,  1.70it/s] 78%|███████▊  | 391/500 [06:05<02:22,  1.31s/it] 79%|███████▊  | 393/500 [06:05<01:39,  1.07it/s] 79%|███████▉  | 395/500 [06:08<01:56,  1.11s/it] 79%|███████▉  | 397/500 [06:08<01:21,  1.26it/s] 80%|███████▉  | 399/500 [06:08<00:58,  1.74it/s] 80%|████████  | 401/500 [06:14<02:08,  1.30s/it] 81%|████████  | 403/500 [06:14<01:29,  1.08it/s] 81%|████████  | 405/500 [06:17<01:47,  1.13s/it] 81%|████████▏ | 407/500 [06:18<01:15,  1.23it/s] 82%|████████▏ | 409/500 [06:18<00:53,  1.70it/s] 82%|████████▏ | 411/500 [06:24<02:00,  1.35s/it] 83%|████████▎ | 413/500 [06:24<01:24,  1.03it/s] 83%|████████▎ | 415/500 [06:27<01:36,  1.13s/it] 83%|████████▎ | 417/500 [06:27<01:07,  1.23it/s] 84%|████████▍ | 419/500 [06:27<00:47,  1.70it/s] 84%|████████▍ | 421/500 [06:33<01:43,  1.31s/it] 85%|████████▍ | 423/500 [06:34<01:12,  1.07it/s]**************************************************learning rate decay**************************************************
Epoch:  366  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  367  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  368  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  369  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  370  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  371  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  372  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  373  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  374  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  375  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  376  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  377  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  378  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  379  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  380  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  381  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  382  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  383  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  384  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  385  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  386  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  387  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  388  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  389  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  390  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  391  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  392  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  393  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  394  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  395  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  396  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  397  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  398  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  399  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  400  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  401  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  402  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  403  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  404  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  405  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  406  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  407  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  408  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  409  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  410  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  411  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  412  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  413  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  414  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  415  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  416  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  417  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  418  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  419  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  420  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  421  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  422  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  423  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  424  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  425  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
 85%|████████▌ | 425/500 [06:37<01:23,  1.11s/it] 85%|████████▌ | 427/500 [06:37<00:58,  1.26it/s] 86%|████████▌ | 429/500 [06:37<00:40,  1.73it/s] 86%|████████▌ | 431/500 [06:43<01:29,  1.30s/it] 87%|████████▋ | 433/500 [06:43<01:02,  1.08it/s] 87%|████████▋ | 435/500 [06:46<01:11,  1.10s/it] 87%|████████▋ | 437/500 [06:46<00:49,  1.26it/s] 88%|████████▊ | 439/500 [06:46<00:34,  1.74it/s] 88%|████████▊ | 441/500 [06:52<01:16,  1.30s/it] 89%|████████▊ | 443/500 [06:52<00:52,  1.08it/s] 89%|████████▉ | 445/500 [06:55<01:01,  1.11s/it] 89%|████████▉ | 447/500 [06:56<00:42,  1.25it/s] 90%|████████▉ | 449/500 [06:56<00:29,  1.73it/s] 90%|█████████ | 451/500 [07:02<01:04,  1.32s/it] 91%|█████████ | 453/500 [07:02<00:44,  1.05it/s] 91%|█████████ | 455/500 [07:05<00:50,  1.13s/it] 91%|█████████▏| 457/500 [07:05<00:34,  1.24it/s] 92%|█████████▏| 459/500 [07:05<00:23,  1.71it/s] 92%|█████████▏| 461/500 [07:11<00:50,  1.30s/it] 93%|█████████▎| 463/500 [07:11<00:34,  1.08it/s] 93%|█████████▎| 465/500 [07:14<00:38,  1.11s/it] 93%|█████████▎| 467/500 [07:15<00:26,  1.26it/s] 94%|█████████▍| 469/500 [07:15<00:17,  1.74it/s] 94%|█████████▍| 471/500 [07:21<00:37,  1.29s/it] 95%|█████████▍| 473/500 [07:21<00:24,  1.08it/s] 95%|█████████▌| 475/500 [07:21<00:16,  1.50it/s] 95%|█████████▌| 477/500 [07:21<00:11,  2.06it/s] 96%|█████████▌| 479/500 [07:21<00:07,  2.78it/s] 96%|█████████▌| 481/500 [07:27<00:21,  1.15s/it] 97%|█████████▋| 483/500 [07:27<00:14,  1.21it/s] 97%|█████████▋| 485/500 [07:30<00:15,  1.03s/it]**************************************************learning rate decay**************************************************
Epoch:  426  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  427  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  428  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  429  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  430  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  431  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  432  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  433  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  434  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  435  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  436  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  437  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  438  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  439  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  440  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  441  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  442  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  443  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  444  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  445  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  446  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  447  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  448  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  449  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  450  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  451  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  452  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  453  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  454  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  455  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  456  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  457  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  458  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  459  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  460  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  461  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  462  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  463  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  464  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  465  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  466  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  467  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  468  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  469  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  470  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  471  	Training Loss: 0.7467442750930786
Test Loss:  0.879029393196106
Valid Loss:  0.8397847414016724
Epoch:  472  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  473  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  474  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  475  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  476  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  477  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  478  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  479  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  480  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  481  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  482  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  483  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  484  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  485  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  486  	Training Loss: 0.7467443346977234
Test Loss:  0.8790292739868164
Valid Loss:   97%|█████████▋| 487/500 [07:30<00:09,  1.35it/s] 98%|█████████▊| 489/500 [07:31<00:05,  1.86it/s] 98%|█████████▊| 491/500 [07:37<00:11,  1.28s/it] 99%|█████████▊| 493/500 [07:37<00:06,  1.10it/s] 99%|█████████▉| 495/500 [07:40<00:05,  1.10s/it] 99%|█████████▉| 497/500 [07:40<00:02,  1.27it/s]100%|█████████▉| 499/500 [07:40<00:00,  1.75it/s]100%|██████████| 500/500 [07:43<00:00,  1.08it/s]
0.8397847414016724
Epoch:  487  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  488  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  489  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  490  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
**************************************************learning rate decay**************************************************
Epoch:  491  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  492  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  493  	Training Loss: 0.7467443346977234
Test Loss:  0.879029393196106
Valid Loss:  0.8397847414016724
Epoch:  494  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  495  	Training Loss: 0.7467442750930786
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
Epoch:  496  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
Epoch:  497  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397847414016724
Epoch:  498  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397846817970276
Epoch:  499  	Training Loss: 0.7467443346977234
Test Loss:  0.8790293335914612
Valid Loss:  0.8397847414016724
Epoch:  500  	Training Loss: 0.7467442750930786
Test Loss:  0.8790292739868164
Valid Loss:  0.8397846817970276
**************************************************learning rate decay**************************************************
seed is  13
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:03<25:02,  3.01s/it]  1%|          | 3/500 [00:03<06:51,  1.21it/s]  1%|          | 5/500 [00:03<03:35,  2.30it/s]  1%|▏         | 7/500 [00:03<02:16,  3.61it/s]  2%|▏         | 9/500 [00:03<01:36,  5.09it/s]  2%|▏         | 11/500 [00:06<05:24,  1.51it/s]  3%|▎         | 13/500 [00:06<03:46,  2.15it/s]  3%|▎         | 15/500 [00:09<06:29,  1.24it/s]  3%|▎         | 17/500 [00:09<04:35,  1.75it/s]  4%|▍         | 19/500 [00:10<03:18,  2.42it/s]  4%|▍         | 21/500 [00:15<09:35,  1.20s/it]  5%|▍         | 23/500 [00:16<06:47,  1.17it/s]  5%|▌         | 25/500 [00:19<08:24,  1.06s/it]  5%|▌         | 27/500 [00:19<06:00,  1.31it/s]  6%|▌         | 29/500 [00:19<04:19,  1.82it/s]  6%|▌         | 31/500 [00:25<10:04,  1.29s/it]  7%|▋         | 33/500 [00:25<07:10,  1.09it/s]  7%|▋         | 35/500 [00:28<08:29,  1.10s/it]  7%|▋         | 37/500 [00:28<06:04,  1.27it/s]  8%|▊         | 39/500 [00:28<04:22,  1.76it/s]  8%|▊         | 41/500 [00:34<09:54,  1.30s/it]  9%|▊         | 43/500 [00:34<07:03,  1.08it/s]  9%|▉         | 45/500 [00:38<08:24,  1.11s/it]  9%|▉         | 47/500 [00:38<06:01,  1.25it/s] 10%|▉         | 49/500 [00:38<04:20,  1.73it/s] 10%|█         | 51/500 [00:44<09:42,  1.30s/it] 11%|█         | 53/500 [00:44<06:55,  1.08it/s] 11%|█         | 55/500 [00:47<08:12,  1.11s/it] 11%|█▏        | 57/500 [00:47<05:51,  1.26it/s] 12%|█▏        | 59/500 [00:47<04:13,  1.74it/s] 12%|█▏        | 61/500 [00:53<09:31,  1.30s/it] 13%|█▎        | 63/500 [00:53<06:46,  1.08it/s]Epoch:  1  	Training Loss: 0.20365582406520844
Test Loss:  437.4856262207031
Valid Loss:  421.398681640625
Epoch:  2  	Training Loss: 423.6333923339844
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  3  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  4  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  5  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  6  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.3285636901855469
Epoch:  7  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  8  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  9  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  10  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  11  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  12  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  13  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  14  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  15  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  16  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856374979019165
Epoch:  17  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856374979019165
Epoch:  18  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  19  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  20  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  21  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  22  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  23  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  24  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  25  	Training Loss: 0.27955862879753113
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  26  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.3285636901855469
Epoch:  27  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  28  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  29  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  30  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  31  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  32  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  33  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  34  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  35  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
**************************************************learning rate decay**************************************************
Epoch:  36  	Training Loss: 0.27955862879753113
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  37  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  38  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  39  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  40  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.3285636901855469
**************************************************learning rate decay**************************************************
Epoch:  41  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  42  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  43  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  44  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  45  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  46  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  47  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  48  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  49  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  50  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  51  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  52  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  53  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  54  	Training Loss: 0.27955862879753113
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  55  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  56  	Training Loss: 0.27955862879753113
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  57  	Training Loss: 0.27955862879753113
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  58  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  59  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  60  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  61  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  62  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  63  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
 13%|█▎        | 65/500 [00:57<08:17,  1.14s/it] 13%|█▎        | 67/500 [00:57<05:56,  1.22it/s] 14%|█▍        | 69/500 [00:57<04:16,  1.68it/s] 14%|█▍        | 71/500 [01:03<09:19,  1.30s/it] 15%|█▍        | 73/500 [01:03<06:38,  1.07it/s] 15%|█▌        | 75/500 [01:06<07:49,  1.10s/it] 15%|█▌        | 77/500 [01:06<05:35,  1.26it/s] 16%|█▌        | 79/500 [01:06<04:01,  1.74it/s] 16%|█▌        | 81/500 [01:12<08:58,  1.29s/it] 17%|█▋        | 83/500 [01:12<06:23,  1.09it/s] 17%|█▋        | 85/500 [01:15<07:37,  1.10s/it] 17%|█▋        | 87/500 [01:15<05:26,  1.26it/s] 18%|█▊        | 89/500 [01:15<03:55,  1.74it/s] 18%|█▊        | 91/500 [01:21<08:49,  1.29s/it] 19%|█▊        | 93/500 [01:22<06:18,  1.08it/s] 19%|█▉        | 95/500 [01:25<07:26,  1.10s/it] 19%|█▉        | 97/500 [01:25<05:18,  1.27it/s] 20%|█▉        | 99/500 [01:25<03:49,  1.75it/s] 20%|██        | 101/500 [01:31<08:33,  1.29s/it] 21%|██        | 103/500 [01:31<06:05,  1.09it/s] 21%|██        | 105/500 [01:34<07:14,  1.10s/it] 21%|██▏       | 107/500 [01:34<05:10,  1.27it/s] 22%|██▏       | 109/500 [01:34<03:43,  1.75it/s] 22%|██▏       | 111/500 [01:40<08:20,  1.29s/it] 23%|██▎       | 113/500 [01:40<05:56,  1.09it/s] 23%|██▎       | 115/500 [01:43<07:02,  1.10s/it] 23%|██▎       | 117/500 [01:43<05:01,  1.27it/s] 24%|██▍       | 119/500 [01:44<03:36,  1.76it/s] 24%|██▍       | 121/500 [01:49<08:07,  1.29s/it] 25%|██▍       | 123/500 [01:50<05:46,  1.09it/s]Epoch:  64  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  65  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
**************************************************learning rate decay**************************************************
Epoch:  66  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  67  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  68  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  69  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  70  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  71  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  72  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  73  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856374979019165
Epoch:  74  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856374979019165
Epoch:  75  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  76  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  77  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  78  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  79  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  80  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  81  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  82  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  83  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  84  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  85  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
**************************************************learning rate decay**************************************************
Epoch:  86  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  87  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  88  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  89  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  90  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
**************************************************learning rate decay**************************************************
Epoch:  91  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  92  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  93  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  94  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  95  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  96  	Training Loss: 0.27955862879753113
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  97  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  98  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  99  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  100  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  101  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  102  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  103  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  104  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  105  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856374979019165
**************************************************learning rate decay**************************************************
Epoch:  106  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  107  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  108  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  109  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  110  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
**************************************************learning rate decay**************************************************
Epoch:  111  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  112  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.3285636901855469
Epoch:  113  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  114  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  115  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856374979019165
**************************************************learning rate decay**************************************************
Epoch:  116  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  117  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  118  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  119  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  120  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  121  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  122  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  123  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  124  	Training Loss: 0.2795586585998535
Test Loss:   25%|██▌       | 125/500 [01:53<06:53,  1.10s/it] 25%|██▌       | 127/500 [01:53<04:55,  1.26it/s] 26%|██▌       | 129/500 [01:53<03:32,  1.74it/s] 26%|██▌       | 131/500 [01:59<07:56,  1.29s/it] 27%|██▋       | 133/500 [01:59<05:39,  1.08it/s] 27%|██▋       | 135/500 [02:02<06:42,  1.10s/it] 27%|██▋       | 137/500 [02:02<04:47,  1.26it/s] 28%|██▊       | 139/500 [02:02<03:26,  1.74it/s] 28%|██▊       | 141/500 [02:08<07:46,  1.30s/it] 29%|██▊       | 143/500 [02:08<05:31,  1.08it/s] 29%|██▉       | 145/500 [02:11<06:33,  1.11s/it] 29%|██▉       | 147/500 [02:12<04:41,  1.25it/s] 30%|██▉       | 149/500 [02:12<03:22,  1.73it/s] 30%|███       | 151/500 [02:18<07:33,  1.30s/it] 31%|███       | 153/500 [02:18<05:23,  1.07it/s] 31%|███       | 155/500 [02:21<06:21,  1.11s/it] 31%|███▏      | 157/500 [02:21<04:32,  1.26it/s] 32%|███▏      | 159/500 [02:21<03:15,  1.74it/s] 32%|███▏      | 161/500 [02:27<07:22,  1.30s/it] 33%|███▎      | 163/500 [02:27<05:14,  1.07it/s] 33%|███▎      | 165/500 [02:30<06:12,  1.11s/it] 33%|███▎      | 167/500 [02:30<04:26,  1.25it/s] 34%|███▍      | 169/500 [02:31<03:11,  1.73it/s] 34%|███▍      | 171/500 [02:37<07:09,  1.31s/it] 35%|███▍      | 173/500 [02:37<05:05,  1.07it/s] 35%|███▌      | 175/500 [02:40<05:59,  1.11s/it] 35%|███▌      | 177/500 [02:40<04:16,  1.26it/s] 36%|███▌      | 179/500 [02:40<03:04,  1.74it/s] 36%|███▌      | 181/500 [02:46<06:50,  1.29s/it] 37%|███▋      | 183/500 [02:46<04:51,  1.09it/s]0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  125  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.3285636901855469
**************************************************learning rate decay**************************************************
Epoch:  126  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  127  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  128  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  129  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  130  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  131  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  132  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  133  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  134  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856374979019165
Epoch:  135  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
**************************************************learning rate decay**************************************************
Epoch:  136  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  137  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  138  	Training Loss: 0.27955862879753113
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  139  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  140  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.3285636901855469
**************************************************learning rate decay**************************************************
Epoch:  141  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  142  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  143  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  144  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  145  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
**************************************************learning rate decay**************************************************
Epoch:  146  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  147  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  148  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  149  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  150  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  151  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  152  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  153  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  154  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856374979019165
Epoch:  155  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
**************************************************learning rate decay**************************************************
Epoch:  156  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  157  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  158  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  159  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  160  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.3285636901855469
**************************************************learning rate decay**************************************************
Epoch:  161  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  162  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  163  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.3285636901855469
Epoch:  164  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  165  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  166  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  167  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  168  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856374979019165
Epoch:  169  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  170  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  171  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  172  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  173  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856374979019165
Epoch:  174  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  175  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  176  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  177  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  178  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  179  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  180  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  181  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  182  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  183  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  184  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:   37%|███▋      | 185/500 [02:49<05:47,  1.10s/it] 37%|███▋      | 187/500 [02:49<04:08,  1.26it/s] 38%|███▊      | 189/500 [02:49<02:58,  1.74it/s] 38%|███▊      | 191/500 [02:55<06:48,  1.32s/it] 39%|███▊      | 193/500 [02:56<04:50,  1.06it/s] 39%|███▉      | 195/500 [02:59<05:39,  1.11s/it] 39%|███▉      | 197/500 [02:59<04:02,  1.25it/s] 40%|███▉      | 199/500 [02:59<02:54,  1.73it/s] 40%|████      | 201/500 [03:05<06:32,  1.31s/it] 41%|████      | 203/500 [03:05<04:39,  1.06it/s] 41%|████      | 205/500 [03:08<05:31,  1.12s/it] 41%|████▏     | 207/500 [03:08<03:56,  1.24it/s] 42%|████▏     | 209/500 [03:08<02:50,  1.71it/s] 42%|████▏     | 211/500 [03:15<06:39,  1.38s/it] 43%|████▎     | 213/500 [03:15<04:44,  1.01it/s] 43%|████▎     | 215/500 [03:18<05:31,  1.16s/it] 43%|████▎     | 217/500 [03:18<03:56,  1.20it/s] 44%|████▍     | 219/500 [03:18<02:49,  1.66it/s] 44%|████▍     | 221/500 [03:24<06:06,  1.31s/it] 45%|████▍     | 223/500 [03:25<04:20,  1.07it/s] 45%|████▌     | 225/500 [03:28<05:06,  1.12s/it] 45%|████▌     | 227/500 [03:28<03:38,  1.25it/s] 46%|████▌     | 229/500 [03:28<02:37,  1.72it/s] 46%|████▌     | 231/500 [03:34<05:48,  1.30s/it] 47%|████▋     | 233/500 [03:34<04:07,  1.08it/s] 47%|████▋     | 235/500 [03:37<04:53,  1.11s/it] 47%|████▋     | 237/500 [03:37<03:28,  1.26it/s] 48%|████▊     | 239/500 [03:37<02:30,  1.74it/s] 48%|████▊     | 241/500 [03:43<05:37,  1.30s/it] 49%|████▊     | 243/500 [03:43<03:59,  1.07it/s]0.32856371998786926
Epoch:  185  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  186  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  187  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856374979019165
Epoch:  188  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  189  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  190  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  191  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  192  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  193  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  194  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  195  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  196  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  197  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  198  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  199  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  200  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  201  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  202  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  203  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  204  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  205  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  206  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  207  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  208  	Training Loss: 0.27955862879753113
Test Loss:  0.3465331494808197
Valid Loss:  0.3285636901855469
Epoch:  209  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  210  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  211  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  212  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  213  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  214  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  215  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
**************************************************learning rate decay**************************************************
Epoch:  216  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  217  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  218  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  219  	Training Loss: 0.27955862879753113
Test Loss:  0.3465331494808197
Valid Loss:  0.32856374979019165
Epoch:  220  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  221  	Training Loss: 0.27955862879753113
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  222  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  223  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  224  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  225  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  226  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  227  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  228  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  229  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  230  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  231  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  232  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  233  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  234  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  235  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  236  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  237  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  238  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  239  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  240  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  241  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.3285636901855469
Epoch:  242  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  243  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  244  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
 49%|████▉     | 245/500 [03:46<04:43,  1.11s/it] 49%|████▉     | 247/500 [03:47<03:21,  1.25it/s] 50%|████▉     | 249/500 [03:47<02:25,  1.73it/s] 50%|█████     | 251/500 [03:53<05:23,  1.30s/it] 51%|█████     | 253/500 [03:53<03:49,  1.08it/s] 51%|█████     | 255/500 [03:56<04:31,  1.11s/it] 51%|█████▏    | 257/500 [03:56<03:13,  1.26it/s] 52%|█████▏    | 259/500 [03:56<02:18,  1.74it/s] 52%|█████▏    | 261/500 [04:02<05:09,  1.30s/it] 53%|█████▎    | 263/500 [04:02<03:39,  1.08it/s] 53%|█████▎    | 265/500 [04:05<04:19,  1.11s/it] 53%|█████▎    | 267/500 [04:05<03:05,  1.26it/s] 54%|█████▍    | 269/500 [04:05<02:12,  1.74it/s] 54%|█████▍    | 271/500 [04:11<04:58,  1.30s/it] 55%|█████▍    | 273/500 [04:12<03:31,  1.07it/s] 55%|█████▌    | 275/500 [04:15<04:09,  1.11s/it] 55%|█████▌    | 277/500 [04:15<02:57,  1.25it/s] 56%|█████▌    | 279/500 [04:15<02:07,  1.73it/s] 56%|█████▌    | 281/500 [04:21<04:43,  1.30s/it] 57%|█████▋    | 283/500 [04:21<03:21,  1.08it/s] 57%|█████▋    | 285/500 [04:24<03:57,  1.10s/it] 57%|█████▋    | 287/500 [04:24<02:49,  1.26it/s] 58%|█████▊    | 289/500 [04:24<02:01,  1.74it/s] 58%|█████▊    | 291/500 [04:30<04:30,  1.29s/it] 59%|█████▊    | 293/500 [04:30<03:11,  1.08it/s] 59%|█████▉    | 295/500 [04:33<03:45,  1.10s/it] 59%|█████▉    | 297/500 [04:34<02:40,  1.27it/s] 60%|█████▉    | 299/500 [04:34<01:54,  1.75it/s] 60%|██████    | 301/500 [04:40<04:17,  1.29s/it] 61%|██████    | 303/500 [04:40<03:02,  1.08it/s]Epoch:  245  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  246  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  247  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  248  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  249  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  250  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  251  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  252  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  253  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  254  	Training Loss: 0.27955862879753113
Test Loss:  0.3465331494808197
Valid Loss:  0.32856374979019165
Epoch:  255  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  256  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  257  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  258  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  259  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  260  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856374979019165
**************************************************learning rate decay**************************************************
Epoch:  261  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  262  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  263  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  264  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  265  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
**************************************************learning rate decay**************************************************
Epoch:  266  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  267  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  268  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  269  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  270  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  271  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  272  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  273  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  274  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  275  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  276  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  277  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856374979019165
Epoch:  278  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  279  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  280  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  281  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  282  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  283  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  284  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  285  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
**************************************************learning rate decay**************************************************
Epoch:  286  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  287  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  288  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  289  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  290  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
**************************************************learning rate decay**************************************************
Epoch:  291  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  292  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  293  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  294  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  295  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  296  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  297  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  298  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  299  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  300  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
**************************************************learning rate decay**************************************************
Epoch:  301  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  302  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  303  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  304  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
 61%|██████    | 305/500 [04:43<03:37,  1.12s/it] 61%|██████▏   | 307/500 [04:43<02:34,  1.25it/s] 62%|██████▏   | 309/500 [04:43<01:50,  1.72it/s] 62%|██████▏   | 311/500 [04:49<04:04,  1.29s/it] 63%|██████▎   | 313/500 [04:49<02:53,  1.08it/s] 63%|██████▎   | 315/500 [04:52<03:23,  1.10s/it] 63%|██████▎   | 317/500 [04:52<02:24,  1.27it/s] 64%|██████▍   | 319/500 [04:52<01:43,  1.75it/s] 64%|██████▍   | 321/500 [04:58<03:51,  1.29s/it] 65%|██████▍   | 323/500 [04:59<02:43,  1.08it/s] 65%|██████▌   | 325/500 [05:02<03:12,  1.10s/it] 65%|██████▌   | 327/500 [05:02<02:17,  1.26it/s] 66%|██████▌   | 329/500 [05:02<01:38,  1.74it/s] 66%|██████▌   | 331/500 [05:08<03:43,  1.33s/it] 67%|██████▋   | 333/500 [05:08<02:38,  1.05it/s] 67%|██████▋   | 335/500 [05:11<03:08,  1.14s/it] 67%|██████▋   | 337/500 [05:11<02:13,  1.22it/s] 68%|██████▊   | 339/500 [05:12<01:35,  1.69it/s] 68%|██████▊   | 341/500 [05:18<03:35,  1.35s/it] 69%|██████▊   | 343/500 [05:18<02:31,  1.03it/s] 69%|██████▉   | 345/500 [05:21<02:55,  1.13s/it] 69%|██████▉   | 347/500 [05:21<02:04,  1.23it/s] 70%|██████▉   | 349/500 [05:21<01:28,  1.70it/s] 70%|███████   | 351/500 [05:27<03:16,  1.32s/it] 71%|███████   | 353/500 [05:27<02:18,  1.06it/s] 71%|███████   | 355/500 [05:31<02:42,  1.12s/it] 71%|███████▏  | 357/500 [05:31<01:55,  1.24it/s] 72%|███████▏  | 359/500 [05:31<01:22,  1.71it/s] 72%|███████▏  | 361/500 [05:37<03:02,  1.31s/it] 73%|███████▎  | 363/500 [05:37<02:08,  1.07it/s]Epoch:  305  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
**************************************************learning rate decay**************************************************
Epoch:  306  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  307  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  308  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  309  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  310  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
**************************************************learning rate decay**************************************************
Epoch:  311  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  312  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  313  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.3285636901855469
Epoch:  314  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  315  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  316  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  317  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  318  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  319  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  320  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  321  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  322  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  323  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  324  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  325  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  326  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  327  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  328  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  329  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  330  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
**************************************************learning rate decay**************************************************
Epoch:  331  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  332  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  333  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  334  	Training Loss: 0.27955862879753113
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  335  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  336  	Training Loss: 0.27955862879753113
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  337  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  338  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  339  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  340  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
**************************************************learning rate decay**************************************************
Epoch:  341  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  342  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  343  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  344  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  345  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  346  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  347  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  348  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  349  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  350  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  351  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  352  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  353  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  354  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  355  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  356  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  357  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  358  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  359  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  360  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  361  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  362  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  363  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  364  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
 73%|███████▎  | 365/500 [05:40<02:30,  1.12s/it] 73%|███████▎  | 367/500 [05:40<01:46,  1.25it/s] 74%|███████▍  | 369/500 [05:40<01:16,  1.72it/s] 74%|███████▍  | 371/500 [05:46<02:51,  1.33s/it] 75%|███████▍  | 373/500 [05:47<02:00,  1.05it/s] 75%|███████▌  | 375/500 [05:50<02:19,  1.12s/it] 75%|███████▌  | 377/500 [05:50<01:38,  1.25it/s] 76%|███████▌  | 379/500 [05:50<01:10,  1.73it/s] 76%|███████▌  | 381/500 [05:56<02:34,  1.30s/it] 77%|███████▋  | 383/500 [05:56<01:48,  1.08it/s] 77%|███████▋  | 385/500 [05:59<02:07,  1.11s/it] 77%|███████▋  | 387/500 [05:59<01:29,  1.26it/s] 78%|███████▊  | 389/500 [05:59<01:03,  1.74it/s] 78%|███████▊  | 391/500 [06:05<02:20,  1.29s/it] 79%|███████▊  | 393/500 [06:05<01:38,  1.08it/s] 79%|███████▉  | 395/500 [06:08<01:55,  1.10s/it] 79%|███████▉  | 397/500 [06:08<01:21,  1.26it/s] 80%|███████▉  | 399/500 [06:09<00:57,  1.74it/s] 80%|████████  | 401/500 [06:15<02:08,  1.30s/it] 81%|████████  | 403/500 [06:15<01:30,  1.08it/s] 81%|████████  | 405/500 [06:18<01:45,  1.11s/it] 81%|████████▏ | 407/500 [06:18<01:13,  1.26it/s] 82%|████████▏ | 409/500 [06:18<00:52,  1.74it/s] 82%|████████▏ | 411/500 [06:24<01:55,  1.29s/it] 83%|████████▎ | 413/500 [06:24<01:20,  1.08it/s] 83%|████████▎ | 415/500 [06:27<01:33,  1.10s/it] 83%|████████▎ | 417/500 [06:27<01:05,  1.26it/s] 84%|████████▍ | 419/500 [06:27<00:46,  1.75it/s] 84%|████████▍ | 421/500 [06:34<01:44,  1.32s/it] 85%|████████▍ | 423/500 [06:34<01:12,  1.06it/s]Epoch:  365  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  366  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  367  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  368  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  369  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  370  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  371  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  372  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  373  	Training Loss: 0.27955862879753113
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  374  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  375  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
**************************************************learning rate decay**************************************************
Epoch:  376  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  377  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  378  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  379  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  380  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
**************************************************learning rate decay**************************************************
Epoch:  381  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  382  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  383  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  384  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  385  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  386  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  387  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  388  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.3285636901855469
Epoch:  389  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  390  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  391  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  392  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  393  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  394  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  395  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  396  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  397  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  398  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  399  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  400  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  401  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  402  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  403  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  404  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  405  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  406  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  407  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  408  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  409  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  410  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  411  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  412  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  413  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  414  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.3285636901855469
Epoch:  415  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.3285636901855469
**************************************************learning rate decay**************************************************
Epoch:  416  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  417  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  418  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  419  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  420  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  421  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  422  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  423  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  424  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
 85%|████████▌ | 425/500 [06:37<01:25,  1.14s/it] 85%|████████▌ | 427/500 [06:37<00:59,  1.22it/s] 86%|████████▌ | 429/500 [06:37<00:42,  1.69it/s] 86%|████████▌ | 431/500 [06:43<01:29,  1.30s/it] 87%|████████▋ | 433/500 [06:43<01:02,  1.07it/s] 87%|████████▋ | 435/500 [06:46<01:12,  1.11s/it] 87%|████████▋ | 437/500 [06:46<00:50,  1.26it/s] 88%|████████▊ | 439/500 [06:46<00:35,  1.73it/s] 88%|████████▊ | 441/500 [06:52<01:16,  1.29s/it] 89%|████████▊ | 443/500 [06:52<00:52,  1.09it/s] 89%|████████▉ | 445/500 [06:56<01:00,  1.10s/it] 89%|████████▉ | 447/500 [06:56<00:41,  1.27it/s] 90%|████████▉ | 449/500 [06:56<00:29,  1.74it/s] 90%|█████████ | 451/500 [07:02<01:03,  1.29s/it] 91%|█████████ | 453/500 [07:02<00:43,  1.08it/s] 91%|█████████ | 455/500 [07:05<00:49,  1.11s/it] 91%|█████████▏| 457/500 [07:05<00:34,  1.26it/s] 92%|█████████▏| 459/500 [07:05<00:23,  1.74it/s] 92%|█████████▏| 461/500 [07:11<00:51,  1.32s/it] 93%|█████████▎| 463/500 [07:11<00:34,  1.06it/s] 93%|█████████▎| 465/500 [07:14<00:39,  1.12s/it] 93%|█████████▎| 467/500 [07:15<00:26,  1.25it/s] 94%|█████████▍| 469/500 [07:15<00:18,  1.72it/s] 94%|█████████▍| 471/500 [07:21<00:37,  1.30s/it] 95%|█████████▍| 473/500 [07:21<00:25,  1.07it/s] 95%|█████████▌| 475/500 [07:24<00:27,  1.11s/it] 95%|█████████▌| 477/500 [07:24<00:18,  1.25it/s] 96%|█████████▌| 479/500 [07:24<00:12,  1.73it/s] 96%|█████████▌| 481/500 [07:30<00:24,  1.30s/it] 97%|█████████▋| 483/500 [07:30<00:15,  1.08it/s]Epoch:  425  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.3285636901855469
**************************************************learning rate decay**************************************************
Epoch:  426  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  427  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  428  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  429  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  430  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  431  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  432  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  433  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  434  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  435  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
**************************************************learning rate decay**************************************************
Epoch:  436  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  437  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  438  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  439  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  440  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
**************************************************learning rate decay**************************************************
Epoch:  441  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  442  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  443  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  444  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  445  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
**************************************************learning rate decay**************************************************
Epoch:  446  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  447  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  448  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  449  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  450  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  451  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  452  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  453  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  454  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  455  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  456  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  457  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  458  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  459  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  460  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  461  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  462  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  463  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  464  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  465  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  466  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  467  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  468  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  469  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  470  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  471  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  472  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  473  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  474  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  475  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  476  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  477  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  478  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  479  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  480  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
**************************************************learning rate decay**************************************************
Epoch:  481  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  482  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  483  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  484  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
 97%|█████████▋| 485/500 [07:33<00:16,  1.11s/it] 97%|█████████▋| 487/500 [07:33<00:10,  1.25it/s] 98%|█████████▊| 489/500 [07:34<00:06,  1.73it/s] 98%|█████████▊| 491/500 [07:40<00:11,  1.31s/it] 99%|█████████▊| 493/500 [07:40<00:06,  1.07it/s] 99%|█████████▉| 495/500 [07:43<00:05,  1.11s/it] 99%|█████████▉| 497/500 [07:43<00:02,  1.25it/s]100%|█████████▉| 499/500 [07:43<00:00,  1.73it/s]100%|██████████| 500/500 [07:46<00:00,  1.07it/s]
Epoch:  485  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
Epoch:  486  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  487  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  488  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  489  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  490  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
**************************************************learning rate decay**************************************************
Epoch:  491  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  492  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  493  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  494  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856374979019165
Epoch:  495  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
**************************************************learning rate decay**************************************************
Epoch:  496  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331494808197
Valid Loss:  0.32856371998786926
Epoch:  497  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  498  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.3285636901855469
Epoch:  499  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
Epoch:  500  	Training Loss: 0.2795586585998535
Test Loss:  0.3465331792831421
Valid Loss:  0.32856371998786926
**************************************************learning rate decay**************************************************
seed is  14
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:02<24:42,  2.97s/it]  1%|          | 3/500 [00:03<06:46,  1.22it/s]  1%|          | 5/500 [00:03<03:32,  2.33it/s]  1%|▏         | 7/500 [00:03<02:15,  3.64it/s]  2%|▏         | 9/500 [00:03<01:35,  5.12it/s]  2%|▏         | 11/500 [00:06<05:21,  1.52it/s]  3%|▎         | 13/500 [00:06<03:45,  2.16it/s]  3%|▎         | 15/500 [00:09<06:29,  1.25it/s]  3%|▎         | 17/500 [00:09<04:35,  1.75it/s]  4%|▍         | 19/500 [00:09<03:18,  2.42it/s]  4%|▍         | 21/500 [00:15<09:40,  1.21s/it]  5%|▍         | 23/500 [00:16<06:50,  1.16it/s]  5%|▌         | 25/500 [00:19<08:24,  1.06s/it]  5%|▌         | 27/500 [00:19<06:00,  1.31it/s]  6%|▌         | 29/500 [00:19<04:19,  1.81it/s]  6%|▌         | 31/500 [00:25<09:59,  1.28s/it]  7%|▋         | 33/500 [00:25<07:06,  1.09it/s]  7%|▋         | 35/500 [00:28<08:26,  1.09s/it]  7%|▋         | 37/500 [00:28<06:01,  1.28it/s]  8%|▊         | 39/500 [00:28<04:20,  1.77it/s]  8%|▊         | 41/500 [00:34<09:50,  1.29s/it]  9%|▊         | 43/500 [00:34<06:59,  1.09it/s]  9%|▉         | 45/500 [00:37<08:19,  1.10s/it]  9%|▉         | 47/500 [00:37<05:57,  1.27it/s] 10%|▉         | 49/500 [00:38<04:17,  1.75it/s] 10%|█         | 51/500 [00:44<09:42,  1.30s/it] 11%|█         | 53/500 [00:44<06:55,  1.08it/s] 11%|█         | 55/500 [00:47<08:10,  1.10s/it] 11%|█▏        | 57/500 [00:47<05:49,  1.27it/s] 12%|█▏        | 59/500 [00:47<04:12,  1.75it/s] 12%|█▏        | 61/500 [00:53<09:28,  1.29s/it] 13%|█▎        | 63/500 [00:53<06:44,  1.08it/s]Epoch:  1  	Training Loss: 0.13826248049736023
Test Loss:  96.56864929199219
Valid Loss:  93.46575927734375
Epoch:  2  	Training Loss: 93.16255187988281
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  3  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  4  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  5  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  6  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  7  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  8  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  9  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  10  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  11  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
Epoch:  12  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  13  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  14  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  15  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
**************************************************learning rate decay**************************************************
Epoch:  16  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  17  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  18  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
Epoch:  19  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  20  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  21  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  22  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  23  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  24  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  25  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  26  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  27  	Training Loss: 0.13831505179405212
Test Loss:  0.1865594983100891
Valid Loss:  0.1785760223865509
Epoch:  28  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  29  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  30  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  31  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  32  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  33  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
Epoch:  34  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  35  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  36  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  37  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  38  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  39  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  40  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  41  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  42  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  43  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  44  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  45  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  46  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  47  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  48  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  49  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  50  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  51  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  52  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  53  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  54  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  55  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  56  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
Epoch:  57  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  58  	Training Loss: 0.13831505179405212
Test Loss:  0.1865594983100891
Valid Loss:  0.1785760223865509
Epoch:  59  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  60  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  61  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
Epoch:  62  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  63  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
 13%|█▎        | 65/500 [00:56<07:59,  1.10s/it] 13%|█▎        | 67/500 [00:56<05:42,  1.27it/s] 14%|█▍        | 69/500 [00:56<04:06,  1.75it/s] 14%|█▍        | 71/500 [00:59<06:07,  1.17it/s] 15%|█▍        | 73/500 [00:59<04:24,  1.61it/s] 15%|█▌        | 75/500 [01:03<06:17,  1.13it/s] 15%|█▌        | 77/500 [01:03<04:31,  1.56it/s] 16%|█▌        | 79/500 [01:03<03:16,  2.14it/s] 16%|█▌        | 81/500 [01:09<08:30,  1.22s/it] 17%|█▋        | 83/500 [01:09<06:03,  1.15it/s] 17%|█▋        | 85/500 [01:12<07:22,  1.07s/it] 17%|█▋        | 87/500 [01:12<05:16,  1.31it/s] 18%|█▊        | 89/500 [01:12<03:48,  1.80it/s] 18%|█▊        | 91/500 [01:18<08:46,  1.29s/it] 19%|█▊        | 93/500 [01:18<06:14,  1.09it/s] 19%|█▉        | 95/500 [01:21<07:24,  1.10s/it] 19%|█▉        | 97/500 [01:21<05:17,  1.27it/s] 20%|█▉        | 99/500 [01:22<03:48,  1.75it/s] 20%|██        | 101/500 [01:28<08:36,  1.29s/it] 21%|██        | 103/500 [01:28<06:07,  1.08it/s] 21%|██        | 105/500 [01:31<07:15,  1.10s/it] 21%|██▏       | 107/500 [01:31<05:10,  1.26it/s] 22%|██▏       | 109/500 [01:31<03:43,  1.75it/s] 22%|██▏       | 111/500 [01:37<08:22,  1.29s/it] 23%|██▎       | 113/500 [01:37<05:57,  1.08it/s] 23%|██▎       | 115/500 [01:40<07:05,  1.11s/it] 23%|██▎       | 117/500 [01:40<05:03,  1.26it/s] 24%|██▍       | 119/500 [01:40<03:38,  1.74it/s] 24%|██▍       | 121/500 [01:46<08:12,  1.30s/it] 25%|██▍       | 123/500 [01:46<05:50,  1.07it/s]Epoch:  64  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  65  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  66  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  67  	Training Loss: 0.13831505179405212
Test Loss:  0.1865594983100891
Valid Loss:  0.1785760372877121
Epoch:  68  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  69  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  70  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  71  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  72  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  73  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  74  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  75  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  76  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  77  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  78  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  79  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  80  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  81  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  82  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  83  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  84  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  85  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  86  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  87  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  88  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
Epoch:  89  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  90  	Training Loss: 0.13831505179405212
Test Loss:  0.1865594983100891
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  91  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  92  	Training Loss: 0.13831502199172974
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  93  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  94  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  95  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  96  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  97  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  98  	Training Loss: 0.13831503689289093
Test Loss:  0.1865594983100891
Valid Loss:  0.1785760372877121
Epoch:  99  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  100  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  101  	Training Loss: 0.13831502199172974
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  102  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  103  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  104  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  105  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  106  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  107  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  108  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  109  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  110  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  111  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  112  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  113  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  114  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  115  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  116  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  117  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  118  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  119  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  120  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  121  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  122  	Training Loss: 0.13831502199172974
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  123  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  124  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  125  	Training Loss: 0.13831505179405212
Test Loss:   25%|██▌       | 125/500 [01:50<06:56,  1.11s/it] 25%|██▌       | 127/500 [01:50<04:57,  1.25it/s] 26%|██▌       | 129/500 [01:50<03:34,  1.73it/s] 26%|██▌       | 131/500 [01:56<07:59,  1.30s/it] 27%|██▋       | 133/500 [01:56<05:41,  1.08it/s] 27%|██▋       | 135/500 [01:59<06:46,  1.11s/it] 27%|██▋       | 137/500 [01:59<04:49,  1.25it/s] 28%|██▊       | 139/500 [01:59<03:28,  1.73it/s] 28%|██▊       | 141/500 [02:05<07:42,  1.29s/it] 29%|██▊       | 143/500 [02:05<05:29,  1.08it/s] 29%|██▉       | 145/500 [02:08<06:30,  1.10s/it] 29%|██▉       | 147/500 [02:08<04:38,  1.27it/s] 30%|██▉       | 149/500 [02:09<03:20,  1.75it/s] 30%|███       | 151/500 [02:15<07:41,  1.32s/it] 31%|███       | 153/500 [02:15<05:28,  1.06it/s] 31%|███       | 155/500 [02:18<06:34,  1.14s/it] 31%|███▏      | 157/500 [02:18<04:41,  1.22it/s] 32%|███▏      | 159/500 [02:18<03:22,  1.68it/s] 32%|███▏      | 161/500 [02:24<07:23,  1.31s/it] 33%|███▎      | 163/500 [02:24<05:15,  1.07it/s] 33%|███▎      | 165/500 [02:27<06:10,  1.11s/it] 33%|███▎      | 167/500 [02:27<04:24,  1.26it/s] 34%|███▍      | 169/500 [02:28<03:10,  1.74it/s] 34%|███▍      | 171/500 [02:34<07:03,  1.29s/it] 35%|███▍      | 173/500 [02:34<05:01,  1.09it/s] 35%|███▌      | 175/500 [02:37<05:57,  1.10s/it] 35%|███▌      | 177/500 [02:37<04:15,  1.26it/s] 36%|███▌      | 179/500 [02:37<03:04,  1.74it/s] 36%|███▌      | 181/500 [02:43<06:55,  1.30s/it] 37%|███▋      | 183/500 [02:43<04:55,  1.07it/s]0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  126  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  127  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  128  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  129  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  130  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  131  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  132  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  133  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  134  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  135  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  136  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  137  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  138  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  139  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  140  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  141  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  142  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  143  	Training Loss: 0.13831503689289093
Test Loss:  0.1865594983100891
Valid Loss:  0.1785760223865509
Epoch:  144  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  145  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  146  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  147  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  148  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  149  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  150  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  151  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  152  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  153  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  154  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  155  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  156  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  157  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  158  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  159  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  160  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  161  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  162  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  163  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  164  	Training Loss: 0.13831503689289093
Test Loss:  0.1865594983100891
Valid Loss:  0.1785760223865509
Epoch:  165  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  166  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
Epoch:  167  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  168  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  169  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  170  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
**************************************************learning rate decay**************************************************
Epoch:  171  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  172  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  173  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  174  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  175  	Training Loss: 0.13831505179405212
Test Loss:  0.1865594983100891
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  176  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  177  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  178  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
Epoch:  179  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  180  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  181  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  182  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  183  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  184  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  185  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:   37%|███▋      | 185/500 [02:46<05:55,  1.13s/it] 37%|███▋      | 187/500 [02:46<04:13,  1.24it/s] 38%|███▊      | 189/500 [02:47<03:02,  1.71it/s] 38%|███▊      | 191/500 [02:53<06:43,  1.31s/it] 39%|███▊      | 193/500 [02:53<04:46,  1.07it/s] 39%|███▉      | 195/500 [02:56<05:37,  1.11s/it] 39%|███▉      | 197/500 [02:56<04:00,  1.26it/s] 40%|███▉      | 199/500 [02:56<02:53,  1.74it/s] 40%|████      | 201/500 [03:02<06:24,  1.29s/it] 41%|████      | 203/500 [03:02<04:33,  1.09it/s] 41%|████      | 205/500 [03:05<05:23,  1.10s/it] 41%|████▏     | 207/500 [03:05<03:50,  1.27it/s] 42%|████▏     | 209/500 [03:05<02:45,  1.75it/s] 42%|████▏     | 211/500 [03:11<06:13,  1.29s/it] 43%|████▎     | 213/500 [03:11<04:25,  1.08it/s] 43%|████▎     | 215/500 [03:14<05:13,  1.10s/it] 43%|████▎     | 217/500 [03:14<03:43,  1.27it/s] 44%|████▍     | 219/500 [03:15<02:40,  1.75it/s] 44%|████▍     | 221/500 [03:21<05:59,  1.29s/it] 45%|████▍     | 223/500 [03:21<04:16,  1.08it/s] 45%|████▌     | 225/500 [03:24<05:02,  1.10s/it] 45%|████▌     | 227/500 [03:24<03:35,  1.27it/s] 46%|████▌     | 229/500 [03:24<02:34,  1.75it/s] 46%|████▌     | 231/500 [03:30<05:48,  1.30s/it] 47%|████▋     | 233/500 [03:30<04:07,  1.08it/s] 47%|████▋     | 235/500 [03:33<04:58,  1.12s/it] 47%|████▋     | 237/500 [03:33<03:32,  1.24it/s] 48%|████▊     | 239/500 [03:33<02:32,  1.71it/s] 48%|████▊     | 241/500 [03:40<05:41,  1.32s/it] 49%|████▊     | 243/500 [03:40<04:02,  1.06it/s]0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  186  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  187  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  188  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  189  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  190  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  191  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  192  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
Epoch:  193  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  194  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  195  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  196  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  197  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  198  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  199  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  200  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  201  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  202  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  203  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  204  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  205  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  206  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  207  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  208  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  209  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  210  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  211  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  212  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  213  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  214  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  215  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  216  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  217  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  218  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  219  	Training Loss: 0.13831502199172974
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  220  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  221  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  222  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  223  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  224  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  225  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  226  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  227  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  228  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  229  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  230  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  231  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  232  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  233  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  234  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  235  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  236  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  237  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  238  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  239  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  240  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
**************************************************learning rate decay**************************************************
Epoch:  241  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  242  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  243  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  244  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  245  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
 49%|████▉     | 245/500 [03:43<04:43,  1.11s/it] 49%|████▉     | 247/500 [03:43<03:21,  1.25it/s] 50%|████▉     | 249/500 [03:43<02:24,  1.73it/s] 50%|█████     | 251/500 [03:49<05:33,  1.34s/it] 51%|█████     | 253/500 [03:49<03:56,  1.04it/s] 51%|█████     | 255/500 [03:52<04:34,  1.12s/it] 51%|█████▏    | 257/500 [03:52<03:15,  1.24it/s] 52%|█████▏    | 259/500 [03:53<02:20,  1.72it/s] 52%|█████▏    | 261/500 [03:59<05:10,  1.30s/it] 53%|█████▎    | 263/500 [03:59<03:40,  1.07it/s] 53%|█████▎    | 265/500 [04:02<04:19,  1.10s/it] 53%|█████▎    | 267/500 [04:02<03:04,  1.26it/s] 54%|█████▍    | 269/500 [04:02<02:12,  1.75it/s] 54%|█████▍    | 271/500 [04:08<05:02,  1.32s/it] 55%|█████▍    | 273/500 [04:08<03:34,  1.06it/s] 55%|█████▌    | 275/500 [04:11<04:11,  1.12s/it] 55%|█████▌    | 277/500 [04:11<02:59,  1.25it/s] 56%|█████▌    | 279/500 [04:12<02:08,  1.72it/s] 56%|█████▌    | 281/500 [04:17<04:45,  1.30s/it] 57%|█████▋    | 283/500 [04:18<03:22,  1.07it/s] 57%|█████▋    | 285/500 [04:21<03:58,  1.11s/it] 57%|█████▋    | 287/500 [04:21<02:49,  1.26it/s] 58%|█████▊    | 289/500 [04:21<02:01,  1.73it/s] 58%|█████▊    | 291/500 [04:27<04:36,  1.32s/it] 59%|█████▊    | 293/500 [04:27<03:15,  1.06it/s] 59%|█████▉    | 295/500 [04:30<03:48,  1.11s/it] 59%|█████▉    | 297/500 [04:30<02:42,  1.25it/s] 60%|█████▉    | 299/500 [04:30<01:56,  1.73it/s] 60%|██████    | 301/500 [04:36<04:17,  1.29s/it] 61%|██████    | 303/500 [04:36<03:02,  1.08it/s]**************************************************learning rate decay**************************************************
Epoch:  246  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  247  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  248  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  249  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  250  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  251  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  252  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  253  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  254  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  255  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  256  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  257  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  258  	Training Loss: 0.13831503689289093
Test Loss:  0.1865594983100891
Valid Loss:  0.1785760372877121
Epoch:  259  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  260  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  261  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  262  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  263  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  264  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  265  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  266  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  267  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  268  	Training Loss: 0.13831503689289093
Test Loss:  0.1865594983100891
Valid Loss:  0.1785760372877121
Epoch:  269  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  270  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  271  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  272  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  273  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  274  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  275  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
**************************************************learning rate decay**************************************************
Epoch:  276  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  277  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  278  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  279  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  280  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  281  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  282  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  283  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  284  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  285  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  286  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  287  	Training Loss: 0.13831505179405212
Test Loss:  0.1865594983100891
Valid Loss:  0.1785760521888733
Epoch:  288  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  289  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  290  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  291  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  292  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  293  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  294  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  295  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  296  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  297  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  298  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  299  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  300  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
**************************************************learning rate decay**************************************************
Epoch:  301  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  302  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  303  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  304  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  305  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
 61%|██████    | 305/500 [04:40<03:34,  1.10s/it] 61%|██████▏   | 307/500 [04:40<02:32,  1.27it/s] 62%|██████▏   | 309/500 [04:40<01:49,  1.75it/s] 62%|██████▏   | 311/500 [04:46<04:04,  1.29s/it] 63%|██████▎   | 313/500 [04:46<02:52,  1.08it/s] 63%|██████▎   | 315/500 [04:49<03:23,  1.10s/it] 63%|██████▎   | 317/500 [04:49<02:24,  1.26it/s] 64%|██████▍   | 319/500 [04:49<01:43,  1.74it/s] 64%|██████▍   | 321/500 [04:55<03:50,  1.29s/it] 65%|██████▍   | 323/500 [04:55<02:43,  1.08it/s] 65%|██████▌   | 325/500 [04:58<03:12,  1.10s/it] 65%|██████▌   | 327/500 [04:58<02:16,  1.27it/s] 66%|██████▌   | 329/500 [04:58<01:37,  1.75it/s] 66%|██████▌   | 331/500 [05:04<03:37,  1.29s/it] 67%|██████▋   | 333/500 [05:05<02:33,  1.09it/s] 67%|██████▋   | 335/500 [05:08<03:00,  1.10s/it] 67%|██████▋   | 337/500 [05:08<02:08,  1.27it/s] 68%|██████▊   | 339/500 [05:08<01:31,  1.75it/s] 68%|██████▊   | 341/500 [05:14<03:25,  1.29s/it] 69%|██████▊   | 343/500 [05:14<02:25,  1.08it/s] 69%|██████▉   | 345/500 [05:17<02:50,  1.10s/it] 69%|██████▉   | 347/500 [05:17<02:00,  1.27it/s] 70%|██████▉   | 349/500 [05:17<01:26,  1.75it/s] 70%|███████   | 351/500 [05:23<03:11,  1.29s/it] 71%|███████   | 353/500 [05:23<02:15,  1.09it/s] 71%|███████   | 355/500 [05:26<02:39,  1.10s/it] 71%|███████▏  | 357/500 [05:26<01:52,  1.27it/s] 72%|███████▏  | 359/500 [05:27<01:20,  1.75it/s] 72%|███████▏  | 361/500 [05:32<03:00,  1.30s/it] 73%|███████▎  | 363/500 [05:33<02:07,  1.07it/s]**************************************************learning rate decay**************************************************
Epoch:  306  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  307  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  308  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  309  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  310  	Training Loss: 0.13831503689289093
Test Loss:  0.1865594983100891
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  311  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  312  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  313  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  314  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  315  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  316  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  317  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  318  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  319  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  320  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  321  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  322  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  323  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  324  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  325  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
**************************************************learning rate decay**************************************************
Epoch:  326  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  327  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  328  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  329  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  330  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  331  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  332  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
Epoch:  333  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  334  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  335  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  336  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  337  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  338  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  339  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  340  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  341  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  342  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  343  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  344  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  345  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  346  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  347  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  348  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  349  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  350  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  351  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  352  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  353  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  354  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  355  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  356  	Training Loss: 0.13831503689289093
Test Loss:  0.1865594983100891
Valid Loss:  0.1785760521888733
Epoch:  357  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  358  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
Epoch:  359  	Training Loss: 0.13831502199172974
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  360  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  361  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  362  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  363  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  364  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  365  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
 73%|███████▎  | 365/500 [05:36<02:30,  1.11s/it] 73%|███████▎  | 367/500 [05:36<01:46,  1.25it/s] 74%|███████▍  | 369/500 [05:36<01:15,  1.73it/s] 74%|███████▍  | 371/500 [05:42<02:47,  1.30s/it] 75%|███████▍  | 373/500 [05:42<01:57,  1.08it/s] 75%|███████▌  | 375/500 [05:45<02:18,  1.10s/it] 75%|███████▌  | 377/500 [05:45<01:37,  1.26it/s] 76%|███████▌  | 379/500 [05:45<01:09,  1.74it/s] 76%|███████▌  | 381/500 [05:51<02:34,  1.30s/it] 77%|███████▋  | 383/500 [05:51<01:48,  1.08it/s] 77%|███████▋  | 385/500 [05:54<02:07,  1.10s/it] 77%|███████▋  | 387/500 [05:55<01:29,  1.26it/s] 78%|███████▊  | 389/500 [05:55<01:03,  1.74it/s] 78%|███████▊  | 391/500 [06:01<02:20,  1.29s/it] 79%|███████▊  | 393/500 [06:01<01:38,  1.08it/s] 79%|███████▉  | 395/500 [06:04<01:55,  1.10s/it] 79%|███████▉  | 397/500 [06:04<01:22,  1.25it/s] 80%|███████▉  | 399/500 [06:04<00:58,  1.73it/s] 80%|████████  | 401/500 [06:07<01:25,  1.16it/s] 81%|████████  | 403/500 [06:07<01:00,  1.61it/s] 81%|████████  | 405/500 [06:10<01:24,  1.12it/s] 81%|████████▏ | 407/500 [06:11<01:00,  1.55it/s] 82%|████████▏ | 409/500 [06:11<00:42,  2.12it/s] 82%|████████▏ | 411/500 [06:14<01:09,  1.28it/s] 83%|████████▎ | 413/500 [06:14<00:49,  1.77it/s] 83%|████████▎ | 415/500 [06:17<01:12,  1.17it/s] 83%|████████▎ | 417/500 [06:17<00:51,  1.61it/s] 84%|████████▍ | 419/500 [06:17<00:36,  2.21it/s] 84%|████████▍ | 421/500 [06:23<01:35,  1.21s/it] 85%|████████▍ | 423/500 [06:23<01:06,  1.16it/s] 85%|████████▌ | 425/500 [06:26<01:19,  1.06s/it]**************************************************learning rate decay**************************************************
Epoch:  366  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  367  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  368  	Training Loss: 0.13831503689289093
Test Loss:  0.1865594983100891
Valid Loss:  0.1785760521888733
Epoch:  369  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  370  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  371  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  372  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  373  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  374  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  375  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  376  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
Epoch:  377  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  378  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  379  	Training Loss: 0.13831503689289093
Test Loss:  0.1865594983100891
Valid Loss:  0.1785760223865509
Epoch:  380  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  381  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  382  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  383  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  384  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  385  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  386  	Training Loss: 0.13831503689289093
Test Loss:  0.1865594983100891
Valid Loss:  0.1785760372877121
Epoch:  387  	Training Loss: 0.13831503689289093
Test Loss:  0.1865594983100891
Valid Loss:  0.1785760372877121
Epoch:  388  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  389  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  390  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  391  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
Epoch:  392  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  393  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  394  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
Epoch:  395  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  396  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  397  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  398  	Training Loss: 0.13831503689289093
Test Loss:  0.1865594983100891
Valid Loss:  0.1785760372877121
Epoch:  399  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  400  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  401  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  402  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  403  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  404  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  405  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  406  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  407  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  408  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  409  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  410  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  411  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  412  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  413  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  414  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  415  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  416  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  417  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  418  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  419  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  420  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
**************************************************learning rate decay**************************************************
Epoch:  421  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  422  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  423  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  424  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  425  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  426  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
 85%|████████▌ | 427/500 [06:26<00:55,  1.32it/s] 86%|████████▌ | 429/500 [06:26<00:39,  1.81it/s] 86%|████████▌ | 431/500 [06:33<01:30,  1.31s/it] 87%|████████▋ | 433/500 [06:33<01:02,  1.07it/s] 87%|████████▋ | 435/500 [06:36<01:13,  1.13s/it] 87%|████████▋ | 437/500 [06:36<00:51,  1.23it/s] 88%|████████▊ | 439/500 [06:36<00:35,  1.69it/s] 88%|████████▊ | 441/500 [06:39<00:51,  1.14it/s] 89%|████████▊ | 443/500 [06:39<00:36,  1.58it/s] 89%|████████▉ | 445/500 [06:42<00:49,  1.12it/s] 89%|████████▉ | 447/500 [06:43<00:34,  1.54it/s] 90%|████████▉ | 449/500 [06:43<00:24,  2.12it/s] 90%|█████████ | 451/500 [06:49<01:00,  1.23s/it] 91%|█████████ | 453/500 [06:49<00:41,  1.14it/s] 91%|█████████ | 455/500 [06:52<00:48,  1.07s/it] 91%|█████████▏| 457/500 [06:52<00:32,  1.30it/s] 92%|█████████▏| 459/500 [06:52<00:22,  1.80it/s] 92%|█████████▏| 461/500 [06:58<00:51,  1.31s/it] 93%|█████████▎| 463/500 [06:58<00:34,  1.06it/s] 93%|█████████▎| 465/500 [07:01<00:38,  1.11s/it] 93%|█████████▎| 467/500 [07:02<00:26,  1.25it/s] 94%|█████████▍| 469/500 [07:02<00:17,  1.73it/s] 94%|█████████▍| 471/500 [07:08<00:37,  1.29s/it] 95%|█████████▍| 473/500 [07:08<00:24,  1.08it/s] 95%|█████████▌| 475/500 [07:11<00:27,  1.10s/it] 95%|█████████▌| 477/500 [07:11<00:18,  1.27it/s] 96%|█████████▌| 479/500 [07:11<00:12,  1.75it/s] 96%|█████████▌| 481/500 [07:17<00:24,  1.30s/it] 97%|█████████▋| 483/500 [07:17<00:15,  1.07it/s] 97%|█████████▋| 485/500 [07:20<00:16,  1.11s/it] 97%|█████████▋| 487/500 [07:20<00:10,  1.25it/s]Epoch:  427  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  428  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  429  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  430  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
**************************************************learning rate decay**************************************************
Epoch:  431  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  432  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  433  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  434  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  435  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  436  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  437  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  438  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  439  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  440  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  441  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
Epoch:  442  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  443  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  444  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  445  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  446  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  447  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  448  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  449  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
Epoch:  450  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  451  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  452  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  453  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  454  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  455  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  456  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  457  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  458  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  459  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  460  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  461  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  462  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  463  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  464  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  465  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  466  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  467  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  468  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  469  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  470  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  471  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  472  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  473  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  474  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760223865509
Epoch:  475  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  476  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  477  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  478  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  479  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  480  	Training Loss: 0.13831503689289093
Test Loss:  0.1865594983100891
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  481  	Training Loss: 0.13831502199172974
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  482  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  483  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  484  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  485  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
Epoch:  486  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  487  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760521888733
 98%|█████████▊| 489/500 [07:20<00:06,  1.73it/s] 98%|█████████▊| 491/500 [07:26<00:11,  1.29s/it] 99%|█████████▊| 493/500 [07:27<00:06,  1.08it/s] 99%|█████████▉| 495/500 [07:30<00:05,  1.10s/it] 99%|█████████▉| 497/500 [07:30<00:02,  1.26it/s]100%|█████████▉| 499/500 [07:30<00:00,  1.75it/s]100%|██████████| 500/500 [07:33<00:00,  1.10it/s]
Epoch:  488  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  489  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  490  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  491  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760372877121
Epoch:  492  	Training Loss: 0.13831505179405212
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  493  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  494  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  495  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
**************************************************learning rate decay**************************************************
Epoch:  496  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  497  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
Epoch:  498  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760521888733
Epoch:  499  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595132112503
Valid Loss:  0.1785760223865509
Epoch:  500  	Training Loss: 0.13831503689289093
Test Loss:  0.1865595281124115
Valid Loss:  0.1785760372877121
**************************************************learning rate decay**************************************************
seed is  15
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:02<24:50,  2.99s/it]  1%|          | 3/500 [00:03<06:48,  1.22it/s]  1%|          | 5/500 [00:03<03:33,  2.31it/s]  1%|▏         | 7/500 [00:03<02:16,  3.62it/s]  2%|▏         | 9/500 [00:03<01:36,  5.11it/s]  2%|▏         | 11/500 [00:06<05:22,  1.52it/s]  3%|▎         | 13/500 [00:06<03:45,  2.16it/s]  3%|▎         | 15/500 [00:09<06:28,  1.25it/s]  3%|▎         | 17/500 [00:09<04:34,  1.76it/s]  4%|▍         | 19/500 [00:09<03:17,  2.43it/s]  4%|▍         | 21/500 [00:15<09:38,  1.21s/it]  5%|▍         | 23/500 [00:16<06:49,  1.16it/s]  5%|▌         | 25/500 [00:19<08:21,  1.06s/it]  5%|▌         | 27/500 [00:19<05:58,  1.32it/s]  6%|▌         | 29/500 [00:19<04:18,  1.82it/s]  6%|▌         | 31/500 [00:25<10:03,  1.29s/it]  7%|▋         | 33/500 [00:25<07:09,  1.09it/s]  7%|▋         | 35/500 [00:28<08:29,  1.10s/it]  7%|▋         | 37/500 [00:28<06:03,  1.27it/s]  8%|▊         | 39/500 [00:28<04:22,  1.76it/s]  8%|▊         | 41/500 [00:34<10:10,  1.33s/it]  9%|▊         | 43/500 [00:35<07:13,  1.05it/s]  9%|▉         | 45/500 [00:38<08:46,  1.16s/it]  9%|▉         | 47/500 [00:38<06:17,  1.20it/s] 10%|▉         | 49/500 [00:38<04:31,  1.66it/s] 10%|█         | 51/500 [00:44<10:02,  1.34s/it] 11%|█         | 53/500 [00:44<07:08,  1.04it/s] 11%|█         | 55/500 [00:47<08:18,  1.12s/it] 11%|█▏        | 57/500 [00:48<05:56,  1.24it/s] 12%|█▏        | 59/500 [00:48<04:16,  1.72it/s] 12%|█▏        | 61/500 [00:54<09:29,  1.30s/it]Epoch:  1  	Training Loss: 0.12840673327445984
Test Loss:  72.80530548095703
Valid Loss:  70.14820861816406
Epoch:  2  	Training Loss: 72.06468200683594
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  3  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  4  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  5  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  6  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  7  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  8  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  9  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  10  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  11  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  12  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  13  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  14  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  15  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  16  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  17  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  18  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  19  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  20  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  21  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  22  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  23  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  24  	Training Loss: 0.13146667182445526
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  25  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  26  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  27  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  28  	Training Loss: 0.13146667182445526
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  29  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  30  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  31  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  32  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  33  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  34  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  35  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  36  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  37  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  38  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  39  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  40  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  41  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  42  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  43  	Training Loss: 0.13146667182445526
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  44  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  45  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  46  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  47  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  48  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  49  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  50  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  51  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  52  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  53  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  54  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  55  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  56  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  57  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  58  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  59  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  60  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  61  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  62  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
 13%|█▎        | 63/500 [00:54<06:45,  1.08it/s] 13%|█▎        | 65/500 [00:57<08:00,  1.11s/it] 13%|█▎        | 67/500 [00:57<05:43,  1.26it/s] 14%|█▍        | 69/500 [00:57<04:07,  1.74it/s] 14%|█▍        | 71/500 [01:03<09:18,  1.30s/it] 15%|█▍        | 73/500 [01:03<06:38,  1.07it/s] 15%|█▌        | 75/500 [01:06<07:48,  1.10s/it] 15%|█▌        | 77/500 [01:06<05:34,  1.27it/s] 16%|█▌        | 79/500 [01:06<04:00,  1.75it/s] 16%|█▌        | 81/500 [01:12<09:00,  1.29s/it] 17%|█▋        | 83/500 [01:12<06:24,  1.08it/s] 17%|█▋        | 85/500 [01:16<07:37,  1.10s/it] 17%|█▋        | 87/500 [01:16<05:27,  1.26it/s] 18%|█▊        | 89/500 [01:16<03:56,  1.74it/s] 18%|█▊        | 91/500 [01:22<09:01,  1.32s/it] 19%|█▊        | 93/500 [01:22<06:25,  1.06it/s] 19%|█▉        | 95/500 [01:25<07:30,  1.11s/it] 19%|█▉        | 97/500 [01:25<05:22,  1.25it/s] 20%|█▉        | 99/500 [01:25<03:52,  1.73it/s] 20%|██        | 101/500 [01:31<08:41,  1.31s/it] 21%|██        | 103/500 [01:31<06:10,  1.07it/s] 21%|██        | 105/500 [01:35<07:24,  1.12s/it] 21%|██▏       | 107/500 [01:35<05:17,  1.24it/s] 22%|██▏       | 109/500 [01:35<03:48,  1.71it/s] 22%|██▏       | 111/500 [01:41<08:33,  1.32s/it] 23%|██▎       | 113/500 [01:41<06:05,  1.06it/s] 23%|██▎       | 115/500 [01:44<07:10,  1.12s/it] 23%|██▎       | 117/500 [01:44<05:06,  1.25it/s] 24%|██▍       | 119/500 [01:44<03:40,  1.73it/s] 24%|██▍       | 121/500 [01:51<08:23,  1.33s/it]Epoch:  63  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  64  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  65  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  66  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  67  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  68  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  69  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  70  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  71  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  72  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  73  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  74  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  75  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  76  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  77  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  78  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  79  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  80  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  81  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  82  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  83  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  84  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  85  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  86  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  87  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  88  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  89  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  90  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  91  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  92  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  93  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  94  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  95  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  96  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  97  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  98  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  99  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  100  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  101  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  102  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  103  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  104  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  105  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  106  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  107  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  108  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  109  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  110  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  111  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  112  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  113  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  114  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  115  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  116  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  117  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  118  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  119  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  120  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  121  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  122  	Training Loss: 0.13146668672561646
Test Loss:   25%|██▍       | 123/500 [01:51<05:57,  1.05it/s] 25%|██▌       | 125/500 [01:54<07:00,  1.12s/it] 25%|██▌       | 127/500 [01:54<05:00,  1.24it/s] 26%|██▌       | 129/500 [01:54<03:36,  1.71it/s] 26%|██▌       | 131/500 [02:00<08:07,  1.32s/it] 27%|██▋       | 133/500 [02:00<05:47,  1.06it/s] 27%|██▋       | 135/500 [02:03<06:48,  1.12s/it] 27%|██▋       | 137/500 [02:03<04:51,  1.25it/s] 28%|██▊       | 139/500 [02:04<03:29,  1.72it/s] 28%|██▊       | 141/500 [02:09<07:44,  1.29s/it] 29%|██▊       | 143/500 [02:10<05:30,  1.08it/s] 29%|██▉       | 145/500 [02:13<06:32,  1.10s/it] 29%|██▉       | 147/500 [02:13<04:40,  1.26it/s] 30%|██▉       | 149/500 [02:13<03:21,  1.74it/s] 30%|███       | 151/500 [02:19<07:33,  1.30s/it] 31%|███       | 153/500 [02:19<05:22,  1.08it/s] 31%|███       | 155/500 [02:22<06:22,  1.11s/it] 31%|███▏      | 157/500 [02:22<04:33,  1.26it/s] 32%|███▏      | 159/500 [02:22<03:16,  1.74it/s] 32%|███▏      | 161/500 [02:28<07:23,  1.31s/it] 33%|███▎      | 163/500 [02:28<05:15,  1.07it/s] 33%|███▎      | 165/500 [02:31<06:11,  1.11s/it] 33%|███▎      | 167/500 [02:32<04:24,  1.26it/s] 34%|███▍      | 169/500 [02:32<03:10,  1.74it/s] 34%|███▍      | 171/500 [02:38<07:07,  1.30s/it] 35%|███▍      | 173/500 [02:38<05:03,  1.08it/s] 35%|███▌      | 175/500 [02:41<06:00,  1.11s/it] 35%|███▌      | 177/500 [02:41<04:17,  1.26it/s] 36%|███▌      | 179/500 [02:41<03:05,  1.73it/s]0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  123  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  124  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  125  	Training Loss: 0.13146667182445526
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  126  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  127  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  128  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  129  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  130  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  131  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  132  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  133  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  134  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  135  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  136  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  137  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  138  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  139  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  140  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  141  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  142  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  143  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  144  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  145  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  146  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  147  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  148  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  149  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  150  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  151  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  152  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  153  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  154  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  155  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  156  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  157  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  158  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  159  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  160  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  161  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  162  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  163  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  164  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  165  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  166  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  167  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  168  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  169  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  170  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  171  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  172  	Training Loss: 0.13146667182445526
Test Loss:  0.16604822874069214
Valid Loss:  0.14861945807933807
Epoch:  173  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  174  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  175  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  176  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  177  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  178  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  179  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  180  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  181  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
 36%|███▌      | 181/500 [02:47<06:55,  1.30s/it] 37%|███▋      | 183/500 [02:47<04:54,  1.07it/s] 37%|███▋      | 185/500 [02:50<05:49,  1.11s/it] 37%|███▋      | 187/500 [02:50<04:09,  1.25it/s] 38%|███▊      | 189/500 [02:51<02:59,  1.73it/s] 38%|███▊      | 191/500 [02:57<06:43,  1.30s/it] 39%|███▊      | 193/500 [02:57<04:46,  1.07it/s] 39%|███▉      | 195/500 [03:00<05:37,  1.11s/it] 39%|███▉      | 197/500 [03:00<04:00,  1.26it/s] 40%|███▉      | 199/500 [03:00<02:53,  1.74it/s] 40%|████      | 201/500 [03:06<06:29,  1.30s/it] 41%|████      | 203/500 [03:06<04:36,  1.07it/s] 41%|████      | 205/500 [03:09<05:27,  1.11s/it] 41%|████▏     | 207/500 [03:09<03:53,  1.25it/s] 42%|████▏     | 209/500 [03:09<02:47,  1.73it/s] 42%|████▏     | 211/500 [03:16<06:18,  1.31s/it] 43%|████▎     | 213/500 [03:16<04:28,  1.07it/s] 43%|████▎     | 215/500 [03:19<05:16,  1.11s/it] 43%|████▎     | 217/500 [03:19<03:45,  1.26it/s] 44%|████▍     | 219/500 [03:19<02:42,  1.73it/s] 44%|████▍     | 221/500 [03:25<06:01,  1.29s/it] 45%|████▍     | 223/500 [03:25<04:16,  1.08it/s] 45%|████▌     | 225/500 [03:28<05:04,  1.11s/it] 45%|████▌     | 227/500 [03:28<03:38,  1.25it/s] 46%|████▌     | 229/500 [03:28<02:37,  1.72it/s] 46%|████▌     | 231/500 [03:34<05:54,  1.32s/it] 47%|████▋     | 233/500 [03:35<04:12,  1.06it/s] 47%|████▋     | 235/500 [03:38<05:01,  1.14s/it] 47%|████▋     | 237/500 [03:38<03:34,  1.23it/s] 48%|████▊     | 239/500 [03:38<02:34,  1.69it/s]Valid Loss:  0.14861944317817688
Epoch:  182  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  183  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  184  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  185  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  186  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  187  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  188  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  189  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  190  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  191  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861945807933807
Epoch:  192  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  193  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  194  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  195  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  196  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  197  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  198  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  199  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  200  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  201  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  202  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  203  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861945807933807
Epoch:  204  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  205  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  206  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  207  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  208  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  209  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  210  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  211  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  212  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  213  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  214  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  215  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  216  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  217  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  218  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  219  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  220  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  221  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  222  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  223  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  224  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  225  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  226  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  227  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  228  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  229  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  230  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  231  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  232  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  233  	Training Loss: 0.13146667182445526
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  234  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  235  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  236  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  237  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  238  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  239  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  240  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
 48%|████▊     | 241/500 [03:44<05:40,  1.31s/it] 49%|████▊     | 243/500 [03:44<04:01,  1.06it/s] 49%|████▉     | 245/500 [03:47<04:43,  1.11s/it] 49%|████▉     | 247/500 [03:47<03:22,  1.25it/s] 50%|████▉     | 249/500 [03:47<02:25,  1.73it/s] 50%|█████     | 251/500 [03:53<05:23,  1.30s/it] 51%|█████     | 253/500 [03:54<03:49,  1.08it/s] 51%|█████     | 255/500 [03:57<04:30,  1.11s/it] 51%|█████▏    | 257/500 [03:57<03:12,  1.26it/s] 52%|█████▏    | 259/500 [03:57<02:18,  1.74it/s] 52%|█████▏    | 261/500 [04:03<05:12,  1.31s/it] 53%|█████▎    | 263/500 [04:03<03:41,  1.07it/s] 53%|█████▎    | 265/500 [04:06<04:20,  1.11s/it] 53%|█████▎    | 267/500 [04:06<03:05,  1.25it/s] 54%|█████▍    | 269/500 [04:06<02:13,  1.73it/s] 54%|█████▍    | 271/500 [04:12<04:59,  1.31s/it] 55%|█████▍    | 273/500 [04:12<03:32,  1.07it/s] 55%|█████▌    | 275/500 [04:16<04:15,  1.14s/it] 55%|█████▌    | 277/500 [04:16<03:02,  1.23it/s] 56%|█████▌    | 279/500 [04:16<02:10,  1.69it/s] 56%|█████▌    | 281/500 [04:22<04:45,  1.31s/it] 57%|█████▋    | 283/500 [04:22<03:22,  1.07it/s] 57%|█████▋    | 285/500 [04:25<03:58,  1.11s/it] 57%|█████▋    | 287/500 [04:25<02:49,  1.25it/s] 58%|█████▊    | 289/500 [04:25<02:01,  1.73it/s] 58%|█████▊    | 291/500 [04:31<04:30,  1.29s/it] 59%|█████▊    | 293/500 [04:31<03:11,  1.08it/s] 59%|█████▉    | 295/500 [04:34<03:45,  1.10s/it] 59%|█████▉    | 297/500 [04:35<02:40,  1.27it/s] 60%|█████▉    | 299/500 [04:35<01:54,  1.75it/s]**************************************************learning rate decay**************************************************
Epoch:  241  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  242  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  243  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  244  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  245  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  246  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  247  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  248  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  249  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  250  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  251  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  252  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  253  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  254  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  255  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  256  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  257  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  258  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  259  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  260  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  261  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  262  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  263  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  264  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  265  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  266  	Training Loss: 0.13146667182445526
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  267  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  268  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  269  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  270  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  271  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  272  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  273  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  274  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  275  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  276  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  277  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  278  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  279  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  280  	Training Loss: 0.13146667182445526
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  281  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  282  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  283  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  284  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  285  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  286  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  287  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  288  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  289  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  290  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  291  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  292  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  293  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  294  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  295  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  296  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  297  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  298  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  299  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
 60%|██████    | 301/500 [04:41<04:20,  1.31s/it] 61%|██████    | 303/500 [04:41<03:04,  1.07it/s] 61%|██████    | 305/500 [04:44<03:44,  1.15s/it] 61%|██████▏   | 307/500 [04:44<02:39,  1.21it/s] 62%|██████▏   | 309/500 [04:44<01:54,  1.67it/s] 62%|██████▏   | 311/500 [04:51<04:11,  1.33s/it] 63%|██████▎   | 313/500 [04:51<02:58,  1.05it/s] 63%|██████▎   | 315/500 [04:54<03:33,  1.15s/it] 63%|██████▎   | 317/500 [04:54<02:31,  1.21it/s] 64%|██████▍   | 319/500 [04:54<01:48,  1.67it/s] 64%|██████▍   | 321/500 [05:00<03:56,  1.32s/it] 65%|██████▍   | 323/500 [05:00<02:47,  1.06it/s] 65%|██████▌   | 325/500 [05:04<03:19,  1.14s/it] 65%|██████▌   | 327/500 [05:04<02:21,  1.22it/s] 66%|██████▌   | 329/500 [05:04<01:41,  1.69it/s] 66%|██████▌   | 331/500 [05:10<03:39,  1.30s/it] 67%|██████▋   | 333/500 [05:10<02:34,  1.08it/s] 67%|██████▋   | 335/500 [05:13<03:03,  1.11s/it] 67%|██████▋   | 337/500 [05:13<02:10,  1.25it/s] 68%|██████▊   | 339/500 [05:13<01:33,  1.72it/s] 68%|██████▊   | 341/500 [05:19<03:31,  1.33s/it] 69%|██████▊   | 343/500 [05:19<02:29,  1.05it/s] 69%|██████▉   | 345/500 [05:23<02:55,  1.13s/it] 69%|██████▉   | 347/500 [05:23<02:04,  1.23it/s] 70%|██████▉   | 349/500 [05:23<01:28,  1.70it/s] 70%|███████   | 351/500 [05:29<03:18,  1.33s/it] 71%|███████   | 353/500 [05:29<02:20,  1.05it/s] 71%|███████   | 355/500 [05:32<02:43,  1.13s/it] 71%|███████▏  | 357/500 [05:32<01:55,  1.24it/s]Epoch:  300  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  301  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  302  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  303  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  304  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  305  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  306  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  307  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  308  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  309  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  310  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  311  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  312  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  313  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  314  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  315  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  316  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  317  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  318  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  319  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  320  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  321  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  322  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  323  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  324  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  325  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  326  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  327  	Training Loss: 0.13146667182445526
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  328  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  329  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  330  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  331  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  332  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  333  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  334  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  335  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  336  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  337  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  338  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  339  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  340  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  341  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  342  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  343  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  344  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  345  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  346  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  347  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  348  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  349  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  350  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  351  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  352  	Training Loss: 0.13146667182445526
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  353  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  354  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  355  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  356  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  357  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  358  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
 72%|███████▏  | 359/500 [05:32<01:22,  1.71it/s] 72%|███████▏  | 361/500 [05:38<03:03,  1.32s/it] 73%|███████▎  | 363/500 [05:39<02:09,  1.06it/s] 73%|███████▎  | 365/500 [05:42<02:31,  1.12s/it] 73%|███████▎  | 367/500 [05:42<01:47,  1.24it/s] 74%|███████▍  | 369/500 [05:42<01:16,  1.72it/s] 74%|███████▍  | 371/500 [05:48<02:48,  1.31s/it] 75%|███████▍  | 373/500 [05:48<01:58,  1.07it/s] 75%|███████▌  | 375/500 [05:51<02:19,  1.11s/it] 75%|███████▌  | 377/500 [05:51<01:38,  1.25it/s] 76%|███████▌  | 379/500 [05:51<01:09,  1.73it/s] 76%|███████▌  | 381/500 [05:57<02:35,  1.30s/it] 77%|███████▋  | 383/500 [05:58<01:49,  1.07it/s] 77%|███████▋  | 385/500 [06:01<02:14,  1.17s/it] 77%|███████▋  | 387/500 [06:01<01:34,  1.19it/s] 78%|███████▊  | 389/500 [06:01<01:07,  1.65it/s] 78%|███████▊  | 391/500 [06:07<02:24,  1.33s/it] 79%|███████▊  | 393/500 [06:07<01:41,  1.05it/s] 79%|███████▉  | 395/500 [06:10<01:57,  1.12s/it] 79%|███████▉  | 397/500 [06:11<01:22,  1.24it/s] 80%|███████▉  | 399/500 [06:11<00:58,  1.72it/s] 80%|████████  | 401/500 [06:17<02:08,  1.29s/it] 81%|████████  | 403/500 [06:17<01:29,  1.08it/s] 81%|████████  | 405/500 [06:20<01:44,  1.10s/it] 81%|████████▏ | 407/500 [06:20<01:13,  1.26it/s] 82%|████████▏ | 409/500 [06:20<00:52,  1.74it/s] 82%|████████▏ | 411/500 [06:26<01:54,  1.29s/it] 83%|████████▎ | 413/500 [06:26<01:20,  1.08it/s] 83%|████████▎ | 415/500 [06:29<01:33,  1.10s/it] 83%|████████▎ | 417/500 [06:29<01:05,  1.27it/s]Epoch:  359  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  360  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  361  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  362  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  363  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  364  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  365  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  366  	Training Loss: 0.13146667182445526
Test Loss:  0.16604822874069214
Valid Loss:  0.14861945807933807
Epoch:  367  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  368  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  369  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  370  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  371  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  372  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  373  	Training Loss: 0.13146667182445526
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  374  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  375  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  376  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  377  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861945807933807
Epoch:  378  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  379  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  380  	Training Loss: 0.13146667182445526
Test Loss:  0.16604822874069214
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  381  	Training Loss: 0.13146667182445526
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  382  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  383  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  384  	Training Loss: 0.13146667182445526
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  385  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  386  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  387  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  388  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  389  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  390  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  391  	Training Loss: 0.13146667182445526
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  392  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  393  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  394  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  395  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  396  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  397  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  398  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  399  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  400  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  401  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  402  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  403  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  404  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  405  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  406  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  407  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  408  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  409  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  410  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  411  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  412  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  413  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  414  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  415  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  416  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  417  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
 84%|████████▍ | 419/500 [06:29<00:46,  1.75it/s] 84%|████████▍ | 421/500 [06:35<01:42,  1.29s/it] 85%|████████▍ | 423/500 [06:35<01:11,  1.08it/s] 85%|████████▌ | 425/500 [06:39<01:23,  1.11s/it] 85%|████████▌ | 427/500 [06:39<00:58,  1.25it/s] 86%|████████▌ | 429/500 [06:39<00:41,  1.73it/s] 86%|████████▌ | 431/500 [06:45<01:33,  1.35s/it] 87%|████████▋ | 433/500 [06:45<01:04,  1.04it/s] 87%|████████▋ | 435/500 [06:48<01:13,  1.13s/it] 87%|████████▋ | 437/500 [06:48<00:50,  1.24it/s] 88%|████████▊ | 439/500 [06:49<00:35,  1.71it/s] 88%|████████▊ | 441/500 [06:54<01:16,  1.30s/it] 89%|████████▊ | 443/500 [06:55<00:53,  1.07it/s] 89%|████████▉ | 445/500 [06:58<01:00,  1.10s/it] 89%|████████▉ | 447/500 [06:58<00:42,  1.26it/s] 90%|████████▉ | 449/500 [06:58<00:29,  1.74it/s] 90%|█████████ | 451/500 [07:04<01:06,  1.37s/it] 91%|█████████ | 453/500 [07:04<00:45,  1.02it/s] 91%|█████████ | 455/500 [07:07<00:51,  1.14s/it] 91%|█████████▏| 457/500 [07:08<00:35,  1.23it/s] 92%|█████████▏| 459/500 [07:08<00:24,  1.70it/s] 92%|█████████▏| 461/500 [07:14<00:50,  1.31s/it] 93%|█████████▎| 463/500 [07:14<00:34,  1.07it/s] 93%|█████████▎| 465/500 [07:17<00:39,  1.12s/it] 93%|█████████▎| 467/500 [07:17<00:26,  1.25it/s] 94%|█████████▍| 469/500 [07:17<00:18,  1.72it/s] 94%|█████████▍| 471/500 [07:23<00:38,  1.31s/it] 95%|█████████▍| 473/500 [07:23<00:25,  1.06it/s] 95%|█████████▌| 475/500 [07:26<00:27,  1.12s/it]Epoch:  418  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  419  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  420  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  421  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  422  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  423  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  424  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  425  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  426  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  427  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  428  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  429  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  430  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  431  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  432  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  433  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  434  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  435  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  436  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  437  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  438  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  439  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  440  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  441  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  442  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  443  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  444  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  445  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  446  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  447  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  448  	Training Loss: 0.13146667182445526
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  449  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  450  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  451  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  452  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  453  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  454  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  455  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  456  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  457  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  458  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  459  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  460  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  461  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  462  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  463  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  464  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  465  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  466  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  467  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  468  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  469  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  470  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  471  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  472  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  473  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  474  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  475  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
**************************************************learning rate decay**************************************************
Epoch:  476  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
 95%|█████████▌| 477/500 [07:27<00:18,  1.25it/s] 96%|█████████▌| 479/500 [07:27<00:12,  1.73it/s] 96%|█████████▌| 481/500 [07:33<00:24,  1.31s/it] 97%|█████████▋| 483/500 [07:33<00:15,  1.07it/s] 97%|█████████▋| 485/500 [07:36<00:16,  1.11s/it] 97%|█████████▋| 487/500 [07:36<00:10,  1.25it/s] 98%|█████████▊| 489/500 [07:36<00:06,  1.73it/s] 98%|█████████▊| 491/500 [07:42<00:11,  1.30s/it] 99%|█████████▊| 493/500 [07:42<00:06,  1.07it/s] 99%|█████████▉| 495/500 [07:45<00:05,  1.11s/it] 99%|█████████▉| 497/500 [07:45<00:02,  1.26it/s]100%|█████████▉| 499/500 [07:46<00:00,  1.74it/s]100%|██████████| 500/500 [07:49<00:00,  1.07it/s]
Epoch:  477  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  478  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  479  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  480  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  481  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  482  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  483  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  484  	Training Loss: 0.13146667182445526
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
Epoch:  485  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  486  	Training Loss: 0.13146667182445526
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  487  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  488  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  489  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  490  	Training Loss: 0.13146668672561646
Test Loss:  0.16604822874069214
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  491  	Training Loss: 0.13146667182445526
Test Loss:  0.16604819893836975
Valid Loss:  0.14861945807933807
Epoch:  492  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  493  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861945807933807
Epoch:  494  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  495  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
Epoch:  496  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  497  	Training Loss: 0.13146668672561646
Test Loss:  0.16604819893836975
Valid Loss:  0.14861944317817688
Epoch:  498  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  499  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
Epoch:  500  	Training Loss: 0.13146668672561646
Test Loss:  0.16604821383953094
Valid Loss:  0.14861944317817688
**************************************************learning rate decay**************************************************
seed is  16
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:03<25:43,  3.09s/it]  1%|          | 3/500 [00:03<07:02,  1.18it/s]  1%|          | 5/500 [00:03<03:40,  2.25it/s]  1%|▏         | 7/500 [00:03<02:19,  3.53it/s]  2%|▏         | 9/500 [00:03<01:38,  4.99it/s]  2%|▏         | 11/500 [00:06<05:27,  1.49it/s]  3%|▎         | 13/500 [00:06<03:48,  2.13it/s]  3%|▎         | 15/500 [00:09<06:32,  1.24it/s]  3%|▎         | 17/500 [00:09<04:37,  1.74it/s]  4%|▍         | 19/500 [00:10<03:19,  2.41it/s]  4%|▍         | 21/500 [00:16<09:40,  1.21s/it]  5%|▍         | 23/500 [00:16<06:51,  1.16it/s]  5%|▌         | 25/500 [00:19<08:26,  1.07s/it]  5%|▌         | 27/500 [00:19<06:02,  1.31it/s]  6%|▌         | 29/500 [00:19<04:20,  1.81it/s]  6%|▌         | 31/500 [00:25<09:59,  1.28s/it]  7%|▋         | 33/500 [00:25<07:06,  1.09it/s]  7%|▋         | 35/500 [00:28<08:28,  1.09s/it]  7%|▋         | 37/500 [00:28<06:02,  1.28it/s]  8%|▊         | 39/500 [00:28<04:21,  1.76it/s]  8%|▊         | 41/500 [00:34<09:53,  1.29s/it]  9%|▊         | 43/500 [00:35<07:01,  1.08it/s]  9%|▉         | 45/500 [00:38<08:21,  1.10s/it]  9%|▉         | 47/500 [00:38<05:58,  1.26it/s] 10%|▉         | 49/500 [00:38<04:18,  1.74it/s] 10%|█         | 51/500 [00:44<09:42,  1.30s/it] 11%|█         | 53/500 [00:44<06:55,  1.08it/s] 11%|█         | 55/500 [00:47<08:11,  1.10s/it] 11%|█▏        | 57/500 [00:47<05:50,  1.26it/s] 12%|█▏        | 59/500 [00:47<04:12,  1.74it/s] 12%|█▏        | 61/500 [00:53<09:28,  1.29s/it] 13%|█▎        | 63/500 [00:53<06:44,  1.08it/s]Epoch:  1  	Training Loss: 0.3186855912208557
Test Loss:  253.8795166015625
Valid Loss:  244.808349609375
Epoch:  2  	Training Loss: 245.5336456298828
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  3  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  4  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  5  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  6  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  7  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  8  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  9  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  10  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  11  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  12  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014091491699
Valid Loss:  0.5123908519744873
Epoch:  13  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  14  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  15  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  16  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  17  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  18  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  19  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  20  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  21  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  22  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  23  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  24  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  25  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  26  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  27  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  28  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  29  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  30  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  31  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123908519744873
Epoch:  32  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  33  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  34  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  35  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  36  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  37  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  38  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  39  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  40  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  41  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  42  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  43  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  44  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  45  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  46  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  47  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  48  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  49  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123908519744873
Epoch:  50  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  51  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
Epoch:  52  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  53  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  54  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
Epoch:  55  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  56  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  57  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  58  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  59  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  60  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  61  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  62  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  63  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  64  	Training Loss: 0.3829297423362732
Test Loss:   13%|█▎        | 65/500 [00:56<08:01,  1.11s/it] 13%|█▎        | 67/500 [00:56<05:44,  1.26it/s] 14%|█▍        | 69/500 [00:57<04:07,  1.74it/s] 14%|█▍        | 71/500 [01:03<09:22,  1.31s/it] 15%|█▍        | 73/500 [01:03<06:40,  1.07it/s] 15%|█▌        | 75/500 [01:06<07:54,  1.12s/it] 15%|█▌        | 77/500 [01:06<05:38,  1.25it/s] 16%|█▌        | 79/500 [01:06<04:03,  1.73it/s] 16%|█▌        | 81/500 [01:12<09:04,  1.30s/it] 17%|█▋        | 83/500 [01:12<06:27,  1.08it/s] 17%|█▋        | 85/500 [01:15<07:40,  1.11s/it] 17%|█▋        | 87/500 [01:15<05:29,  1.25it/s] 18%|█▊        | 89/500 [01:16<03:57,  1.73it/s] 18%|█▊        | 91/500 [01:22<08:52,  1.30s/it] 19%|█▊        | 93/500 [01:22<06:19,  1.07it/s] 19%|█▉        | 95/500 [01:25<07:29,  1.11s/it] 19%|█▉        | 97/500 [01:25<05:20,  1.26it/s] 20%|█▉        | 99/500 [01:25<03:50,  1.74it/s] 20%|██        | 101/500 [01:31<08:41,  1.31s/it] 21%|██        | 103/500 [01:31<06:10,  1.07it/s] 21%|██        | 105/500 [01:34<07:22,  1.12s/it] 21%|██▏       | 107/500 [01:34<05:16,  1.24it/s] 22%|██▏       | 109/500 [01:35<03:47,  1.72it/s] 22%|██▏       | 111/500 [01:40<08:27,  1.30s/it] 23%|██▎       | 113/500 [01:41<06:01,  1.07it/s] 23%|██▎       | 115/500 [01:44<07:06,  1.11s/it] 23%|██▎       | 117/500 [01:44<05:04,  1.26it/s] 24%|██▍       | 119/500 [01:44<03:39,  1.74it/s] 24%|██▍       | 121/500 [01:50<08:35,  1.36s/it] 25%|██▍       | 123/500 [01:50<06:06,  1.03it/s]0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  65  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  66  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  67  	Training Loss: 0.3829297125339508
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
Epoch:  68  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  69  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  70  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  71  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  72  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  73  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  74  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  75  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  76  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  77  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  78  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  79  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  80  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  81  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  82  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  83  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  84  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
Epoch:  85  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  86  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  87  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  88  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  89  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  90  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  91  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  92  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  93  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  94  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  95  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  96  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  97  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  98  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  99  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  100  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  101  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  102  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  103  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  104  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  105  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  106  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  107  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  108  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  109  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  110  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  111  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  112  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  113  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  114  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  115  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  116  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  117  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  118  	Training Loss: 0.3829297125339508
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  119  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  120  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  121  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123908519744873
Epoch:  122  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  123  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
Epoch:  124  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  125  	Training Loss: 0.3829297423362732
Test Loss:   25%|██▌       | 125/500 [01:54<07:23,  1.18s/it] 25%|██▌       | 127/500 [01:54<05:16,  1.18it/s] 26%|██▌       | 129/500 [01:54<03:47,  1.63it/s] 26%|██▌       | 131/500 [02:00<08:13,  1.34s/it] 27%|██▋       | 133/500 [02:00<05:51,  1.04it/s] 27%|██▋       | 135/500 [02:03<06:49,  1.12s/it] 27%|██▋       | 137/500 [02:03<04:51,  1.24it/s] 28%|██▊       | 139/500 [02:03<03:30,  1.72it/s] 28%|██▊       | 141/500 [02:10<08:03,  1.35s/it] 29%|██▊       | 143/500 [02:10<05:43,  1.04it/s] 29%|██▉       | 145/500 [02:13<06:48,  1.15s/it] 29%|██▉       | 147/500 [02:13<04:51,  1.21it/s] 30%|██▉       | 149/500 [02:13<03:29,  1.67it/s] 30%|███       | 151/500 [02:20<07:56,  1.37s/it] 31%|███       | 153/500 [02:20<05:39,  1.02it/s] 31%|███       | 155/500 [02:23<06:51,  1.19s/it] 31%|███▏      | 157/500 [02:23<04:52,  1.17it/s] 32%|███▏      | 159/500 [02:23<03:30,  1.62it/s] 32%|███▏      | 161/500 [02:30<07:42,  1.36s/it] 33%|███▎      | 163/500 [02:30<05:28,  1.02it/s] 33%|███▎      | 165/500 [02:33<06:25,  1.15s/it] 33%|███▎      | 167/500 [02:33<04:35,  1.21it/s] 34%|███▍      | 169/500 [02:33<03:17,  1.67it/s] 34%|███▍      | 171/500 [02:39<07:15,  1.32s/it] 35%|███▍      | 173/500 [02:39<05:09,  1.06it/s] 35%|███▌      | 175/500 [02:42<06:04,  1.12s/it] 35%|███▌      | 177/500 [02:43<04:19,  1.24it/s] 36%|███▌      | 179/500 [02:43<03:06,  1.72it/s] 36%|███▌      | 181/500 [02:49<07:07,  1.34s/it] 37%|███▋      | 183/500 [02:49<05:03,  1.04it/s]0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  126  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  127  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  128  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  129  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  130  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  131  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  132  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  133  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  134  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  135  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  136  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  137  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  138  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  139  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  140  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  141  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  142  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  143  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  144  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014091491699
Valid Loss:  0.5123908519744873
Epoch:  145  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  146  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  147  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  148  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  149  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  150  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  151  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  152  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  153  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  154  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123908519744873
Epoch:  155  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  156  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  157  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  158  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
Epoch:  159  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  160  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  161  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  162  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  163  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  164  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  165  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  166  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123908519744873
Epoch:  167  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  168  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  169  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  170  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  171  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  172  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  173  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  174  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  175  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  176  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  177  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  178  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  179  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  180  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  181  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  182  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  183  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  184  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
Epoch:  185  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
 37%|███▋      | 185/500 [02:52<06:04,  1.16s/it] 37%|███▋      | 187/500 [02:52<04:20,  1.20it/s] 38%|███▊      | 189/500 [02:53<03:07,  1.66it/s] 38%|███▊      | 191/500 [02:59<06:55,  1.34s/it] 39%|███▊      | 193/500 [02:59<04:55,  1.04it/s] 39%|███▉      | 195/500 [03:02<05:50,  1.15s/it] 39%|███▉      | 197/500 [03:02<04:09,  1.21it/s] 40%|███▉      | 199/500 [03:02<02:59,  1.68it/s] 40%|████      | 201/500 [03:08<06:30,  1.31s/it] 41%|████      | 203/500 [03:08<04:37,  1.07it/s] 41%|████      | 205/500 [03:11<05:32,  1.13s/it] 41%|████▏     | 207/500 [03:12<03:57,  1.24it/s] 42%|████▏     | 209/500 [03:12<02:50,  1.71it/s] 42%|████▏     | 211/500 [03:18<06:14,  1.29s/it] 43%|████▎     | 213/500 [03:18<04:26,  1.08it/s] 43%|████▎     | 215/500 [03:21<05:14,  1.11s/it] 43%|████▎     | 217/500 [03:21<03:44,  1.26it/s] 44%|████▍     | 219/500 [03:21<02:41,  1.74it/s] 44%|████▍     | 221/500 [03:27<06:17,  1.35s/it] 45%|████▍     | 223/500 [03:28<04:27,  1.03it/s] 45%|████▌     | 225/500 [03:31<05:15,  1.15s/it] 45%|████▌     | 227/500 [03:31<03:45,  1.21it/s] 46%|████▌     | 229/500 [03:31<02:41,  1.68it/s] 46%|████▌     | 231/500 [03:37<05:51,  1.31s/it] 47%|████▋     | 233/500 [03:37<04:09,  1.07it/s] 47%|████▋     | 235/500 [03:40<05:00,  1.14s/it] 47%|████▋     | 237/500 [03:40<03:34,  1.23it/s] 48%|████▊     | 239/500 [03:40<02:33,  1.70it/s] 48%|████▊     | 241/500 [03:47<05:46,  1.34s/it] 49%|████▊     | 243/500 [03:47<04:05,  1.05it/s]**************************************************learning rate decay**************************************************
Epoch:  186  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  187  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  188  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  189  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  190  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  191  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  192  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  193  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  194  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  195  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  196  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  197  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  198  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  199  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  200  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  201  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  202  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  203  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  204  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  205  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909711837769
**************************************************learning rate decay**************************************************
Epoch:  206  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  207  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  208  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  209  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  210  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  211  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  212  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  213  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
Epoch:  214  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  215  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  216  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  217  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  218  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  219  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123908519744873
Epoch:  220  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  221  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  222  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  223  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  224  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  225  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  226  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
Epoch:  227  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  228  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  229  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  230  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  231  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  232  	Training Loss: 0.3829297125339508
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  233  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  234  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  235  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  236  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  237  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  238  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  239  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  240  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  241  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  242  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  243  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  244  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  245  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
 49%|████▉     | 245/500 [03:50<04:47,  1.13s/it] 49%|████▉     | 247/500 [03:50<03:24,  1.24it/s] 50%|████▉     | 249/500 [03:50<02:26,  1.71it/s] 50%|█████     | 251/500 [03:56<05:24,  1.30s/it] 51%|█████     | 253/500 [03:56<03:50,  1.07it/s] 51%|█████     | 255/500 [03:59<04:30,  1.10s/it] 51%|█████▏    | 257/500 [03:59<03:12,  1.26it/s] 52%|█████▏    | 259/500 [03:59<02:18,  1.74it/s] 52%|█████▏    | 261/500 [04:06<05:13,  1.31s/it] 53%|█████▎    | 263/500 [04:06<03:42,  1.07it/s] 53%|█████▎    | 265/500 [04:09<04:24,  1.13s/it] 53%|█████▎    | 267/500 [04:09<03:08,  1.24it/s] 54%|█████▍    | 269/500 [04:09<02:15,  1.71it/s] 54%|█████▍    | 271/500 [04:15<05:03,  1.33s/it] 55%|█████▍    | 273/500 [04:15<03:35,  1.05it/s] 55%|█████▌    | 275/500 [04:15<02:33,  1.46it/s] 55%|█████▌    | 277/500 [04:16<01:51,  2.01it/s] 56%|█████▌    | 279/500 [04:16<01:21,  2.72it/s] 56%|█████▌    | 281/500 [04:22<04:12,  1.15s/it] 57%|█████▋    | 283/500 [04:22<02:59,  1.21it/s] 57%|█████▋    | 285/500 [04:25<03:44,  1.04s/it] 57%|█████▋    | 287/500 [04:25<02:39,  1.33it/s] 58%|█████▊    | 289/500 [04:25<01:54,  1.84it/s] 58%|█████▊    | 291/500 [04:31<04:32,  1.31s/it] 59%|█████▊    | 293/500 [04:31<03:13,  1.07it/s] 59%|█████▉    | 295/500 [04:34<03:47,  1.11s/it] 59%|█████▉    | 297/500 [04:35<02:41,  1.25it/s] 60%|█████▉    | 299/500 [04:35<01:56,  1.73it/s] 60%|██████    | 301/500 [04:41<04:17,  1.30s/it] 61%|██████    | 303/500 [04:41<03:02,  1.08it/s] 61%|██████    | 305/500 [04:44<03:36,  1.11s/it]**************************************************learning rate decay**************************************************
Epoch:  246  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  247  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  248  	Training Loss: 0.3829296827316284
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  249  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014091491699
Valid Loss:  0.5123908519744873
Epoch:  250  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  251  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  252  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  253  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  254  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  255  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  256  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  257  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  258  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  259  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
Epoch:  260  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  261  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  262  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  263  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  264  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  265  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  266  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  267  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  268  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  269  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  270  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  271  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  272  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
Epoch:  273  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  274  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123908519744873
Epoch:  275  	Training Loss: 0.3829297125339508
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  276  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  277  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  278  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  279  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  280  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  281  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  282  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  283  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123908519744873
Epoch:  284  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  285  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  286  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  287  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  288  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  289  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  290  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  291  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  292  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  293  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  294  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  295  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  296  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  297  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  298  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  299  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  300  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014091491699
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  301  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  302  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  303  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  304  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909711837769
Epoch:  305  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  306  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:   61%|██████▏   | 307/500 [04:44<02:33,  1.25it/s] 62%|██████▏   | 309/500 [04:44<01:50,  1.73it/s] 62%|██████▏   | 311/500 [04:50<04:04,  1.30s/it] 63%|██████▎   | 313/500 [04:50<02:53,  1.08it/s] 63%|██████▎   | 315/500 [04:53<03:25,  1.11s/it] 63%|██████▎   | 317/500 [04:53<02:25,  1.26it/s] 64%|██████▍   | 319/500 [04:54<01:44,  1.73it/s] 64%|██████▍   | 321/500 [04:59<03:51,  1.29s/it] 65%|██████▍   | 323/500 [05:00<02:44,  1.08it/s] 65%|██████▌   | 325/500 [05:03<03:13,  1.11s/it] 65%|██████▌   | 327/500 [05:03<02:17,  1.26it/s] 66%|██████▌   | 329/500 [05:03<01:38,  1.74it/s] 66%|██████▌   | 331/500 [05:09<03:38,  1.29s/it] 67%|██████▋   | 333/500 [05:09<02:34,  1.08it/s] 67%|██████▋   | 335/500 [05:12<03:02,  1.11s/it] 67%|██████▋   | 337/500 [05:12<02:09,  1.25it/s] 68%|██████▊   | 339/500 [05:12<01:32,  1.73it/s] 68%|██████▊   | 341/500 [05:18<03:27,  1.31s/it] 69%|██████▊   | 343/500 [05:19<02:27,  1.07it/s] 69%|██████▉   | 345/500 [05:22<02:52,  1.11s/it] 69%|██████▉   | 347/500 [05:22<02:02,  1.25it/s] 70%|██████▉   | 349/500 [05:22<01:27,  1.73it/s] 70%|███████   | 351/500 [05:28<03:13,  1.30s/it] 71%|███████   | 353/500 [05:28<02:16,  1.08it/s] 71%|███████   | 355/500 [05:31<02:40,  1.11s/it] 71%|███████▏  | 357/500 [05:31<01:53,  1.26it/s] 72%|███████▏  | 359/500 [05:31<01:21,  1.74it/s] 72%|███████▏  | 361/500 [05:37<03:00,  1.30s/it] 73%|███████▎  | 363/500 [05:37<02:07,  1.08it/s] 73%|███████▎  | 365/500 [05:40<02:29,  1.11s/it]0.5123909115791321
Epoch:  307  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  308  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  309  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  310  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  311  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  312  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  313  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  314  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  315  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  316  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  317  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  318  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  319  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  320  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  321  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  322  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  323  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  324  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  325  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  326  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  327  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  328  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  329  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  330  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  331  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  332  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  333  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  334  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  335  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  336  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  337  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  338  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  339  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  340  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  341  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  342  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  343  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  344  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  345  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  346  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  347  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  348  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  349  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  350  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  351  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  352  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  353  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  354  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  355  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  356  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  357  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  358  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
Epoch:  359  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  360  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  361  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  362  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  363  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  364  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  365  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  366  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  367  	Training Loss: 0.3829297125339508
Test Loss:   73%|███████▎  | 367/500 [05:41<01:46,  1.25it/s] 74%|███████▍  | 369/500 [05:41<01:15,  1.73it/s] 74%|███████▍  | 371/500 [05:47<02:49,  1.31s/it] 75%|███████▍  | 373/500 [05:47<01:59,  1.06it/s] 75%|███████▌  | 375/500 [05:50<02:19,  1.12s/it] 75%|███████▌  | 377/500 [05:50<01:39,  1.24it/s] 76%|███████▌  | 379/500 [05:50<01:10,  1.72it/s] 76%|███████▌  | 381/500 [05:56<02:34,  1.30s/it] 77%|███████▋  | 383/500 [05:56<01:48,  1.08it/s] 77%|███████▋  | 385/500 [05:59<02:07,  1.11s/it] 77%|███████▋  | 387/500 [05:59<01:30,  1.25it/s] 78%|███████▊  | 389/500 [06:00<01:04,  1.73it/s] 78%|███████▊  | 391/500 [06:06<02:25,  1.33s/it] 79%|███████▊  | 393/500 [06:06<01:41,  1.05it/s] 79%|███████▉  | 395/500 [06:09<01:58,  1.13s/it] 79%|███████▉  | 397/500 [06:09<01:23,  1.23it/s] 80%|███████▉  | 399/500 [06:09<00:59,  1.70it/s] 80%|████████  | 401/500 [06:15<02:09,  1.30s/it] 81%|████████  | 403/500 [06:15<01:30,  1.07it/s] 81%|████████  | 405/500 [06:18<01:45,  1.11s/it] 81%|████████▏ | 407/500 [06:19<01:13,  1.26it/s] 82%|████████▏ | 409/500 [06:19<00:52,  1.74it/s] 82%|████████▏ | 411/500 [06:25<01:55,  1.30s/it] 83%|████████▎ | 413/500 [06:25<01:20,  1.08it/s] 83%|████████▎ | 415/500 [06:28<01:34,  1.11s/it] 83%|████████▎ | 417/500 [06:28<01:06,  1.26it/s] 84%|████████▍ | 419/500 [06:28<00:46,  1.73it/s] 84%|████████▍ | 421/500 [06:34<01:41,  1.29s/it] 85%|████████▍ | 423/500 [06:34<01:11,  1.08it/s] 85%|████████▌ | 425/500 [06:37<01:22,  1.10s/it] 85%|████████▌ | 427/500 [06:37<00:57,  1.27it/s]0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  368  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  369  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  370  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  371  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  372  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  373  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  374  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  375  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  376  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  377  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  378  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  379  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  380  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  381  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
Epoch:  382  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  383  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  384  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  385  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  386  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  387  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  388  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  389  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  390  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  391  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  392  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  393  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  394  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  395  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  396  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  397  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  398  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  399  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  400  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  401  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  402  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  403  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  404  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  405  	Training Loss: 0.3829297125339508
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  406  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  407  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  408  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  409  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  410  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  411  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  412  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  413  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  414  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  415  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  416  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  417  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  418  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  419  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  420  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  421  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  422  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  423  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  424  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  425  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  426  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  427  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
 86%|████████▌ | 429/500 [06:37<00:40,  1.76it/s] 86%|████████▌ | 431/500 [06:43<01:29,  1.29s/it] 87%|████████▋ | 433/500 [06:43<01:01,  1.08it/s] 87%|████████▋ | 435/500 [06:47<01:11,  1.11s/it] 87%|████████▋ | 437/500 [06:47<00:50,  1.26it/s] 88%|████████▊ | 439/500 [06:47<00:35,  1.73it/s] 88%|████████▊ | 441/500 [06:53<01:16,  1.29s/it] 89%|████████▊ | 443/500 [06:53<00:52,  1.08it/s] 89%|████████▉ | 445/500 [06:56<01:00,  1.10s/it] 89%|████████▉ | 447/500 [06:56<00:41,  1.26it/s] 90%|████████▉ | 449/500 [06:56<00:29,  1.75it/s] 90%|█████████ | 451/500 [07:02<01:03,  1.30s/it] 91%|█████████ | 453/500 [07:02<00:43,  1.08it/s] 91%|█████████ | 455/500 [07:05<00:50,  1.11s/it] 91%|█████████▏| 457/500 [07:05<00:34,  1.25it/s] 92%|█████████▏| 459/500 [07:06<00:23,  1.73it/s] 92%|█████████▏| 461/500 [07:12<00:50,  1.29s/it] 93%|█████████▎| 463/500 [07:12<00:34,  1.08it/s] 93%|█████████▎| 465/500 [07:15<00:38,  1.10s/it] 93%|█████████▎| 467/500 [07:15<00:26,  1.26it/s] 94%|█████████▍| 469/500 [07:15<00:17,  1.74it/s] 94%|█████████▍| 471/500 [07:21<00:37,  1.30s/it] 95%|█████████▍| 473/500 [07:21<00:25,  1.08it/s] 95%|█████████▌| 475/500 [07:24<00:27,  1.11s/it] 95%|█████████▌| 477/500 [07:24<00:18,  1.26it/s] 96%|█████████▌| 479/500 [07:24<00:12,  1.74it/s] 96%|█████████▌| 481/500 [07:30<00:24,  1.30s/it] 97%|█████████▋| 483/500 [07:30<00:15,  1.08it/s] 97%|█████████▋| 485/500 [07:34<00:16,  1.11s/it] 97%|█████████▋| 487/500 [07:34<00:10,  1.26it/s]Epoch:  428  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  429  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  430  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  431  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  432  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  433  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
Epoch:  434  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  435  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  436  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  437  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
Epoch:  438  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  439  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  440  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  441  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  442  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  443  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  444  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  445  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  446  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
Epoch:  447  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  448  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  449  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  450  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  451  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  452  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  453  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
Epoch:  454  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  455  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  456  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  457  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  458  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  459  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  460  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  461  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  462  	Training Loss: 0.3829297125339508
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  463  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  464  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  465  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  466  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  467  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014091491699
Valid Loss:  0.5123908519744873
Epoch:  468  	Training Loss: 0.3829297125339508
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  469  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  470  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014091491699
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  471  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  472  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  473  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  474  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  475  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  476  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  477  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  478  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  479  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  480  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  481  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  482  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  483  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  484  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  485  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  486  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123909115791321
Epoch:  487  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909711837769
Epoch:  488  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:   98%|█████████▊| 489/500 [07:34<00:06,  1.74it/s] 98%|█████████▊| 491/500 [07:40<00:11,  1.30s/it] 99%|█████████▊| 493/500 [07:40<00:06,  1.07it/s] 99%|█████████▉| 495/500 [07:43<00:05,  1.12s/it] 99%|█████████▉| 497/500 [07:43<00:02,  1.24it/s]100%|█████████▉| 499/500 [07:43<00:00,  1.71it/s]100%|██████████| 500/500 [07:46<00:00,  1.07it/s]
0.5123909711837769
Epoch:  489  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909711837769
Epoch:  490  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
Epoch:  491  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  492  	Training Loss: 0.3829297423362732
Test Loss:  0.5049015283584595
Valid Loss:  0.5123908519744873
Epoch:  493  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  494  	Training Loss: 0.3829297125339508
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  495  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
**************************************************learning rate decay**************************************************
Epoch:  496  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123908519744873
Epoch:  497  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  498  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  499  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
Epoch:  500  	Training Loss: 0.3829297423362732
Test Loss:  0.5049014687538147
Valid Loss:  0.5123909115791321
**************************************************learning rate decay**************************************************
seed is  17
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:02<24:53,  2.99s/it]  1%|          | 3/500 [00:03<06:49,  1.21it/s]  1%|          | 5/500 [00:03<03:34,  2.31it/s]  1%|▏         | 7/500 [00:03<02:16,  3.62it/s]  2%|▏         | 9/500 [00:03<01:36,  5.10it/s]  2%|▏         | 11/500 [00:06<05:23,  1.51it/s]  3%|▎         | 13/500 [00:06<03:45,  2.16it/s]  3%|▎         | 15/500 [00:09<06:30,  1.24it/s]  3%|▎         | 17/500 [00:09<04:36,  1.75it/s]  4%|▍         | 19/500 [00:09<03:18,  2.42it/s]  4%|▍         | 21/500 [00:15<09:37,  1.21s/it]  5%|▍         | 23/500 [00:16<06:48,  1.17it/s]  5%|▌         | 25/500 [00:19<08:22,  1.06s/it]  5%|▌         | 27/500 [00:19<05:59,  1.32it/s]  6%|▌         | 29/500 [00:19<04:18,  1.82it/s]  6%|▌         | 31/500 [00:25<10:03,  1.29s/it]  7%|▋         | 33/500 [00:25<07:10,  1.09it/s]  7%|▋         | 35/500 [00:28<08:29,  1.09s/it]  7%|▋         | 37/500 [00:28<06:03,  1.27it/s]  8%|▊         | 39/500 [00:28<04:22,  1.76it/s]  8%|▊         | 41/500 [00:34<09:55,  1.30s/it]  9%|▊         | 43/500 [00:34<07:03,  1.08it/s]  9%|▉         | 45/500 [00:37<08:21,  1.10s/it]  9%|▉         | 47/500 [00:38<05:59,  1.26it/s] 10%|▉         | 49/500 [00:38<04:19,  1.74it/s] 10%|█         | 51/500 [00:44<09:46,  1.31s/it] 11%|█         | 53/500 [00:44<06:57,  1.07it/s] 11%|█         | 55/500 [00:47<08:12,  1.11s/it] 11%|█▏        | 57/500 [00:47<05:51,  1.26it/s] 12%|█▏        | 59/500 [00:47<04:13,  1.74it/s] 12%|█▏        | 61/500 [00:53<09:30,  1.30s/it]Epoch:  1  	Training Loss: 0.07160218060016632
Test Loss:  147.82913208007812
Valid Loss:  143.43048095703125
Epoch:  2  	Training Loss: 139.6652374267578
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  3  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  4  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  5  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  6  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  7  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  8  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  9  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  10  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  11  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  12  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  13  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  14  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  15  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  16  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  17  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  18  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  19  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  20  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
**************************************************learning rate decay**************************************************
Epoch:  21  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  22  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
Epoch:  23  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  24  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  25  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  26  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  27  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  28  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  29  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  30  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
**************************************************learning rate decay**************************************************
Epoch:  31  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  32  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  33  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  34  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  35  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
**************************************************learning rate decay**************************************************
Epoch:  36  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  37  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  38  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  39  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  40  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  41  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  42  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  43  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  44  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  45  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
**************************************************learning rate decay**************************************************
Epoch:  46  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  47  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  48  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  49  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  50  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  51  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  52  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  53  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  54  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  55  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  56  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  57  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  58  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  59  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  60  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  61  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
Epoch:  62  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  63  	Training Loss: 0.08354227244853973
Test Loss:   13%|█▎        | 63/500 [00:53<06:45,  1.08it/s] 13%|█▎        | 65/500 [00:56<08:02,  1.11s/it] 13%|█▎        | 67/500 [00:56<05:44,  1.26it/s] 14%|█▍        | 69/500 [00:57<04:08,  1.73it/s] 14%|█▍        | 71/500 [01:03<09:17,  1.30s/it] 15%|█▍        | 73/500 [01:03<06:37,  1.07it/s] 15%|█▌        | 75/500 [01:06<07:49,  1.10s/it] 15%|█▌        | 77/500 [01:06<05:34,  1.26it/s] 16%|█▌        | 79/500 [01:06<04:01,  1.74it/s] 16%|█▌        | 81/500 [01:12<09:07,  1.31s/it] 17%|█▋        | 83/500 [01:12<06:29,  1.07it/s] 17%|█▋        | 85/500 [01:15<07:40,  1.11s/it] 17%|█▋        | 87/500 [01:15<05:29,  1.25it/s] 18%|█▊        | 89/500 [01:15<03:57,  1.73it/s] 18%|█▊        | 91/500 [01:21<08:51,  1.30s/it] 19%|█▊        | 93/500 [01:22<06:18,  1.08it/s] 19%|█▉        | 95/500 [01:25<07:27,  1.10s/it] 19%|█▉        | 97/500 [01:25<05:19,  1.26it/s] 20%|█▉        | 99/500 [01:25<03:50,  1.74it/s] 20%|██        | 101/500 [01:31<08:41,  1.31s/it] 21%|██        | 103/500 [01:31<06:10,  1.07it/s] 21%|██        | 105/500 [01:34<07:19,  1.11s/it] 21%|██▏       | 107/500 [01:34<05:14,  1.25it/s] 22%|██▏       | 109/500 [01:34<03:46,  1.73it/s] 22%|██▏       | 111/500 [01:40<08:27,  1.30s/it] 23%|██▎       | 113/500 [01:40<06:01,  1.07it/s] 23%|██▎       | 115/500 [01:43<07:08,  1.11s/it] 23%|██▎       | 117/500 [01:44<05:05,  1.25it/s] 24%|██▍       | 119/500 [01:44<03:40,  1.73it/s] 24%|██▍       | 121/500 [01:50<08:22,  1.33s/it]0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  64  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  65  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  66  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  67  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  68  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  69  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  70  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  71  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  72  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  73  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
Epoch:  74  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
Epoch:  75  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  76  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  77  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  78  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  79  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  80  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  81  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  82  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
Epoch:  83  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  84  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
Epoch:  85  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  86  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
Epoch:  87  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  88  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  89  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  90  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  91  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  92  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  93  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  94  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  95  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  96  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  97  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  98  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  99  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
Epoch:  100  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  101  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  102  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  103  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  104  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  105  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  106  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  107  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  108  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  109  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  110  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  111  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  112  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  113  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  114  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  115  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
**************************************************learning rate decay**************************************************
Epoch:  116  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  117  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  118  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  119  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  120  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  121  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  122  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  123  	Training Loss: 0.08354227244853973
Test Loss:   25%|██▍       | 123/500 [01:50<05:57,  1.05it/s] 25%|██▌       | 125/500 [01:53<06:58,  1.12s/it] 25%|██▌       | 127/500 [01:53<04:59,  1.25it/s] 26%|██▌       | 129/500 [01:53<03:35,  1.72it/s] 26%|██▌       | 131/500 [01:59<07:56,  1.29s/it] 27%|██▋       | 133/500 [01:59<05:38,  1.08it/s] 27%|██▋       | 135/500 [02:02<06:44,  1.11s/it] 27%|██▋       | 137/500 [02:03<04:48,  1.26it/s] 28%|██▊       | 139/500 [02:03<03:27,  1.74it/s] 28%|██▊       | 141/500 [02:09<07:47,  1.30s/it] 29%|██▊       | 143/500 [02:09<05:33,  1.07it/s] 29%|██▉       | 145/500 [02:12<06:43,  1.14s/it] 29%|██▉       | 147/500 [02:12<04:48,  1.22it/s] 30%|██▉       | 149/500 [02:12<03:28,  1.69it/s] 30%|███       | 151/500 [02:19<07:53,  1.36s/it] 31%|███       | 153/500 [02:19<05:36,  1.03it/s] 31%|███       | 155/500 [02:22<06:38,  1.16s/it] 31%|███▏      | 157/500 [02:22<04:43,  1.21it/s] 32%|███▏      | 159/500 [02:22<03:24,  1.67it/s] 32%|███▏      | 161/500 [02:28<07:23,  1.31s/it] 33%|███▎      | 163/500 [02:28<05:15,  1.07it/s] 33%|███▎      | 165/500 [02:31<06:14,  1.12s/it] 33%|███▎      | 167/500 [02:31<04:27,  1.25it/s] 34%|███▍      | 169/500 [02:32<03:12,  1.72it/s] 34%|███▍      | 171/500 [02:37<07:05,  1.29s/it] 35%|███▍      | 173/500 [02:38<05:03,  1.08it/s] 35%|███▌      | 175/500 [02:41<05:59,  1.11s/it] 35%|███▌      | 177/500 [02:41<04:16,  1.26it/s] 36%|███▌      | 179/500 [02:41<03:04,  1.74it/s] 36%|███▌      | 181/500 [02:47<06:51,  1.29s/it]0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  124  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  125  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  126  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  127  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  128  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  129  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  130  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  131  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  132  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  133  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  134  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  135  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  136  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  137  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  138  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  139  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  140  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  141  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  142  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  143  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  144  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  145  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  146  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  147  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  148  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  149  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  150  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  151  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  152  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  153  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  154  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  155  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  156  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  157  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  158  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  159  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  160  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  161  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  162  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  163  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  164  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  165  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
**************************************************learning rate decay**************************************************
Epoch:  166  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  167  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  168  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
Epoch:  169  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  170  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  171  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  172  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  173  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  174  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  175  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  176  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  177  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  178  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  179  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  180  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  181  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  182  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
 37%|███▋      | 183/500 [02:47<04:52,  1.08it/s] 37%|███▋      | 185/500 [02:50<05:46,  1.10s/it] 37%|███▋      | 187/500 [02:50<04:07,  1.26it/s] 38%|███▊      | 189/500 [02:50<02:58,  1.74it/s] 38%|███▊      | 191/500 [02:56<06:41,  1.30s/it] 39%|███▊      | 193/500 [02:56<04:45,  1.07it/s] 39%|███▉      | 195/500 [02:59<05:36,  1.10s/it] 39%|███▉      | 197/500 [03:00<03:59,  1.26it/s] 40%|███▉      | 199/500 [03:00<02:52,  1.75it/s] 40%|████      | 201/500 [03:06<06:25,  1.29s/it] 41%|████      | 203/500 [03:06<04:33,  1.08it/s] 41%|████      | 205/500 [03:09<05:25,  1.10s/it] 41%|████▏     | 207/500 [03:09<03:52,  1.26it/s] 42%|████▏     | 209/500 [03:09<02:47,  1.74it/s] 42%|████▏     | 211/500 [03:15<06:14,  1.30s/it] 43%|████▎     | 213/500 [03:15<04:26,  1.08it/s] 43%|████▎     | 215/500 [03:18<05:14,  1.10s/it] 43%|████▎     | 217/500 [03:18<03:43,  1.26it/s] 44%|████▍     | 219/500 [03:18<02:41,  1.75it/s] 44%|████▍     | 221/500 [03:24<06:06,  1.31s/it] 45%|████▍     | 223/500 [03:25<04:19,  1.07it/s] 45%|████▌     | 225/500 [03:28<05:05,  1.11s/it] 45%|████▌     | 227/500 [03:28<03:37,  1.25it/s] 46%|████▌     | 229/500 [03:28<02:36,  1.73it/s] 46%|████▌     | 231/500 [03:34<05:47,  1.29s/it] 47%|████▋     | 233/500 [03:34<04:07,  1.08it/s] 47%|████▋     | 235/500 [03:37<04:55,  1.11s/it] 47%|████▋     | 237/500 [03:37<03:30,  1.25it/s] 48%|████▊     | 239/500 [03:37<02:31,  1.73it/s] 48%|████▊     | 241/500 [03:43<05:41,  1.32s/it]Epoch:  183  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  184  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  185  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  186  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  187  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  188  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  189  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  190  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  191  	Training Loss: 0.08354227244853973
Test Loss:  0.11438216269016266
Valid Loss:  0.1164434552192688
Epoch:  192  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  193  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  194  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  195  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  196  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  197  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
Epoch:  198  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  199  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
Epoch:  200  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
**************************************************learning rate decay**************************************************
Epoch:  201  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  202  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  203  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  204  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
Epoch:  205  	Training Loss: 0.08354227244853973
Test Loss:  0.11438216269016266
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  206  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  207  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  208  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  209  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  210  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  211  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  212  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  213  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  214  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  215  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  216  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  217  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  218  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  219  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  220  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  221  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  222  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  223  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  224  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  225  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  226  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  227  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  228  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  229  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  230  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  231  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  232  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  233  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  234  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  235  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
**************************************************learning rate decay**************************************************
Epoch:  236  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  237  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  238  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  239  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  240  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  241  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  242  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
 49%|████▊     | 243/500 [03:44<04:02,  1.06it/s] 49%|████▉     | 245/500 [03:47<04:43,  1.11s/it] 49%|████▉     | 247/500 [03:47<03:22,  1.25it/s] 50%|████▉     | 249/500 [03:47<02:25,  1.73it/s] 50%|█████     | 251/500 [03:53<05:22,  1.29s/it] 51%|█████     | 253/500 [03:53<03:49,  1.08it/s] 51%|█████     | 255/500 [03:56<04:31,  1.11s/it] 51%|█████▏    | 257/500 [03:56<03:13,  1.25it/s] 52%|█████▏    | 259/500 [03:56<02:19,  1.73it/s] 52%|█████▏    | 261/500 [04:02<05:14,  1.32s/it] 53%|█████▎    | 263/500 [04:02<03:43,  1.06it/s] 53%|█████▎    | 265/500 [04:06<04:22,  1.12s/it] 53%|█████▎    | 267/500 [04:06<03:07,  1.24it/s] 54%|█████▍    | 269/500 [04:06<02:14,  1.72it/s] 54%|█████▍    | 271/500 [04:12<05:01,  1.32s/it] 55%|█████▍    | 273/500 [04:12<03:33,  1.06it/s] 55%|█████▌    | 275/500 [04:15<04:09,  1.11s/it] 55%|█████▌    | 277/500 [04:15<02:57,  1.26it/s] 56%|█████▌    | 279/500 [04:15<02:07,  1.74it/s] 56%|█████▌    | 281/500 [04:21<04:43,  1.29s/it] 57%|█████▋    | 283/500 [04:21<03:20,  1.08it/s] 57%|█████▋    | 285/500 [04:24<03:56,  1.10s/it] 57%|█████▋    | 287/500 [04:24<02:48,  1.26it/s] 58%|█████▊    | 289/500 [04:25<02:00,  1.75it/s] 58%|█████▊    | 291/500 [04:31<04:34,  1.31s/it] 59%|█████▊    | 293/500 [04:31<03:15,  1.06it/s] 59%|█████▉    | 295/500 [04:34<04:04,  1.19s/it] 59%|█████▉    | 297/500 [04:35<02:53,  1.17it/s] 60%|█████▉    | 299/500 [04:35<02:04,  1.62it/s] 60%|██████    | 301/500 [04:41<04:26,  1.34s/it]Valid Loss:  0.1164434403181076
Epoch:  243  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  244  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  245  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  246  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  247  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  248  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  249  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  250  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  251  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  252  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  253  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  254  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  255  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  256  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
Epoch:  257  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  258  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  259  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  260  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  261  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  262  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  263  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  264  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
Epoch:  265  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  266  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  267  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  268  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  269  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  270  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  271  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  272  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  273  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
Epoch:  274  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  275  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  276  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  277  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  278  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  279  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  280  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  281  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  282  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  283  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  284  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  285  	Training Loss: 0.08354227989912033
Test Loss:  0.11438216269016266
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  286  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  287  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  288  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  289  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  290  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
**************************************************learning rate decay**************************************************
Epoch:  291  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  292  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
Epoch:  293  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  294  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  295  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  296  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  297  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  298  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  299  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  300  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  301  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  302  	Training Loss: 0.08354227244853973
 61%|██████    | 303/500 [04:41<03:08,  1.04it/s] 61%|██████    | 305/500 [04:44<03:47,  1.17s/it] 61%|██████▏   | 307/500 [04:44<02:41,  1.20it/s] 62%|██████▏   | 309/500 [04:44<01:55,  1.65it/s] 62%|██████▏   | 311/500 [04:50<04:11,  1.33s/it] 63%|██████▎   | 313/500 [04:51<02:57,  1.05it/s] 63%|██████▎   | 315/500 [04:54<03:28,  1.12s/it] 63%|██████▎   | 317/500 [04:54<02:27,  1.24it/s] 64%|██████▍   | 319/500 [04:54<01:45,  1.71it/s] 64%|██████▍   | 321/500 [05:00<03:54,  1.31s/it] 65%|██████▍   | 323/500 [05:00<02:45,  1.07it/s] 65%|██████▌   | 325/500 [05:03<03:14,  1.11s/it] 65%|██████▌   | 327/500 [05:03<02:18,  1.25it/s] 66%|██████▌   | 329/500 [05:03<01:38,  1.73it/s] 66%|██████▌   | 331/500 [05:10<03:46,  1.34s/it] 67%|██████▋   | 333/500 [05:10<02:39,  1.04it/s] 67%|██████▋   | 335/500 [05:13<03:05,  1.13s/it] 67%|██████▋   | 337/500 [05:13<02:11,  1.24it/s] 68%|██████▊   | 339/500 [05:13<01:34,  1.71it/s] 68%|██████▊   | 341/500 [05:19<03:27,  1.30s/it] 69%|██████▊   | 343/500 [05:19<02:26,  1.07it/s] 69%|██████▉   | 345/500 [05:22<02:52,  1.11s/it] 69%|██████▉   | 347/500 [05:22<02:02,  1.25it/s] 70%|██████▉   | 349/500 [05:22<01:27,  1.73it/s] 70%|███████   | 351/500 [05:28<03:13,  1.30s/it] 71%|███████   | 353/500 [05:29<02:16,  1.08it/s] 71%|███████   | 355/500 [05:32<02:40,  1.11s/it] 71%|███████▏  | 357/500 [05:32<01:54,  1.25it/s] 72%|███████▏  | 359/500 [05:32<01:21,  1.73it/s] 72%|███████▏  | 361/500 [05:38<02:59,  1.29s/it]Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  303  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  304  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  305  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  306  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  307  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  308  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  309  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  310  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  311  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  312  	Training Loss: 0.08354228734970093
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  313  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  314  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  315  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
**************************************************learning rate decay**************************************************
Epoch:  316  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
Epoch:  317  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  318  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  319  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  320  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  321  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  322  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  323  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  324  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  325  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  326  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  327  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  328  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  329  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  330  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  331  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  332  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  333  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  334  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  335  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  336  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  337  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  338  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  339  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  340  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
**************************************************learning rate decay**************************************************
Epoch:  341  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  342  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  343  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  344  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  345  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  346  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  347  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  348  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  349  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  350  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  351  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  352  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  353  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  354  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  355  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  356  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  357  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  358  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  359  	Training Loss: 0.08354227244853973
Test Loss:  0.11438216269016266
Valid Loss:  0.1164434477686882
Epoch:  360  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  361  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
 73%|███████▎  | 363/500 [05:38<02:06,  1.08it/s] 73%|███████▎  | 365/500 [05:41<02:30,  1.12s/it] 73%|███████▎  | 367/500 [05:41<01:46,  1.25it/s] 74%|███████▍  | 369/500 [05:41<01:16,  1.72it/s] 74%|███████▍  | 371/500 [05:47<02:48,  1.30s/it] 75%|███████▍  | 373/500 [05:47<01:58,  1.07it/s] 75%|███████▌  | 375/500 [05:50<02:18,  1.11s/it] 75%|███████▌  | 377/500 [05:51<01:37,  1.26it/s] 76%|███████▌  | 379/500 [05:51<01:09,  1.73it/s] 76%|███████▌  | 381/500 [05:57<02:34,  1.30s/it] 77%|███████▋  | 383/500 [05:57<01:48,  1.08it/s] 77%|███████▋  | 385/500 [06:00<02:07,  1.11s/it] 77%|███████▋  | 387/500 [06:00<01:29,  1.26it/s] 78%|███████▊  | 389/500 [06:00<01:03,  1.74it/s] 78%|███████▊  | 391/500 [06:06<02:22,  1.31s/it] 79%|███████▊  | 393/500 [06:06<01:40,  1.07it/s] 79%|███████▉  | 395/500 [06:09<01:57,  1.11s/it] 79%|███████▉  | 397/500 [06:10<01:22,  1.25it/s] 80%|███████▉  | 399/500 [06:10<00:58,  1.73it/s] 80%|████████  | 401/500 [06:16<02:08,  1.30s/it] 81%|████████  | 403/500 [06:16<01:30,  1.08it/s] 81%|████████  | 405/500 [06:19<01:45,  1.11s/it] 81%|████████▏ | 407/500 [06:19<01:14,  1.25it/s] 82%|████████▏ | 409/500 [06:19<00:52,  1.73it/s] 82%|████████▏ | 411/500 [06:25<01:56,  1.31s/it] 83%|████████▎ | 413/500 [06:25<01:21,  1.07it/s] 83%|████████▎ | 415/500 [06:28<01:34,  1.11s/it] 83%|████████▎ | 417/500 [06:28<01:06,  1.26it/s] 84%|████████▍ | 419/500 [06:28<00:46,  1.74it/s]Epoch:  362  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  363  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  364  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  365  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  366  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  367  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  368  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  369  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  370  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  371  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  372  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  373  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  374  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  375  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  376  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  377  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  378  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  379  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  380  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
**************************************************learning rate decay**************************************************
Epoch:  381  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  382  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  383  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  384  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  385  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  386  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  387  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  388  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  389  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  390  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  391  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  392  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  393  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
Epoch:  394  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  395  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  396  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  397  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  398  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  399  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  400  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  401  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  402  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  403  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  404  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  405  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
**************************************************learning rate decay**************************************************
Epoch:  406  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  407  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  408  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  409  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  410  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  411  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  412  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  413  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  414  	Training Loss: 0.08354227244853973
Test Loss:  0.11438216269016266
Valid Loss:  0.1164434477686882
Epoch:  415  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  416  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  417  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  418  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  419  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  420  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  421  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
 84%|████████▍ | 421/500 [06:34<01:42,  1.30s/it] 85%|████████▍ | 423/500 [06:35<01:11,  1.08it/s] 85%|████████▌ | 425/500 [06:38<01:23,  1.11s/it] 85%|████████▌ | 427/500 [06:38<00:58,  1.26it/s] 86%|████████▌ | 429/500 [06:38<00:40,  1.74it/s] 86%|████████▌ | 431/500 [06:44<01:29,  1.30s/it] 87%|████████▋ | 433/500 [06:44<01:02,  1.07it/s] 87%|████████▋ | 435/500 [06:47<01:13,  1.12s/it] 87%|████████▋ | 437/500 [06:47<00:50,  1.24it/s] 88%|████████▊ | 439/500 [06:47<00:35,  1.71it/s] 88%|████████▊ | 441/500 [06:54<01:19,  1.34s/it] 89%|████████▊ | 443/500 [06:54<00:54,  1.04it/s] 89%|████████▉ | 445/500 [06:57<01:02,  1.14s/it] 89%|████████▉ | 447/500 [06:57<00:43,  1.22it/s] 90%|████████▉ | 449/500 [06:57<00:30,  1.68it/s] 90%|█████████ | 451/500 [07:03<01:03,  1.31s/it] 91%|█████████ | 453/500 [07:03<00:43,  1.07it/s] 91%|█████████ | 455/500 [07:06<00:49,  1.11s/it] 91%|█████████▏| 457/500 [07:06<00:34,  1.25it/s] 92%|█████████▏| 459/500 [07:07<00:23,  1.73it/s] 92%|█████████▏| 461/500 [07:13<00:50,  1.30s/it] 93%|█████████▎| 463/500 [07:13<00:34,  1.07it/s] 93%|█████████▎| 465/500 [07:16<00:38,  1.11s/it] 93%|█████████▎| 467/500 [07:16<00:26,  1.25it/s] 94%|█████████▍| 469/500 [07:16<00:17,  1.73it/s] 94%|█████████▍| 471/500 [07:22<00:37,  1.31s/it] 95%|█████████▍| 473/500 [07:22<00:25,  1.07it/s] 95%|█████████▌| 475/500 [07:25<00:27,  1.11s/it] 95%|█████████▌| 477/500 [07:25<00:18,  1.26it/s] 96%|█████████▌| 479/500 [07:25<00:12,  1.74it/s]Valid Loss:  0.1164434477686882
Epoch:  422  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  423  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  424  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  425  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  426  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  427  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  428  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  429  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  430  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  431  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  432  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  433  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  434  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  435  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  436  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  437  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  438  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  439  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  440  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  441  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  442  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  443  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  444  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  445  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  446  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  447  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  448  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  449  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  450  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  451  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  452  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  453  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  454  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  455  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  456  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  457  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434403181076
Epoch:  458  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  459  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  460  	Training Loss: 0.08354227244853973
Test Loss:  0.11438216269016266
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  461  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  462  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  463  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  464  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  465  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
**************************************************learning rate decay**************************************************
Epoch:  466  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  467  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  468  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  469  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  470  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  471  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  472  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  473  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  474  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  475  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  476  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  477  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  478  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  479  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  480  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  481  	Training Loss: 0.08354227244853973
 96%|█████████▌| 481/500 [07:31<00:24,  1.30s/it] 97%|█████████▋| 483/500 [07:32<00:15,  1.08it/s] 97%|█████████▋| 485/500 [07:35<00:16,  1.11s/it] 97%|█████████▋| 487/500 [07:35<00:10,  1.26it/s] 98%|█████████▊| 489/500 [07:35<00:06,  1.74it/s] 98%|█████████▊| 491/500 [07:41<00:11,  1.29s/it] 99%|█████████▊| 493/500 [07:41<00:06,  1.08it/s] 99%|█████████▉| 495/500 [07:44<00:05,  1.10s/it] 99%|█████████▉| 497/500 [07:44<00:02,  1.27it/s]100%|█████████▉| 499/500 [07:44<00:00,  1.75it/s]100%|██████████| 500/500 [07:47<00:00,  1.07it/s]
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  482  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  483  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  484  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  485  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  486  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  487  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  488  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  489  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  490  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
Epoch:  491  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  492  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434552192688
Epoch:  493  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  494  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  495  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
**************************************************learning rate decay**************************************************
Epoch:  496  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434552192688
Epoch:  497  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434477686882
Epoch:  498  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217759132385
Valid Loss:  0.1164434403181076
Epoch:  499  	Training Loss: 0.08354227989912033
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
Epoch:  500  	Training Loss: 0.08354227244853973
Test Loss:  0.11438217014074326
Valid Loss:  0.1164434477686882
**************************************************learning rate decay**************************************************
seed is  18
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:02<24:31,  2.95s/it]  1%|          | 3/500 [00:03<06:43,  1.23it/s]  1%|          | 5/500 [00:03<03:31,  2.34it/s]  1%|▏         | 7/500 [00:03<02:14,  3.66it/s]  2%|▏         | 9/500 [00:03<01:35,  5.15it/s]  2%|▏         | 11/500 [00:06<05:36,  1.45it/s]  3%|▎         | 13/500 [00:06<03:56,  2.06it/s]  3%|▎         | 15/500 [00:09<06:35,  1.22it/s]  3%|▎         | 17/500 [00:09<04:39,  1.73it/s]  4%|▍         | 19/500 [00:10<03:21,  2.39it/s]  4%|▍         | 21/500 [00:16<09:37,  1.21s/it]  5%|▍         | 23/500 [00:16<06:50,  1.16it/s]  5%|▌         | 25/500 [00:19<08:22,  1.06s/it]  5%|▌         | 27/500 [00:19<05:59,  1.31it/s]  6%|▌         | 29/500 [00:19<04:19,  1.82it/s]  6%|▌         | 31/500 [00:25<10:05,  1.29s/it]  7%|▋         | 33/500 [00:25<07:11,  1.08it/s]  7%|▋         | 35/500 [00:28<08:31,  1.10s/it]  7%|▋         | 37/500 [00:28<06:05,  1.27it/s]  8%|▊         | 39/500 [00:28<04:23,  1.75it/s]  8%|▊         | 41/500 [00:34<09:59,  1.31s/it]  9%|▊         | 43/500 [00:35<07:06,  1.07it/s]  9%|▉         | 45/500 [00:38<08:23,  1.11s/it]  9%|▉         | 47/500 [00:38<06:00,  1.26it/s] 10%|▉         | 49/500 [00:38<04:19,  1.74it/s] 10%|█         | 51/500 [00:44<09:45,  1.30s/it] 11%|█         | 53/500 [00:44<06:57,  1.07it/s] 11%|█         | 55/500 [00:47<08:11,  1.10s/it] 11%|█▏        | 57/500 [00:47<05:50,  1.26it/s] 12%|█▏        | 59/500 [00:47<04:12,  1.74it/s] 12%|█▏        | 61/500 [00:53<09:27,  1.29s/it] 13%|█▎        | 63/500 [00:53<06:44,  1.08it/s]Epoch:  1  	Training Loss: 0.10791479051113129
Test Loss:  91.38058471679688
Valid Loss:  88.57432556152344
Epoch:  2  	Training Loss: 87.87010192871094
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  3  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  4  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  5  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  6  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  7  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813989520073
Epoch:  8  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  9  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  10  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  11  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  12  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
Epoch:  13  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  14  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813989520073
Epoch:  15  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  16  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  17  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  18  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  19  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  20  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  21  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  22  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  23  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  24  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  25  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  26  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  27  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  28  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  29  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  30  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  31  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813840508461
Epoch:  32  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  33  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813840508461
Epoch:  34  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  35  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  36  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813989520073
Epoch:  37  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  38  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  39  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  40  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  41  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813989520073
Epoch:  42  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813989520073
Epoch:  43  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813989520073
Epoch:  44  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  45  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  46  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  47  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  48  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  49  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  50  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  51  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  52  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  53  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  54  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  55  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  56  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  57  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  58  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  59  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  60  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  61  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  62  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  63  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  64  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
 13%|█▎        | 65/500 [00:56<07:59,  1.10s/it] 13%|█▎        | 67/500 [00:57<05:42,  1.26it/s] 14%|█▍        | 69/500 [00:57<04:07,  1.74it/s] 14%|█▍        | 71/500 [01:03<09:14,  1.29s/it] 15%|█▍        | 73/500 [01:03<06:35,  1.08it/s] 15%|█▌        | 75/500 [01:06<07:47,  1.10s/it] 15%|█▌        | 77/500 [01:06<05:33,  1.27it/s] 16%|█▌        | 79/500 [01:06<04:00,  1.75it/s] 16%|█▌        | 81/500 [01:12<09:06,  1.30s/it] 17%|█▋        | 83/500 [01:12<06:28,  1.07it/s] 17%|█▋        | 85/500 [01:15<07:38,  1.10s/it] 17%|█▋        | 87/500 [01:15<05:27,  1.26it/s] 18%|█▊        | 89/500 [01:15<03:56,  1.74it/s] 18%|█▊        | 91/500 [01:21<08:51,  1.30s/it] 19%|█▊        | 93/500 [01:22<06:18,  1.08it/s] 19%|█▉        | 95/500 [01:25<07:25,  1.10s/it] 19%|█▉        | 97/500 [01:25<05:18,  1.27it/s] 20%|█▉        | 99/500 [01:25<03:49,  1.75it/s] 20%|██        | 101/500 [01:31<08:33,  1.29s/it] 21%|██        | 103/500 [01:31<06:05,  1.09it/s] 21%|██        | 105/500 [01:34<07:15,  1.10s/it] 21%|██▏       | 107/500 [01:34<05:11,  1.26it/s] 22%|██▏       | 109/500 [01:34<03:44,  1.74it/s] 22%|██▏       | 111/500 [01:40<08:23,  1.29s/it] 23%|██▎       | 113/500 [01:40<05:58,  1.08it/s] 23%|██▎       | 115/500 [01:43<07:02,  1.10s/it] 23%|██▎       | 117/500 [01:43<05:01,  1.27it/s] 24%|██▍       | 119/500 [01:44<03:37,  1.75it/s] 24%|██▍       | 121/500 [01:50<08:25,  1.33s/it] 25%|██▍       | 123/500 [01:50<06:00,  1.05it/s]Epoch:  65  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  66  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  67  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  68  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  69  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  70  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  71  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
Epoch:  72  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  73  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  74  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  75  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  76  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  77  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  78  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  79  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  80  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  81  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  82  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
Epoch:  83  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  84  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  85  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  86  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  87  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  88  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  89  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  90  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813989520073
**************************************************learning rate decay**************************************************
Epoch:  91  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  92  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  93  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  94  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  95  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  96  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  97  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  98  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  99  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  100  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  101  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  102  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  103  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  104  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  105  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  106  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813989520073
Epoch:  107  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  108  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813840508461
Epoch:  109  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813840508461
Epoch:  110  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  111  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  112  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  113  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  114  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  115  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
**************************************************learning rate decay**************************************************
Epoch:  116  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  117  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  118  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  119  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  120  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  121  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813989520073
Epoch:  122  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  123  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  124  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  125  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
 25%|██▌       | 125/500 [01:53<07:06,  1.14s/it] 25%|██▌       | 127/500 [01:53<05:04,  1.23it/s] 26%|██▌       | 129/500 [01:53<03:39,  1.69it/s] 26%|██▌       | 131/500 [01:59<07:58,  1.30s/it] 27%|██▋       | 133/500 [01:59<05:40,  1.08it/s] 27%|██▋       | 135/500 [02:02<06:43,  1.11s/it] 27%|██▋       | 137/500 [02:02<04:48,  1.26it/s] 28%|██▊       | 139/500 [02:03<03:27,  1.74it/s] 28%|██▊       | 141/500 [02:09<07:42,  1.29s/it] 29%|██▊       | 143/500 [02:09<05:28,  1.09it/s] 29%|██▉       | 145/500 [02:12<06:32,  1.11s/it] 29%|██▉       | 147/500 [02:12<04:40,  1.26it/s] 30%|██▉       | 149/500 [02:12<03:22,  1.74it/s] 30%|███       | 151/500 [02:18<07:31,  1.29s/it] 31%|███       | 153/500 [02:18<05:21,  1.08it/s] 31%|███       | 155/500 [02:21<06:21,  1.11s/it] 31%|███▏      | 157/500 [02:21<04:32,  1.26it/s] 32%|███▏      | 159/500 [02:21<03:15,  1.74it/s] 32%|███▏      | 161/500 [02:27<07:23,  1.31s/it] 33%|███▎      | 163/500 [02:28<05:16,  1.06it/s] 33%|███▎      | 165/500 [02:31<06:14,  1.12s/it] 33%|███▎      | 167/500 [02:31<04:27,  1.25it/s] 34%|███▍      | 169/500 [02:31<03:12,  1.72it/s] 34%|███▍      | 171/500 [02:37<07:05,  1.29s/it] 35%|███▍      | 173/500 [02:37<05:02,  1.08it/s] 35%|███▌      | 175/500 [02:40<06:01,  1.11s/it] 35%|███▌      | 177/500 [02:40<04:17,  1.25it/s] 36%|███▌      | 179/500 [02:40<03:05,  1.73it/s] 36%|███▌      | 181/500 [02:46<06:53,  1.30s/it] 37%|███▋      | 183/500 [02:46<04:54,  1.08it/s] 37%|███▋      | 185/500 [02:50<05:57,  1.13s/it]**************************************************learning rate decay**************************************************
Epoch:  126  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  127  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  128  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  129  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  130  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  131  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  132  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  133  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  134  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  135  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  136  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  137  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813989520073
Epoch:  138  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  139  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  140  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  141  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  142  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  143  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  144  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  145  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  146  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  147  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  148  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  149  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  150  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  151  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  152  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  153  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  154  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  155  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  156  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  157  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  158  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813840508461
Epoch:  159  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  160  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  161  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  162  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  163  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  164  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  165  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  166  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  167  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  168  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  169  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  170  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  171  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  172  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  173  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813989520073
Epoch:  174  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  175  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  176  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  177  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  178  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  179  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  180  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  181  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  182  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  183  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  184  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  185  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
 37%|███▋      | 187/500 [02:50<04:14,  1.23it/s] 38%|███▊      | 189/500 [02:50<03:03,  1.69it/s] 38%|███▊      | 191/500 [02:56<06:44,  1.31s/it] 39%|███▊      | 193/500 [02:56<04:47,  1.07it/s] 39%|███▉      | 195/500 [02:59<05:38,  1.11s/it] 39%|███▉      | 197/500 [02:59<04:01,  1.26it/s] 40%|███▉      | 199/500 [02:59<02:53,  1.74it/s] 40%|████      | 201/500 [03:05<06:31,  1.31s/it] 41%|████      | 203/500 [03:05<04:37,  1.07it/s] 41%|████      | 205/500 [03:09<05:37,  1.14s/it] 41%|████▏     | 207/500 [03:09<04:00,  1.22it/s] 42%|████▏     | 209/500 [03:09<02:52,  1.68it/s] 42%|████▏     | 211/500 [03:15<06:18,  1.31s/it] 43%|████▎     | 213/500 [03:15<04:29,  1.07it/s] 43%|████▎     | 215/500 [03:18<05:16,  1.11s/it] 43%|████▎     | 217/500 [03:18<03:45,  1.26it/s] 44%|████▍     | 219/500 [03:18<02:41,  1.74it/s] 44%|████▍     | 221/500 [03:24<06:04,  1.31s/it] 45%|████▍     | 223/500 [03:24<04:18,  1.07it/s] 45%|████▌     | 225/500 [03:28<05:05,  1.11s/it] 45%|████▌     | 227/500 [03:28<03:37,  1.25it/s] 46%|████▌     | 229/500 [03:28<02:36,  1.73it/s] 46%|████▌     | 231/500 [03:34<05:50,  1.30s/it] 47%|████▋     | 233/500 [03:34<04:09,  1.07it/s] 47%|████▋     | 235/500 [03:37<05:01,  1.14s/it] 47%|████▋     | 237/500 [03:37<03:35,  1.22it/s] 48%|████▊     | 239/500 [03:37<02:34,  1.69it/s] 48%|████▊     | 241/500 [03:43<05:37,  1.30s/it] 49%|████▊     | 243/500 [03:43<03:59,  1.08it/s] 49%|████▉     | 245/500 [03:47<04:43,  1.11s/it]Epoch:  186  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  187  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  188  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  189  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  190  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  191  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  192  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  193  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
Epoch:  194  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  195  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  196  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  197  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  198  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  199  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  200  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813989520073
**************************************************learning rate decay**************************************************
Epoch:  201  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  202  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  203  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  204  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  205  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813989520073
**************************************************learning rate decay**************************************************
Epoch:  206  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  207  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  208  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  209  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  210  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  211  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  212  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  213  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
Epoch:  214  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  215  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  216  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813840508461
Epoch:  217  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  218  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  219  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813840508461
Epoch:  220  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  221  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  222  	Training Loss: 0.094660684466362
Test Loss:  0.13050022721290588
Valid Loss:  0.1242813915014267
Epoch:  223  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  224  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  225  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  226  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  227  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  228  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  229  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  230  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  231  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  232  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  233  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  234  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
Epoch:  235  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  236  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  237  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  238  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  239  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  240  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  241  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  242  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  243  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
Epoch:  244  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  245  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  246  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
 49%|████▉     | 247/500 [03:47<03:21,  1.25it/s] 50%|████▉     | 249/500 [03:47<02:25,  1.73it/s] 50%|█████     | 251/500 [03:53<05:22,  1.30s/it] 51%|█████     | 253/500 [03:53<03:49,  1.08it/s] 51%|█████     | 255/500 [03:56<04:30,  1.10s/it] 51%|█████▏    | 257/500 [03:56<03:12,  1.26it/s] 52%|█████▏    | 259/500 [03:56<02:18,  1.74it/s] 52%|█████▏    | 261/500 [04:02<05:11,  1.30s/it] 53%|█████▎    | 263/500 [04:02<03:40,  1.07it/s] 53%|█████▎    | 265/500 [04:05<04:20,  1.11s/it] 53%|█████▎    | 267/500 [04:05<03:05,  1.26it/s] 54%|█████▍    | 269/500 [04:06<02:13,  1.73it/s] 54%|█████▍    | 271/500 [04:12<04:58,  1.30s/it] 55%|█████▍    | 273/500 [04:12<03:31,  1.07it/s] 55%|█████▌    | 275/500 [04:15<04:08,  1.10s/it] 55%|█████▌    | 277/500 [04:15<02:56,  1.26it/s] 56%|█████▌    | 279/500 [04:15<02:06,  1.74it/s] 56%|█████▌    | 281/500 [04:21<04:44,  1.30s/it] 57%|█████▋    | 283/500 [04:21<03:21,  1.08it/s] 57%|█████▋    | 285/500 [04:24<03:58,  1.11s/it] 57%|█████▋    | 287/500 [04:24<02:50,  1.25it/s] 58%|█████▊    | 289/500 [04:24<02:02,  1.73it/s] 58%|█████▊    | 291/500 [04:30<04:32,  1.30s/it] 59%|█████▊    | 293/500 [04:31<03:12,  1.07it/s] 59%|█████▉    | 295/500 [04:34<03:47,  1.11s/it] 59%|█████▉    | 297/500 [04:34<02:41,  1.26it/s] 60%|█████▉    | 299/500 [04:34<01:55,  1.74it/s] 60%|██████    | 301/500 [04:40<04:18,  1.30s/it] 61%|██████    | 303/500 [04:40<03:02,  1.08it/s] 61%|██████    | 305/500 [04:43<03:35,  1.11s/it] 61%|██████▏   | 307/500 [04:43<02:33,  1.26it/s]Epoch:  247  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  248  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  249  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  250  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  251  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813989520073
Epoch:  252  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813989520073
Epoch:  253  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813989520073
Epoch:  254  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  255  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  256  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  257  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  258  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  259  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  260  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  261  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  262  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  263  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  264  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  265  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  266  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  267  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  268  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813989520073
Epoch:  269  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  270  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  271  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  272  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  273  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  274  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  275  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  276  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  277  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  278  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  279  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  280  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  281  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  282  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  283  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813989520073
Epoch:  284  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  285  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
**************************************************learning rate decay**************************************************
Epoch:  286  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  287  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  288  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813989520073
Epoch:  289  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  290  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  291  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  292  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  293  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
Epoch:  294  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  295  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  296  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813989520073
Epoch:  297  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  298  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  299  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
Epoch:  300  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  301  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813989520073
Epoch:  302  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  303  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  304  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  305  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  306  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  307  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813840508461
 62%|██████▏   | 309/500 [04:43<01:50,  1.74it/s] 62%|██████▏   | 311/500 [04:49<04:06,  1.31s/it] 63%|██████▎   | 313/500 [04:49<02:54,  1.07it/s] 63%|██████▎   | 315/500 [04:53<03:25,  1.11s/it] 63%|██████▎   | 317/500 [04:53<02:25,  1.25it/s] 64%|██████▍   | 319/500 [04:53<01:44,  1.73it/s] 64%|██████▍   | 321/500 [04:59<03:58,  1.33s/it] 65%|██████▍   | 323/500 [04:59<02:48,  1.05it/s] 65%|██████▌   | 325/500 [05:02<03:16,  1.12s/it] 65%|██████▌   | 327/500 [05:02<02:19,  1.24it/s] 66%|██████▌   | 329/500 [05:02<01:39,  1.71it/s] 66%|██████▌   | 331/500 [05:08<03:39,  1.30s/it] 67%|██████▋   | 333/500 [05:08<02:34,  1.08it/s] 67%|██████▋   | 335/500 [05:11<03:01,  1.10s/it] 67%|██████▋   | 337/500 [05:12<02:08,  1.27it/s] 68%|██████▊   | 339/500 [05:12<01:32,  1.75it/s] 68%|██████▊   | 341/500 [05:18<03:25,  1.30s/it] 69%|██████▊   | 343/500 [05:18<02:25,  1.08it/s] 69%|██████▉   | 345/500 [05:21<02:52,  1.11s/it] 69%|██████▉   | 347/500 [05:21<02:02,  1.25it/s] 70%|██████▉   | 349/500 [05:21<01:27,  1.73it/s] 70%|███████   | 351/500 [05:27<03:13,  1.30s/it] 71%|███████   | 353/500 [05:27<02:16,  1.08it/s] 71%|███████   | 355/500 [05:30<02:39,  1.10s/it] 71%|███████▏  | 357/500 [05:30<01:53,  1.26it/s] 72%|███████▏  | 359/500 [05:31<01:20,  1.75it/s] 72%|███████▏  | 361/500 [05:36<02:58,  1.28s/it] 73%|███████▎  | 363/500 [05:37<02:05,  1.09it/s] 73%|███████▎  | 365/500 [05:40<02:28,  1.10s/it] 73%|███████▎  | 367/500 [05:40<01:45,  1.26it/s]Epoch:  308  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  309  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  310  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  311  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813840508461
Epoch:  312  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  313  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  314  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  315  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813989520073
**************************************************learning rate decay**************************************************
Epoch:  316  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  317  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  318  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  319  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  320  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  321  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  322  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  323  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
Epoch:  324  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  325  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  326  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  327  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  328  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  329  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  330  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  331  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  332  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  333  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  334  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  335  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  336  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  337  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  338  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  339  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813840508461
Epoch:  340  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  341  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813989520073
Epoch:  342  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  343  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  344  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  345  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813989520073
**************************************************learning rate decay**************************************************
Epoch:  346  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813989520073
Epoch:  347  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  348  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  349  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  350  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  351  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  352  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  353  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813840508461
Epoch:  354  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  355  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  356  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  357  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  358  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  359  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  360  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  361  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  362  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  363  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  364  	Training Loss: 0.094660684466362
Test Loss:  0.13050022721290588
Valid Loss:  0.1242813915014267
Epoch:  365  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  366  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  367  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813840508461
Epoch:  368  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
 74%|███████▍  | 369/500 [05:40<01:15,  1.75it/s] 74%|███████▍  | 371/500 [05:46<02:48,  1.31s/it] 75%|███████▍  | 373/500 [05:46<01:58,  1.07it/s] 75%|███████▌  | 375/500 [05:49<02:18,  1.11s/it] 75%|███████▌  | 377/500 [05:49<01:37,  1.26it/s] 76%|███████▌  | 379/500 [05:49<01:09,  1.74it/s] 76%|███████▌  | 381/500 [05:55<02:35,  1.31s/it] 77%|███████▋  | 383/500 [05:55<01:49,  1.07it/s] 77%|███████▋  | 385/500 [05:56<01:17,  1.49it/s] 77%|███████▋  | 387/500 [05:56<00:55,  2.04it/s] 78%|███████▊  | 389/500 [05:56<00:40,  2.76it/s] 78%|███████▊  | 391/500 [06:02<02:05,  1.15s/it] 79%|███████▊  | 393/500 [06:02<01:28,  1.21it/s] 79%|███████▉  | 395/500 [06:05<01:48,  1.04s/it] 79%|███████▉  | 397/500 [06:05<01:16,  1.34it/s] 80%|███████▉  | 399/500 [06:05<00:54,  1.84it/s] 80%|████████  | 401/500 [06:11<02:05,  1.27s/it] 81%|████████  | 403/500 [06:11<01:28,  1.10it/s] 81%|████████  | 405/500 [06:14<01:43,  1.09s/it] 81%|████████▏ | 407/500 [06:14<01:12,  1.28it/s] 82%|████████▏ | 409/500 [06:15<00:51,  1.77it/s] 82%|████████▏ | 411/500 [06:21<01:54,  1.29s/it] 83%|████████▎ | 413/500 [06:21<01:20,  1.09it/s] 83%|████████▎ | 415/500 [06:24<01:33,  1.10s/it] 83%|████████▎ | 417/500 [06:24<01:05,  1.26it/s] 84%|████████▍ | 419/500 [06:24<00:46,  1.74it/s] 84%|████████▍ | 421/500 [06:30<01:41,  1.29s/it] 85%|████████▍ | 423/500 [06:30<01:11,  1.08it/s] 85%|████████▌ | 425/500 [06:33<01:22,  1.10s/it] 85%|████████▌ | 427/500 [06:33<00:57,  1.26it/s] 86%|████████▌ | 429/500 [06:33<00:40,  1.75it/s]Epoch:  369  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  370  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  371  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
Epoch:  372  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813989520073
Epoch:  373  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  374  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  375  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  376  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  377  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  378  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  379  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  380  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  381  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
Epoch:  382  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
Epoch:  383  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  384  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  385  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  386  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  387  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813989520073
Epoch:  388  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813989520073
Epoch:  389  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  390  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  391  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  392  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  393  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  394  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813989520073
Epoch:  395  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  396  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  397  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  398  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  399  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  400  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  401  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  402  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  403  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  404  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  405  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813840508461
**************************************************learning rate decay**************************************************
Epoch:  406  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  407  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  408  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  409  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  410  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  411  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
Epoch:  412  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  413  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  414  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  415  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  416  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  417  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  418  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  419  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813989520073
Epoch:  420  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  421  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  422  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  423  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  424  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813840508461
Epoch:  425  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  426  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  427  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  428  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  429  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  430  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
 86%|████████▌ | 431/500 [06:39<01:29,  1.30s/it] 87%|████████▋ | 433/500 [06:39<01:02,  1.08it/s] 87%|████████▋ | 435/500 [06:43<01:11,  1.10s/it] 87%|████████▋ | 437/500 [06:43<00:50,  1.26it/s] 88%|████████▊ | 439/500 [06:43<00:35,  1.73it/s] 88%|████████▊ | 441/500 [06:49<01:16,  1.29s/it] 89%|████████▊ | 443/500 [06:49<00:52,  1.08it/s] 89%|████████▉ | 445/500 [06:52<01:01,  1.11s/it] 89%|████████▉ | 447/500 [06:52<00:42,  1.26it/s] 90%|████████▉ | 449/500 [06:52<00:29,  1.74it/s] 90%|█████████ | 451/500 [06:58<01:03,  1.29s/it] 91%|█████████ | 453/500 [06:58<00:43,  1.09it/s] 91%|█████████ | 455/500 [07:01<00:49,  1.10s/it] 91%|█████████▏| 457/500 [07:01<00:33,  1.27it/s] 92%|█████████▏| 459/500 [07:01<00:23,  1.75it/s] 92%|█████████▏| 461/500 [07:08<00:53,  1.38s/it] 93%|█████████▎| 463/500 [07:08<00:36,  1.01it/s] 93%|█████████▎| 465/500 [07:11<00:40,  1.15s/it] 93%|█████████▎| 467/500 [07:11<00:27,  1.22it/s] 94%|█████████▍| 469/500 [07:11<00:18,  1.68it/s] 94%|█████████▍| 471/500 [07:17<00:37,  1.30s/it] 95%|█████████▍| 473/500 [07:18<00:25,  1.07it/s] 95%|█████████▌| 475/500 [07:21<00:27,  1.12s/it] 95%|█████████▌| 477/500 [07:21<00:18,  1.25it/s] 96%|█████████▌| 479/500 [07:21<00:12,  1.72it/s] 96%|█████████▌| 481/500 [07:27<00:24,  1.31s/it] 97%|█████████▋| 483/500 [07:27<00:16,  1.06it/s] 97%|█████████▋| 485/500 [07:30<00:16,  1.11s/it] 97%|█████████▋| 487/500 [07:30<00:10,  1.26it/s] 98%|█████████▊| 489/500 [07:30<00:06,  1.73it/s]**************************************************learning rate decay**************************************************
Epoch:  431  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813840508461
Epoch:  432  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  433  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  434  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  435  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  436  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  437  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  438  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  439  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  440  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  441  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813840508461
Epoch:  442  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  443  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  444  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813989520073
Epoch:  445  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  446  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  447  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  448  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813840508461
Epoch:  449  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
Epoch:  450  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  451  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  452  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813989520073
Epoch:  453  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  454  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  455  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813840508461
**************************************************learning rate decay**************************************************
Epoch:  456  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  457  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
Epoch:  458  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  459  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  460  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  461  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  462  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  463  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  464  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  465  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
**************************************************learning rate decay**************************************************
Epoch:  466  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813989520073
Epoch:  467  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  468  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  469  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813989520073
Epoch:  470  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  471  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813840508461
Epoch:  472  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  473  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  474  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  475  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  476  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  477  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813989520073
Epoch:  478  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
Epoch:  479  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  480  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  481  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  482  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  483  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  484  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  485  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  486  	Training Loss: 0.0946606770157814
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  487  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  488  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  489  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  490  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
 98%|█████████▊| 491/500 [07:36<00:11,  1.30s/it] 99%|█████████▊| 493/500 [07:36<00:06,  1.07it/s] 99%|█████████▉| 495/500 [07:39<00:05,  1.11s/it] 99%|█████████▉| 497/500 [07:40<00:02,  1.26it/s]100%|█████████▉| 499/500 [07:40<00:00,  1.74it/s]100%|██████████| 500/500 [07:43<00:00,  1.08it/s]
Epoch:  491  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813989520073
Epoch:  492  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  493  	Training Loss: 0.0946606770157814
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  494  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  495  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
**************************************************learning rate decay**************************************************
Epoch:  496  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  497  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  498  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813915014267
Epoch:  499  	Training Loss: 0.094660684466362
Test Loss:  0.1305002123117447
Valid Loss:  0.1242813915014267
Epoch:  500  	Training Loss: 0.094660684466362
Test Loss:  0.1305001974105835
Valid Loss:  0.1242813840508461
**************************************************learning rate decay**************************************************
seed is  19
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:02<24:37,  2.96s/it]  1%|          | 3/500 [00:03<06:45,  1.23it/s]  1%|          | 5/500 [00:03<03:31,  2.34it/s]  1%|▏         | 7/500 [00:03<02:14,  3.65it/s]  2%|▏         | 9/500 [00:03<01:35,  5.15it/s]  2%|▏         | 11/500 [00:06<05:30,  1.48it/s]  3%|▎         | 13/500 [00:06<03:50,  2.11it/s]  3%|▎         | 15/500 [00:09<06:30,  1.24it/s]  3%|▎         | 17/500 [00:09<04:36,  1.75it/s]  4%|▍         | 19/500 [00:10<03:19,  2.42it/s]  4%|▍         | 21/500 [00:15<09:37,  1.21s/it]  5%|▍         | 23/500 [00:16<06:48,  1.17it/s]  5%|▌         | 25/500 [00:19<08:22,  1.06s/it]  5%|▌         | 27/500 [00:19<05:58,  1.32it/s]  6%|▌         | 29/500 [00:19<04:18,  1.82it/s]  6%|▌         | 31/500 [00:25<09:58,  1.28s/it]  7%|▋         | 33/500 [00:25<07:05,  1.10it/s]  7%|▋         | 35/500 [00:28<08:28,  1.09s/it]  7%|▋         | 37/500 [00:28<06:03,  1.27it/s]  8%|▊         | 39/500 [00:28<04:21,  1.76it/s]  8%|▊         | 41/500 [00:34<09:52,  1.29s/it]  9%|▊         | 43/500 [00:34<07:01,  1.08it/s]  9%|▉         | 45/500 [00:34<05:02,  1.50it/s]  9%|▉         | 47/500 [00:35<03:39,  2.07it/s] 10%|▉         | 49/500 [00:35<02:41,  2.80it/s] 10%|█         | 51/500 [00:41<08:36,  1.15s/it] 11%|█         | 53/500 [00:41<06:08,  1.21it/s] 11%|█         | 55/500 [00:44<07:39,  1.03s/it] 11%|█▏        | 57/500 [00:44<05:28,  1.35it/s] 12%|█▏        | 59/500 [00:44<03:57,  1.85it/s] 12%|█▏        | 61/500 [00:50<09:17,  1.27s/it] 13%|█▎        | 63/500 [00:50<06:37,  1.10it/s]Epoch:  1  	Training Loss: 0.05025854706764221
Test Loss:  1.7104747295379639
Valid Loss:  1.6981220245361328
Epoch:  2  	Training Loss: 1.7132582664489746
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  3  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  4  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  5  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  6  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  7  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  8  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  9  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  10  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  11  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  12  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  13  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  14  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  15  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  16  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  17  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  18  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  19  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  20  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  21  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
Epoch:  22  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  23  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  24  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  25  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  26  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  27  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  28  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  29  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  30  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108234614133835
**************************************************learning rate decay**************************************************
Epoch:  31  	Training Loss: 0.0508367195725441
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  32  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  33  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  34  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  35  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  36  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  37  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  38  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  39  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  40  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  41  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  42  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  43  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  44  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  45  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  46  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  47  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  48  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  49  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  50  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  51  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  52  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  53  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  54  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  55  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  56  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  57  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  58  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  59  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  60  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  61  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  62  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  63  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  64  	Training Loss: 0.0508367158472538
Test Loss:   13%|█▎        | 65/500 [00:53<08:09,  1.13s/it] 13%|█▎        | 67/500 [00:54<05:50,  1.24it/s] 14%|█▍        | 69/500 [00:54<04:12,  1.71it/s] 14%|█▍        | 71/500 [01:00<09:14,  1.29s/it] 15%|█▍        | 73/500 [01:00<06:34,  1.08it/s] 15%|█▌        | 75/500 [01:03<07:58,  1.13s/it] 15%|█▌        | 77/500 [01:03<05:45,  1.23it/s] 16%|█▌        | 79/500 [01:03<04:11,  1.68it/s] 16%|█▌        | 80/500 [01:06<07:17,  1.04s/it] 16%|█▌        | 81/500 [01:09<10:04,  1.44s/it] 17%|█▋        | 83/500 [01:10<06:32,  1.06it/s] 17%|█▋        | 85/500 [01:13<07:53,  1.14s/it] 17%|█▋        | 87/500 [01:13<05:24,  1.27it/s] 18%|█▊        | 89/500 [01:13<03:48,  1.80it/s] 18%|█▊        | 91/500 [01:19<08:56,  1.31s/it] 19%|█▊        | 93/500 [01:19<06:16,  1.08it/s] 19%|█▉        | 95/500 [01:22<07:28,  1.11s/it] 19%|█▉        | 97/500 [01:22<05:18,  1.27it/s] 20%|█▉        | 99/500 [01:22<03:48,  1.75it/s] 20%|██        | 101/500 [01:28<08:44,  1.31s/it] 21%|██        | 103/500 [01:28<06:12,  1.07it/s] 21%|██        | 105/500 [01:32<07:30,  1.14s/it] 21%|██▏       | 107/500 [01:32<05:21,  1.22it/s] 22%|██▏       | 109/500 [01:32<03:50,  1.69it/s] 22%|██▏       | 111/500 [01:38<08:35,  1.33s/it] 23%|██▎       | 113/500 [01:38<06:06,  1.06it/s] 23%|██▎       | 115/500 [01:38<04:22,  1.47it/s] 23%|██▎       | 117/500 [01:38<03:10,  2.01it/s] 24%|██▍       | 119/500 [01:39<02:19,  2.73it/s] 24%|██▍       | 121/500 [01:45<07:21,  1.17s/it] 25%|██▍       | 123/500 [01:45<05:15,  1.20it/s]0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  65  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  66  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  67  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  68  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  69  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  70  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  71  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  72  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  73  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  74  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108234614133835
Epoch:  75  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  76  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  77  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  78  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  79  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  80  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  81  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  82  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  83  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  84  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  85  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  86  	Training Loss: 0.0508367195725441
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  87  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  88  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  89  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  90  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  91  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  92  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  93  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  94  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  95  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  96  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  97  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  98  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  99  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  100  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  101  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  102  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  103  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  104  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  105  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  106  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
Epoch:  107  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  108  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  109  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  110  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  111  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
Epoch:  112  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  113  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  114  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  115  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  116  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  117  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108234614133835
Epoch:  118  	Training Loss: 0.0508367195725441
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  119  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  120  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  121  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  122  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  123  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  124  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  125  	Training Loss: 0.0508367121219635
Test Loss:   25%|██▌       | 125/500 [01:48<06:33,  1.05s/it] 25%|██▌       | 127/500 [01:48<04:41,  1.32it/s] 26%|██▌       | 129/500 [01:48<03:23,  1.82it/s] 26%|██▌       | 131/500 [01:54<07:55,  1.29s/it] 27%|██▋       | 133/500 [01:54<05:38,  1.08it/s] 27%|██▋       | 135/500 [01:57<06:40,  1.10s/it] 27%|██▋       | 137/500 [01:57<04:46,  1.27it/s] 28%|██▊       | 139/500 [01:58<03:26,  1.75it/s] 28%|██▊       | 141/500 [02:03<07:42,  1.29s/it] 29%|██▊       | 143/500 [02:04<05:28,  1.09it/s] 29%|██▉       | 145/500 [02:07<06:32,  1.10s/it] 29%|██▉       | 147/500 [02:07<04:40,  1.26it/s] 30%|██▉       | 149/500 [02:07<03:22,  1.74it/s] 30%|███       | 151/500 [02:13<07:29,  1.29s/it] 31%|███       | 153/500 [02:13<05:20,  1.08it/s] 31%|███       | 155/500 [02:13<03:49,  1.50it/s] 31%|███▏      | 157/500 [02:13<02:46,  2.06it/s] 32%|███▏      | 159/500 [02:13<02:02,  2.79it/s] 32%|███▏      | 161/500 [02:19<06:27,  1.14s/it] 33%|███▎      | 163/500 [02:19<04:36,  1.22it/s] 33%|███▎      | 165/500 [02:22<05:42,  1.02s/it] 33%|███▎      | 167/500 [02:23<04:04,  1.36it/s] 34%|███▍      | 169/500 [02:23<02:56,  1.87it/s] 34%|███▍      | 171/500 [02:29<06:57,  1.27s/it] 35%|███▍      | 173/500 [02:29<04:57,  1.10it/s] 35%|███▌      | 175/500 [02:32<05:55,  1.09s/it] 35%|███▌      | 177/500 [02:32<04:14,  1.27it/s] 36%|███▌      | 179/500 [02:32<03:02,  1.76it/s] 36%|███▌      | 181/500 [02:38<06:54,  1.30s/it] 37%|███▋      | 183/500 [02:38<04:54,  1.08it/s]0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  126  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  127  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  128  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  129  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  130  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  131  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  132  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  133  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  134  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  135  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  136  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  137  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  138  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  139  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  140  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  141  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  142  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  143  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
Epoch:  144  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  145  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  146  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  147  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  148  	Training Loss: 0.0508367195725441
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  149  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  150  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  151  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  152  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  153  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  154  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  155  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  156  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  157  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108234614133835
Epoch:  158  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  159  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  160  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  161  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108234614133835
Epoch:  162  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  163  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  164  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  165  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  166  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  167  	Training Loss: 0.0508367195725441
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  168  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  169  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  170  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  171  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  172  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  173  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  174  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  175  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  176  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  177  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  178  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  179  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  180  	Training Loss: 0.0508367195725441
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  181  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  182  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  183  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  184  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  185  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
 37%|███▋      | 185/500 [02:41<05:47,  1.10s/it] 37%|███▋      | 187/500 [02:41<04:08,  1.26it/s] 38%|███▊      | 189/500 [02:42<02:58,  1.74it/s] 38%|███▊      | 191/500 [02:48<06:50,  1.33s/it] 39%|███▊      | 193/500 [02:48<04:51,  1.05it/s] 39%|███▉      | 195/500 [02:51<05:41,  1.12s/it] 39%|███▉      | 197/500 [02:51<04:04,  1.24it/s] 40%|███▉      | 199/500 [02:51<02:55,  1.71it/s] 40%|████      | 201/500 [02:57<06:27,  1.30s/it] 41%|████      | 203/500 [02:57<04:35,  1.08it/s] 41%|████      | 205/500 [03:00<05:24,  1.10s/it] 41%|████▏     | 207/500 [03:00<03:51,  1.27it/s] 42%|████▏     | 209/500 [03:00<02:46,  1.75it/s] 42%|████▏     | 211/500 [03:04<04:08,  1.16it/s] 43%|████▎     | 213/500 [03:04<02:58,  1.60it/s] 43%|████▎     | 215/500 [03:07<04:18,  1.10it/s] 43%|████▎     | 217/500 [03:07<03:05,  1.53it/s] 44%|████▍     | 219/500 [03:07<02:14,  2.10it/s] 44%|████▍     | 221/500 [03:13<05:42,  1.23s/it] 45%|████▍     | 223/500 [03:13<04:03,  1.14it/s] 45%|████▌     | 225/500 [03:16<04:56,  1.08s/it] 45%|████▌     | 227/500 [03:16<03:31,  1.29it/s] 46%|████▌     | 229/500 [03:17<02:32,  1.78it/s] 46%|████▌     | 231/500 [03:23<05:49,  1.30s/it] 47%|████▋     | 233/500 [03:23<04:08,  1.07it/s] 47%|████▋     | 235/500 [03:26<04:58,  1.13s/it] 47%|████▋     | 237/500 [03:26<03:32,  1.24it/s] 48%|████▊     | 239/500 [03:26<02:32,  1.71it/s] 48%|████▊     | 241/500 [03:32<05:49,  1.35s/it] 49%|████▊     | 243/500 [03:33<04:07,  1.04it/s]**************************************************learning rate decay**************************************************
Epoch:  186  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  187  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  188  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  189  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  190  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  191  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  192  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  193  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  194  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  195  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  196  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  197  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  198  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  199  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  200  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  201  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  202  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  203  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  204  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  205  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  206  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  207  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  208  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  209  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  210  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  211  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  212  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  213  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  214  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  215  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  216  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
Epoch:  217  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  218  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  219  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  220  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  221  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  222  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  223  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  224  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  225  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  226  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  227  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  228  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  229  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  230  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  231  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  232  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  233  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  234  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  235  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  236  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  237  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  238  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  239  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  240  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  241  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  242  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  243  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  244  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  245  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
 49%|████▉     | 245/500 [03:36<04:51,  1.14s/it] 49%|████▉     | 247/500 [03:36<03:27,  1.22it/s] 50%|████▉     | 249/500 [03:36<02:29,  1.68it/s] 50%|█████     | 251/500 [03:42<05:24,  1.30s/it] 51%|█████     | 253/500 [03:42<03:50,  1.07it/s] 51%|█████     | 255/500 [03:45<04:32,  1.11s/it] 51%|█████▏    | 257/500 [03:45<03:13,  1.25it/s] 52%|█████▏    | 259/500 [03:45<02:19,  1.73it/s] 52%|█████▏    | 261/500 [03:51<05:08,  1.29s/it] 53%|█████▎    | 263/500 [03:51<03:38,  1.08it/s] 53%|█████▎    | 265/500 [03:54<04:19,  1.11s/it] 53%|█████▎    | 267/500 [03:55<03:05,  1.26it/s] 54%|█████▍    | 269/500 [03:55<02:12,  1.74it/s] 54%|█████▍    | 271/500 [04:01<04:59,  1.31s/it] 55%|█████▍    | 273/500 [04:01<03:32,  1.07it/s] 55%|█████▌    | 275/500 [04:04<04:08,  1.11s/it] 55%|█████▌    | 277/500 [04:04<02:56,  1.26it/s] 56%|█████▌    | 279/500 [04:04<02:06,  1.74it/s] 56%|█████▌    | 281/500 [04:07<03:07,  1.17it/s] 57%|█████▋    | 283/500 [04:07<02:14,  1.61it/s] 57%|█████▋    | 285/500 [04:10<03:09,  1.13it/s] 57%|█████▋    | 287/500 [04:10<02:15,  1.57it/s] 58%|█████▊    | 289/500 [04:10<01:38,  2.15it/s] 58%|█████▊    | 291/500 [04:14<02:42,  1.28it/s] 59%|█████▊    | 293/500 [04:14<01:57,  1.77it/s] 59%|█████▉    | 295/500 [04:17<02:53,  1.18it/s] 59%|█████▉    | 297/500 [04:17<02:04,  1.63it/s] 60%|█████▉    | 299/500 [04:17<01:30,  2.23it/s] 60%|██████    | 301/500 [04:23<03:59,  1.20s/it] 61%|██████    | 303/500 [04:23<02:49,  1.16it/s] 61%|██████    | 305/500 [04:26<03:26,  1.06s/it]**************************************************learning rate decay**************************************************
Epoch:  246  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  247  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  248  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  249  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  250  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  251  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  252  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  253  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  254  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  255  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  256  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  257  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  258  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  259  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  260  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  261  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  262  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  263  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  264  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  265  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  266  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  267  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  268  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  269  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  270  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  271  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  272  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  273  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  274  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  275  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  276  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  277  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  278  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  279  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  280  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  281  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  282  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  283  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  284  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
Epoch:  285  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  286  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  287  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  288  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
Epoch:  289  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  290  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
Epoch:  291  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  292  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  293  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  294  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  295  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  296  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  297  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  298  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  299  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  300  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  301  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  302  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  303  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  304  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  305  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  306  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  61%|██████▏   | 307/500 [04:26<02:26,  1.31it/s] 62%|██████▏   | 309/500 [04:26<01:45,  1.81it/s] 62%|██████▏   | 311/500 [04:32<04:06,  1.31s/it] 63%|██████▎   | 313/500 [04:33<02:54,  1.07it/s] 63%|██████▎   | 315/500 [04:36<03:28,  1.13s/it] 63%|██████▎   | 317/500 [04:36<02:28,  1.24it/s] 64%|██████▍   | 319/500 [04:36<01:45,  1.71it/s] 64%|██████▍   | 321/500 [04:42<03:51,  1.29s/it] 65%|██████▍   | 323/500 [04:42<02:43,  1.08it/s] 65%|██████▌   | 325/500 [04:45<03:13,  1.10s/it] 65%|██████▌   | 327/500 [04:45<02:17,  1.26it/s] 66%|██████▌   | 329/500 [04:45<01:38,  1.74it/s] 66%|██████▌   | 331/500 [04:51<03:38,  1.29s/it] 67%|██████▋   | 333/500 [04:51<02:34,  1.08it/s] 67%|██████▋   | 335/500 [04:54<03:01,  1.10s/it] 67%|██████▋   | 337/500 [04:55<02:08,  1.27it/s] 68%|██████▊   | 339/500 [04:55<01:32,  1.75it/s] 68%|██████▊   | 341/500 [05:01<03:25,  1.30s/it] 69%|██████▊   | 343/500 [05:01<02:25,  1.08it/s] 69%|██████▉   | 345/500 [05:04<02:50,  1.10s/it] 69%|██████▉   | 347/500 [05:04<02:00,  1.27it/s] 70%|██████▉   | 349/500 [05:04<01:26,  1.75it/s] 70%|███████   | 351/500 [05:10<03:11,  1.29s/it] 71%|███████   | 353/500 [05:10<02:15,  1.09it/s] 71%|███████   | 355/500 [05:13<02:39,  1.10s/it] 71%|███████▏  | 357/500 [05:13<01:52,  1.27it/s] 72%|███████▏  | 359/500 [05:13<01:20,  1.75it/s] 72%|███████▏  | 361/500 [05:19<02:59,  1.29s/it] 73%|███████▎  | 363/500 [05:19<02:06,  1.08it/s] 73%|███████▎  | 365/500 [05:22<02:29,  1.10s/it] 0.07108233869075775
Epoch:  307  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  308  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  309  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  310  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  311  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  312  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  313  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  314  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  315  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  316  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  317  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  318  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  319  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  320  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  321  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  322  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  323  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  324  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  325  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  326  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  327  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  328  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  329  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  330  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  331  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  332  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  333  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  334  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  335  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  336  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  337  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  338  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  339  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
Epoch:  340  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  341  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  342  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  343  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  344  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  345  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  346  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  347  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  348  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  349  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  350  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  351  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  352  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  353  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  354  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  355  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  356  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  357  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
Epoch:  358  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108234614133835
Epoch:  359  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  360  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  361  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  362  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  363  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  364  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  365  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  366  	Training Loss: 0.0508367158472538
Test Loss:   73%|███████▎  | 367/500 [05:23<01:45,  1.26it/s] 74%|███████▍  | 369/500 [05:23<01:15,  1.74it/s] 74%|███████▍  | 371/500 [05:29<02:46,  1.29s/it] 75%|███████▍  | 373/500 [05:29<01:57,  1.08it/s] 75%|███████▌  | 375/500 [05:32<02:17,  1.10s/it] 75%|███████▌  | 377/500 [05:32<01:37,  1.26it/s] 76%|███████▌  | 379/500 [05:32<01:09,  1.74it/s] 76%|███████▌  | 381/500 [05:38<02:34,  1.30s/it] 77%|███████▋  | 383/500 [05:38<01:48,  1.08it/s] 77%|███████▋  | 385/500 [05:41<02:07,  1.11s/it] 77%|███████▋  | 387/500 [05:41<01:29,  1.26it/s] 78%|███████▊  | 389/500 [05:42<01:03,  1.74it/s] 78%|███████▊  | 391/500 [05:47<02:21,  1.30s/it] 79%|███████▊  | 393/500 [05:48<01:39,  1.08it/s] 79%|███████▉  | 395/500 [05:51<01:57,  1.12s/it] 79%|███████▉  | 397/500 [05:51<01:22,  1.24it/s] 80%|███████▉  | 399/500 [05:51<00:58,  1.72it/s] 80%|████████  | 401/500 [05:57<02:07,  1.29s/it] 81%|████████  | 403/500 [05:57<01:29,  1.08it/s] 81%|████████  | 405/500 [06:00<01:44,  1.10s/it] 81%|████████▏ | 407/500 [06:00<01:13,  1.26it/s] 82%|████████▏ | 409/500 [06:00<00:52,  1.74it/s] 82%|████████▏ | 411/500 [06:06<01:55,  1.29s/it] 83%|████████▎ | 413/500 [06:06<01:20,  1.08it/s] 83%|████████▎ | 415/500 [06:09<01:33,  1.10s/it] 83%|████████▎ | 417/500 [06:10<01:05,  1.26it/s] 84%|████████▍ | 419/500 [06:10<00:46,  1.74it/s] 84%|████████▍ | 421/500 [06:16<01:42,  1.30s/it] 85%|████████▍ | 423/500 [06:16<01:11,  1.07it/s] 85%|████████▌ | 425/500 [06:19<01:22,  1.10s/it]0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  367  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  368  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  369  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  370  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  371  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108234614133835
Epoch:  372  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
Epoch:  373  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  374  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  375  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  376  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
Epoch:  377  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
Epoch:  378  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  379  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  380  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  381  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  382  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  383  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
Epoch:  384  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  385  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  386  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  387  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  388  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  389  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  390  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  391  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  392  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  393  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  394  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  395  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  396  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  397  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  398  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  399  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  400  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  401  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  402  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
Epoch:  403  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  404  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  405  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  406  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  407  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  408  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  409  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
Epoch:  410  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  411  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  412  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  413  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  414  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  415  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  416  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  417  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  418  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  419  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  420  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  421  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  422  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  423  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  424  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  425  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
 85%|████████▌ | 427/500 [06:19<00:57,  1.26it/s] 86%|████████▌ | 429/500 [06:19<00:40,  1.74it/s] 86%|████████▌ | 431/500 [06:25<01:29,  1.30s/it] 87%|████████▋ | 433/500 [06:25<01:02,  1.08it/s] 87%|████████▋ | 435/500 [06:28<01:11,  1.11s/it] 87%|████████▋ | 437/500 [06:28<00:50,  1.26it/s] 88%|████████▊ | 439/500 [06:29<00:35,  1.73it/s] 88%|████████▊ | 441/500 [06:34<01:16,  1.30s/it] 89%|████████▊ | 443/500 [06:35<00:52,  1.08it/s] 89%|████████▉ | 445/500 [06:38<01:00,  1.10s/it] 89%|████████▉ | 447/500 [06:38<00:41,  1.26it/s] 90%|████████▉ | 449/500 [06:38<00:29,  1.75it/s] 90%|█████████ | 451/500 [06:44<01:03,  1.29s/it] 91%|█████████ | 453/500 [06:44<00:43,  1.08it/s] 91%|█████████ | 455/500 [06:47<00:50,  1.12s/it] 91%|█████████▏| 457/500 [06:47<00:34,  1.25it/s] 92%|█████████▏| 459/500 [06:47<00:23,  1.72it/s] 92%|█████████▏| 461/500 [06:53<00:51,  1.31s/it] 93%|█████████▎| 463/500 [06:54<00:34,  1.07it/s] 93%|█████████▎| 465/500 [06:57<00:38,  1.11s/it] 93%|█████████▎| 467/500 [06:57<00:26,  1.26it/s] 94%|█████████▍| 469/500 [06:57<00:17,  1.74it/s] 94%|█████████▍| 471/500 [07:03<00:38,  1.32s/it] 95%|█████████▍| 473/500 [07:03<00:25,  1.06it/s] 95%|█████████▌| 475/500 [07:06<00:28,  1.12s/it] 95%|█████████▌| 477/500 [07:06<00:18,  1.24it/s] 96%|█████████▌| 479/500 [07:06<00:12,  1.71it/s] 96%|█████████▌| 481/500 [07:12<00:24,  1.31s/it] 97%|█████████▋| 483/500 [07:13<00:15,  1.07it/s]Epoch:  426  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  427  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  428  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  429  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  430  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  431  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  432  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  433  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  434  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  435  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  436  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  437  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  438  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  439  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  440  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  441  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  442  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  443  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  444  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  445  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  446  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  447  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  448  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  449  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  450  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  451  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  452  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  453  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  454  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  455  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  456  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  457  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  458  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  459  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  460  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  461  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  462  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
Epoch:  463  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  464  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  465  	Training Loss: 0.0508367195725441
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  466  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  467  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  468  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  469  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  470  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  471  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  472  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  473  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  474  	Training Loss: 0.0508367195725441
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  475  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  476  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  477  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  478  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108234614133835
Epoch:  479  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  480  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  481  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096821784973
Valid Loss:  0.07108233869075775
Epoch:  482  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  483  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  484  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  485  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
 97%|█████████▋| 485/500 [07:16<00:16,  1.11s/it] 97%|█████████▋| 487/500 [07:16<00:10,  1.25it/s] 98%|█████████▊| 489/500 [07:16<00:06,  1.73it/s] 98%|█████████▊| 491/500 [07:22<00:11,  1.31s/it] 99%|█████████▊| 493/500 [07:22<00:06,  1.07it/s] 99%|█████████▉| 495/500 [07:25<00:05,  1.11s/it] 99%|█████████▉| 497/500 [07:25<00:02,  1.25it/s]100%|█████████▉| 499/500 [07:25<00:00,  1.73it/s]100%|██████████| 500/500 [07:28<00:00,  1.11it/s]
**************************************************learning rate decay**************************************************
Epoch:  486  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  487  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  488  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  489  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  490  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  491  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  492  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  493  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  494  	Training Loss: 0.0508367158472538
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  495  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
Epoch:  496  	Training Loss: 0.0508367121219635
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  497  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  498  	Training Loss: 0.0508367121219635
Test Loss:  0.06600095331668854
Valid Loss:  0.07108233869075775
Epoch:  499  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
Epoch:  500  	Training Loss: 0.0508367158472538
Test Loss:  0.06600096076726913
Valid Loss:  0.07108233869075775
**************************************************learning rate decay**************************************************
seed is  20
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:03<25:21,  3.05s/it]  1%|          | 3/500 [00:03<06:56,  1.19it/s]  1%|          | 5/500 [00:03<03:37,  2.27it/s]  1%|▏         | 7/500 [00:03<02:18,  3.56it/s]  2%|▏         | 9/500 [00:03<01:37,  5.03it/s]  2%|▏         | 11/500 [00:06<05:22,  1.52it/s]  3%|▎         | 13/500 [00:06<03:46,  2.15it/s]  3%|▎         | 15/500 [00:09<06:29,  1.25it/s]  3%|▎         | 17/500 [00:09<04:35,  1.76it/s]  4%|▍         | 19/500 [00:10<03:18,  2.43it/s]  4%|▍         | 21/500 [00:16<09:38,  1.21s/it]  5%|▍         | 23/500 [00:16<06:49,  1.17it/s]  5%|▌         | 25/500 [00:19<08:24,  1.06s/it]  5%|▌         | 27/500 [00:19<05:59,  1.31it/s]  6%|▌         | 29/500 [00:19<04:19,  1.82it/s]  6%|▌         | 31/500 [00:25<10:04,  1.29s/it]  7%|▋         | 33/500 [00:25<07:10,  1.08it/s]  7%|▋         | 35/500 [00:28<08:31,  1.10s/it]  7%|▋         | 37/500 [00:28<06:05,  1.27it/s]  8%|▊         | 39/500 [00:28<04:23,  1.75it/s]  8%|▊         | 41/500 [00:34<09:58,  1.30s/it]  9%|▊         | 43/500 [00:35<07:05,  1.07it/s]  9%|▉         | 45/500 [00:38<08:23,  1.11s/it]  9%|▉         | 47/500 [00:38<05:59,  1.26it/s] 10%|▉         | 49/500 [00:38<04:19,  1.74it/s] 10%|█         | 51/500 [00:44<09:38,  1.29s/it] 11%|█         | 53/500 [00:44<06:52,  1.08it/s] 11%|█         | 55/500 [00:47<08:08,  1.10s/it] 11%|█▏        | 57/500 [00:47<05:48,  1.27it/s] 12%|█▏        | 59/500 [00:47<04:11,  1.75it/s] 12%|█▏        | 61/500 [00:53<09:26,  1.29s/it]Epoch:  1  	Training Loss: 0.16159100830554962
Test Loss:  96.68785095214844
Valid Loss:  93.6259765625
Epoch:  2  	Training Loss: 93.17604064941406
Test Loss:  0.5825637578964233
Valid Loss:  0.5457144379615784
Epoch:  3  	Training Loss: 0.4752715826034546
Test Loss:  0.2762100100517273
Valid Loss:  0.2681681513786316
Epoch:  4  	Training Loss: 0.21206578612327576
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  5  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  6  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  7  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  8  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  9  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  10  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587987780570984
Epoch:  11  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  12  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  13  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  14  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  15  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  16  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  17  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  18  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  19  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  20  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  21  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  22  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  23  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  24  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  25  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  26  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  27  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  28  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  29  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  30  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  31  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  32  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  33  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  34  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  35  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587987780570984
**************************************************learning rate decay**************************************************
Epoch:  36  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  37  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  38  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  39  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  40  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  41  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  42  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  43  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  44  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  45  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  46  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  47  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  48  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  49  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  50  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  51  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  52  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  53  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  54  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  55  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  56  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  57  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  58  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  59  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  60  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  61  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  62  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  63  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:   13%|█▎        | 63/500 [00:53<06:43,  1.08it/s] 13%|█▎        | 65/500 [00:56<07:58,  1.10s/it] 13%|█▎        | 67/500 [00:56<05:42,  1.26it/s] 14%|█▍        | 69/500 [00:57<04:06,  1.75it/s] 14%|█▍        | 71/500 [01:02<09:14,  1.29s/it] 15%|█▍        | 73/500 [01:03<06:35,  1.08it/s] 15%|█▌        | 75/500 [01:06<07:49,  1.10s/it] 15%|█▌        | 77/500 [01:06<05:35,  1.26it/s] 16%|█▌        | 79/500 [01:06<04:01,  1.74it/s] 16%|█▌        | 81/500 [01:12<09:02,  1.29s/it] 17%|█▋        | 83/500 [01:12<06:25,  1.08it/s] 17%|█▋        | 85/500 [01:15<07:40,  1.11s/it] 17%|█▋        | 87/500 [01:15<05:28,  1.26it/s] 18%|█▊        | 89/500 [01:15<03:57,  1.73it/s] 18%|█▊        | 91/500 [01:21<08:56,  1.31s/it] 19%|█▊        | 93/500 [01:21<06:21,  1.07it/s] 19%|█▉        | 95/500 [01:25<07:31,  1.11s/it] 19%|█▉        | 97/500 [01:25<05:22,  1.25it/s] 20%|█▉        | 99/500 [01:25<03:51,  1.73it/s] 20%|██        | 101/500 [01:31<08:39,  1.30s/it] 21%|██        | 103/500 [01:31<06:10,  1.07it/s] 21%|██        | 105/500 [01:34<07:18,  1.11s/it] 21%|██▏       | 107/500 [01:34<05:13,  1.25it/s] 22%|██▏       | 109/500 [01:34<03:45,  1.73it/s] 22%|██▏       | 111/500 [01:40<08:22,  1.29s/it] 23%|██▎       | 113/500 [01:40<05:58,  1.08it/s] 23%|██▎       | 115/500 [01:43<07:06,  1.11s/it] 23%|██▎       | 117/500 [01:43<05:04,  1.26it/s] 24%|██▍       | 119/500 [01:44<03:39,  1.74it/s] 24%|██▍       | 121/500 [01:50<08:10,  1.30s/it] 25%|██▍       | 123/500 [01:50<05:49,  1.08it/s]0.2587988078594208
Epoch:  64  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  65  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  66  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  67  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  68  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  69  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  70  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  71  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  72  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  73  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  74  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  75  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  76  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  77  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  78  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  79  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587987780570984
Epoch:  80  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  81  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  82  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  83  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  84  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  85  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  86  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  87  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  88  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  89  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  90  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  91  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  92  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  93  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  94  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  95  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  96  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  97  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  98  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  99  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  100  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  101  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  102  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  103  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  104  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  105  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  106  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  107  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  108  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  109  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  110  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  111  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  112  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  113  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  114  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  115  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  116  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  117  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  118  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  119  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  120  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  121  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  122  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  123  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
 25%|██▌       | 125/500 [01:53<06:57,  1.11s/it] 25%|██▌       | 127/500 [01:53<04:58,  1.25it/s] 26%|██▌       | 129/500 [01:53<03:34,  1.73it/s] 26%|██▌       | 131/500 [01:59<07:59,  1.30s/it] 27%|██▋       | 133/500 [01:59<05:41,  1.08it/s] 27%|██▋       | 135/500 [02:02<06:43,  1.11s/it] 27%|██▋       | 137/500 [02:02<04:47,  1.26it/s] 28%|██▊       | 139/500 [02:02<03:27,  1.74it/s] 28%|██▊       | 141/500 [02:08<07:46,  1.30s/it] 29%|██▊       | 143/500 [02:09<05:31,  1.08it/s] 29%|██▉       | 145/500 [02:12<06:35,  1.11s/it] 29%|██▉       | 147/500 [02:12<04:42,  1.25it/s] 30%|██▉       | 149/500 [02:12<03:23,  1.73it/s] 30%|███       | 151/500 [02:18<07:35,  1.31s/it] 31%|███       | 153/500 [02:18<05:24,  1.07it/s] 31%|███       | 155/500 [02:21<06:21,  1.11s/it] 31%|███▏      | 157/500 [02:21<04:32,  1.26it/s] 32%|███▏      | 159/500 [02:21<03:16,  1.74it/s] 32%|███▏      | 161/500 [02:27<07:18,  1.29s/it] 33%|███▎      | 163/500 [02:27<05:12,  1.08it/s] 33%|███▎      | 165/500 [02:30<06:10,  1.11s/it] 33%|███▎      | 167/500 [02:31<04:24,  1.26it/s] 34%|███▍      | 169/500 [02:31<03:10,  1.74it/s] 34%|███▍      | 171/500 [02:37<07:06,  1.30s/it] 35%|███▍      | 173/500 [02:37<05:03,  1.08it/s] 35%|███▌      | 175/500 [02:40<06:00,  1.11s/it] 35%|███▌      | 177/500 [02:40<04:16,  1.26it/s] 36%|███▌      | 179/500 [02:40<03:04,  1.74it/s] 36%|███▌      | 181/500 [02:46<06:52,  1.29s/it] 37%|███▋      | 183/500 [02:46<04:53,  1.08it/s]Epoch:  124  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  125  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  126  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  127  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  128  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  129  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  130  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  131  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  132  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  133  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  134  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  135  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  136  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  137  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  138  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  139  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  140  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  141  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  142  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  143  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  144  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  145  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  146  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  147  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  148  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  149  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  150  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587987780570984
**************************************************learning rate decay**************************************************
Epoch:  151  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  152  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  153  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  154  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  155  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  156  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  157  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  158  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  159  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  160  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  161  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  162  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  163  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  164  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  165  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  166  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  167  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  168  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  169  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  170  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  171  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  172  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  173  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  174  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  175  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  176  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  177  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  178  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587987780570984
Epoch:  179  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  180  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  181  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  182  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  183  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
 37%|███▋      | 185/500 [02:49<05:47,  1.10s/it] 37%|███▋      | 187/500 [02:49<04:07,  1.26it/s] 38%|███▊      | 189/500 [02:49<02:58,  1.74it/s] 38%|███▊      | 191/500 [02:55<06:39,  1.29s/it] 39%|███▊      | 193/500 [02:56<04:43,  1.08it/s] 39%|███▉      | 195/500 [02:59<05:35,  1.10s/it] 39%|███▉      | 197/500 [02:59<03:59,  1.27it/s] 40%|███▉      | 199/500 [02:59<02:52,  1.75it/s] 40%|████      | 201/500 [03:02<04:17,  1.16it/s] 41%|████      | 203/500 [03:02<03:05,  1.60it/s] 41%|████      | 205/500 [03:05<04:23,  1.12it/s] 41%|████▏     | 207/500 [03:05<03:08,  1.55it/s] 42%|████▏     | 209/500 [03:05<02:16,  2.13it/s] 42%|████▏     | 211/500 [03:11<05:51,  1.22s/it] 43%|████▎     | 213/500 [03:11<04:10,  1.15it/s] 43%|████▎     | 215/500 [03:14<05:05,  1.07s/it] 43%|████▎     | 217/500 [03:15<03:37,  1.30it/s] 44%|████▍     | 219/500 [03:15<02:36,  1.79it/s] 44%|████▍     | 221/500 [03:21<05:55,  1.28s/it] 45%|████▍     | 223/500 [03:21<04:13,  1.09it/s] 45%|████▌     | 225/500 [03:24<05:00,  1.09s/it] 45%|████▌     | 227/500 [03:24<03:33,  1.28it/s] 46%|████▌     | 229/500 [03:24<02:33,  1.76it/s] 46%|████▌     | 231/500 [03:30<05:47,  1.29s/it] 47%|████▋     | 233/500 [03:30<04:06,  1.08it/s] 47%|████▋     | 235/500 [03:33<04:52,  1.10s/it] 47%|████▋     | 237/500 [03:33<03:28,  1.26it/s] 48%|████▊     | 239/500 [03:33<02:30,  1.74it/s] 48%|████▊     | 241/500 [03:39<05:34,  1.29s/it] 49%|████▊     | 243/500 [03:39<03:57,  1.08it/s]Epoch:  184  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  185  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  186  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  187  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  188  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  189  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  190  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  191  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  192  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  193  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  194  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  195  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  196  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  197  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  198  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  199  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  200  	Training Loss: 0.19853276014328003
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  201  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  202  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  203  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  204  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  205  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  206  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  207  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  208  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  209  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  210  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  211  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  212  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  213  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  214  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  215  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  216  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  217  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  218  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  219  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  220  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  221  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  222  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  223  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  224  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  225  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  226  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  227  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  228  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  229  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  230  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  231  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  232  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587987780570984
Epoch:  233  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  234  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  235  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  236  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  237  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  238  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  239  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  240  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  241  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  242  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  243  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  244  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
 49%|████▉     | 245/500 [03:43<04:41,  1.10s/it] 49%|████▉     | 247/500 [03:43<03:20,  1.26it/s] 50%|████▉     | 249/500 [03:43<02:23,  1.75it/s] 50%|█████     | 251/500 [03:49<05:20,  1.29s/it] 51%|█████     | 253/500 [03:49<03:47,  1.09it/s] 51%|█████     | 255/500 [03:52<04:28,  1.10s/it] 51%|█████▏    | 257/500 [03:52<03:11,  1.27it/s] 52%|█████▏    | 259/500 [03:52<02:17,  1.75it/s] 52%|█████▏    | 261/500 [03:58<05:09,  1.29s/it] 53%|█████▎    | 263/500 [03:58<03:39,  1.08it/s] 53%|█████▎    | 265/500 [04:01<04:18,  1.10s/it] 53%|█████▎    | 267/500 [04:01<03:03,  1.27it/s] 54%|█████▍    | 269/500 [04:01<02:12,  1.75it/s] 54%|█████▍    | 271/500 [04:07<04:56,  1.29s/it] 55%|█████▍    | 273/500 [04:08<03:29,  1.08it/s] 55%|█████▌    | 275/500 [04:11<04:08,  1.10s/it] 55%|█████▌    | 277/500 [04:11<02:56,  1.26it/s] 56%|█████▌    | 279/500 [04:11<02:06,  1.74it/s] 56%|█████▌    | 281/500 [04:17<04:42,  1.29s/it] 57%|█████▋    | 283/500 [04:17<03:20,  1.08it/s] 57%|█████▋    | 285/500 [04:20<03:56,  1.10s/it] 57%|█████▋    | 287/500 [04:20<02:47,  1.27it/s] 58%|█████▊    | 289/500 [04:20<02:00,  1.75it/s] 58%|█████▊    | 291/500 [04:26<04:32,  1.30s/it] 59%|█████▊    | 293/500 [04:26<03:12,  1.07it/s] 59%|█████▉    | 295/500 [04:29<03:47,  1.11s/it] 59%|█████▉    | 297/500 [04:30<02:41,  1.25it/s] 60%|█████▉    | 299/500 [04:30<01:56,  1.73it/s] 60%|██████    | 301/500 [04:36<04:21,  1.31s/it] 61%|██████    | 303/500 [04:36<03:04,  1.06it/s]Epoch:  245  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  246  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  247  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  248  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  249  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  250  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  251  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  252  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  253  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  254  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  255  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  256  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  257  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  258  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  259  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  260  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  261  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  262  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  263  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  264  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  265  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  266  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  267  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  268  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  269  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  270  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  271  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  272  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  273  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  274  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  275  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  276  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  277  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  278  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  279  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  280  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  281  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  282  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587987780570984
Epoch:  283  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  284  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  285  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  286  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  287  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  288  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  289  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  290  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  291  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  292  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  293  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587987780570984
Epoch:  294  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  295  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  296  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  297  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  298  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  299  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  300  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  301  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  302  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  303  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  304  	Training Loss: 0.19853276014328003
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
 61%|██████    | 305/500 [04:39<03:36,  1.11s/it] 61%|██████▏   | 307/500 [04:39<02:33,  1.26it/s] 62%|██████▏   | 309/500 [04:39<01:50,  1.73it/s] 62%|██████▏   | 311/500 [04:45<04:05,  1.30s/it] 63%|██████▎   | 313/500 [04:45<02:53,  1.08it/s] 63%|██████▎   | 315/500 [04:48<03:24,  1.10s/it] 63%|██████▎   | 317/500 [04:48<02:25,  1.26it/s] 64%|██████▍   | 319/500 [04:49<01:44,  1.74it/s] 64%|██████▍   | 321/500 [04:54<03:52,  1.30s/it] 65%|██████▍   | 323/500 [04:55<02:44,  1.08it/s] 65%|██████▌   | 325/500 [04:58<03:13,  1.11s/it] 65%|██████▌   | 327/500 [04:58<02:17,  1.26it/s] 66%|██████▌   | 329/500 [04:58<01:38,  1.74it/s] 66%|██████▌   | 331/500 [05:04<03:47,  1.35s/it] 67%|██████▋   | 333/500 [05:04<02:40,  1.04it/s] 67%|██████▋   | 335/500 [05:07<03:06,  1.13s/it] 67%|██████▋   | 337/500 [05:08<02:12,  1.23it/s] 68%|██████▊   | 339/500 [05:08<01:34,  1.70it/s] 68%|██████▊   | 341/500 [05:14<03:28,  1.31s/it] 69%|██████▊   | 343/500 [05:14<02:27,  1.07it/s] 69%|██████▉   | 345/500 [05:17<02:51,  1.11s/it] 69%|██████▉   | 347/500 [05:17<02:01,  1.26it/s] 70%|██████▉   | 349/500 [05:17<01:26,  1.74it/s] 70%|███████   | 351/500 [05:23<03:15,  1.31s/it] 71%|███████   | 353/500 [05:23<02:17,  1.07it/s] 71%|███████   | 355/500 [05:26<02:41,  1.11s/it] 71%|███████▏  | 357/500 [05:26<01:54,  1.25it/s] 72%|███████▏  | 359/500 [05:27<01:21,  1.73it/s] 72%|███████▏  | 361/500 [05:33<03:01,  1.31s/it] 73%|███████▎  | 363/500 [05:33<02:08,  1.07it/s]Epoch:  305  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  306  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  307  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  308  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  309  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  310  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  311  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  312  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  313  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  314  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  315  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  316  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  317  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  318  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  319  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  320  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  321  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  322  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  323  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  324  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  325  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  326  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  327  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  328  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  329  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  330  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  331  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  332  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  333  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  334  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  335  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  336  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  337  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  338  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  339  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  340  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  341  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  342  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  343  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  344  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  345  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  346  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  347  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  348  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  349  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  350  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  351  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  352  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  353  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  354  	Training Loss: 0.19853276014328003
Test Loss:  0.2667001485824585
Valid Loss:  0.2587987780570984
Epoch:  355  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  356  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  357  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  358  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  359  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  360  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  361  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  362  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  363  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  364  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
 73%|███████▎  | 365/500 [05:36<02:29,  1.11s/it] 73%|███████▎  | 367/500 [05:36<01:45,  1.26it/s] 74%|███████▍  | 369/500 [05:36<01:15,  1.74it/s] 74%|███████▍  | 371/500 [05:42<02:47,  1.30s/it] 75%|███████▍  | 373/500 [05:42<01:58,  1.08it/s] 75%|███████▌  | 375/500 [05:45<02:19,  1.12s/it] 75%|███████▌  | 377/500 [05:45<01:38,  1.25it/s] 76%|███████▌  | 379/500 [05:45<01:10,  1.72it/s] 76%|███████▌  | 381/500 [05:51<02:35,  1.31s/it] 77%|███████▋  | 383/500 [05:52<01:49,  1.07it/s] 77%|███████▋  | 385/500 [05:55<02:08,  1.12s/it] 77%|███████▋  | 387/500 [05:55<01:30,  1.25it/s] 78%|███████▊  | 389/500 [05:55<01:04,  1.73it/s] 78%|███████▊  | 391/500 [06:01<02:21,  1.30s/it] 79%|███████▊  | 393/500 [06:01<01:39,  1.08it/s] 79%|███████▉  | 395/500 [06:04<01:57,  1.12s/it] 79%|███████▉  | 397/500 [06:04<01:22,  1.24it/s] 80%|███████▉  | 399/500 [06:04<00:58,  1.72it/s] 80%|████████  | 401/500 [06:10<02:09,  1.30s/it] 81%|████████  | 403/500 [06:11<01:30,  1.07it/s] 81%|████████  | 405/500 [06:14<01:45,  1.11s/it] 81%|████████▏ | 407/500 [06:14<01:14,  1.25it/s] 82%|████████▏ | 409/500 [06:14<00:52,  1.73it/s] 82%|████████▏ | 411/500 [06:20<01:55,  1.30s/it] 83%|████████▎ | 413/500 [06:20<01:20,  1.08it/s] 83%|████████▎ | 415/500 [06:23<01:34,  1.11s/it] 83%|████████▎ | 417/500 [06:23<01:06,  1.25it/s] 84%|████████▍ | 419/500 [06:23<00:46,  1.73it/s] 84%|████████▍ | 421/500 [06:29<01:42,  1.30s/it] 85%|████████▍ | 423/500 [06:29<01:11,  1.07it/s]Epoch:  365  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  366  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  367  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  368  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  369  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  370  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  371  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  372  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  373  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  374  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  375  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  376  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  377  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  378  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  379  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  380  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  381  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  382  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  383  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  384  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  385  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  386  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  387  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  388  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  389  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  390  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  391  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  392  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  393  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  394  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  395  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  396  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  397  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  398  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  399  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  400  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  401  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  402  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  403  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  404  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  405  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  406  	Training Loss: 0.19853276014328003
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  407  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  408  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  409  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  410  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  411  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  412  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  413  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  414  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  415  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  416  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  417  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  418  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  419  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  420  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  421  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  422  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  423  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  424  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
 85%|████████▌ | 425/500 [06:32<01:23,  1.11s/it] 85%|████████▌ | 427/500 [06:33<00:58,  1.25it/s] 86%|████████▌ | 429/500 [06:33<00:40,  1.73it/s] 86%|████████▌ | 431/500 [06:39<01:30,  1.31s/it] 87%|████████▋ | 433/500 [06:39<01:02,  1.07it/s] 87%|████████▋ | 435/500 [06:42<01:13,  1.13s/it] 87%|████████▋ | 437/500 [06:42<00:51,  1.23it/s] 88%|████████▊ | 439/500 [06:42<00:35,  1.70it/s] 88%|████████▊ | 441/500 [06:48<01:19,  1.34s/it] 89%|████████▊ | 443/500 [06:49<00:54,  1.04it/s] 89%|████████▉ | 445/500 [06:52<01:02,  1.13s/it] 89%|████████▉ | 447/500 [06:52<00:43,  1.23it/s] 90%|████████▉ | 449/500 [06:52<00:30,  1.70it/s] 90%|█████████ | 451/500 [06:58<01:04,  1.31s/it] 91%|█████████ | 453/500 [06:58<00:43,  1.07it/s] 91%|█████████ | 455/500 [07:01<00:49,  1.11s/it] 91%|█████████▏| 457/500 [07:01<00:34,  1.25it/s] 92%|█████████▏| 459/500 [07:01<00:23,  1.73it/s] 92%|█████████▏| 461/500 [07:07<00:50,  1.31s/it] 93%|█████████▎| 463/500 [07:08<00:34,  1.07it/s] 93%|█████████▎| 465/500 [07:11<00:38,  1.11s/it] 93%|█████████▎| 467/500 [07:11<00:26,  1.25it/s] 94%|█████████▍| 469/500 [07:11<00:17,  1.73it/s] 94%|█████████▍| 471/500 [07:17<00:39,  1.36s/it] 95%|█████████▍| 473/500 [07:17<00:26,  1.03it/s] 95%|█████████▌| 475/500 [07:21<00:29,  1.17s/it] 95%|█████████▌| 477/500 [07:21<00:19,  1.19it/s] 96%|█████████▌| 479/500 [07:21<00:12,  1.64it/s] 96%|█████████▌| 481/500 [07:27<00:25,  1.33s/it] 97%|█████████▋| 483/500 [07:27<00:16,  1.05it/s]Epoch:  425  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  426  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  427  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  428  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  429  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  430  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  431  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  432  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  433  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  434  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  435  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  436  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  437  	Training Loss: 0.19853278994560242
Test Loss:  0.26670020818710327
Valid Loss:  0.2587988078594208
Epoch:  438  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  439  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  440  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  441  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  442  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  443  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  444  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  445  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  446  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  447  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  448  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  449  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  450  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  451  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  452  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  453  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  454  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  455  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  456  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  457  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  458  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  459  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  460  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  461  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  462  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  463  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  464  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  465  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  466  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  467  	Training Loss: 0.19853276014328003
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  468  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  469  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  470  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
**************************************************learning rate decay**************************************************
Epoch:  471  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  472  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  473  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  474  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  475  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  476  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  477  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  478  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  479  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  480  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  481  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  482  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  483  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  484  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
 97%|█████████▋| 485/500 [07:30<00:16,  1.12s/it] 97%|█████████▋| 487/500 [07:30<00:10,  1.25it/s] 98%|█████████▊| 489/500 [07:30<00:06,  1.72it/s] 98%|█████████▊| 491/500 [07:37<00:11,  1.33s/it] 99%|█████████▊| 493/500 [07:37<00:06,  1.05it/s] 99%|█████████▉| 495/500 [07:40<00:05,  1.13s/it] 99%|█████████▉| 497/500 [07:40<00:02,  1.23it/s]100%|█████████▉| 499/500 [07:40<00:00,  1.70it/s]100%|██████████| 500/500 [07:43<00:00,  1.08it/s]
Epoch:  485  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  486  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  487  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  488  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  489  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  490  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  491  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  492  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  493  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  494  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001485824585
Valid Loss:  0.25879883766174316
Epoch:  495  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
Epoch:  496  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.25879883766174316
Epoch:  497  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
Epoch:  498  	Training Loss: 0.19853277504444122
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  499  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001783847809
Valid Loss:  0.2587988078594208
Epoch:  500  	Training Loss: 0.19853278994560242
Test Loss:  0.2667001485824585
Valid Loss:  0.2587988078594208
**************************************************learning rate decay**************************************************
