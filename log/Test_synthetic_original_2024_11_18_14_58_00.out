/nishome/yui/ModifiedNGD/utils/readData.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)
Train info: 
 train data shape: torch.Size([512, 1]), 
 train lable shape: torch.Size([512, 1]), 
 positive / negative: 0.021206805482506752 / 0.978793203830719, 
 discrepancy norm error: 5.045955617788422e-07
Test info: 
 test data shape: torch.Size([128, 1]), 
 test lable shape: torch.Size([128, 1]), , 
 positive / negative: -0.030759211629629135 / 1.0307592153549194, 
 discrepancy norm error: 2.750667249529215e-07
Valid info: 
 valid data shape: torch.Size([128, 1]), valid lable shape: torch.Size([128, 1]), 
 positive / negative: 0.015859205275774002 / 0.9841408133506775, 
 discrepancy norm error: 2.3576444618811365e-07
torch.Size([512, 1]) torch.Size([512])
seed is  2191
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/1 [00:00<?, ?it/s]/nishome/yui/anaconda3/envs/ng/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971214/work/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch:   1
train data shape:  torch.Size([512, 1]) torch.Size([512, 1]) torch.Size([512])
LOSS BY ALPHA:  tensor(0.0702, device='cuda:0') 512
max of Lambda2 tensor(1000., device='cuda:0')
min of Lambda2 tensor(0.0100, device='cuda:0')
eigenvalues preserved:  512
Test train Loss:  0.05464249104261398
Test train Acc:  0.0
Test Loss:  0.0616748183965683
Test Acc:  0.0
Valid Loss:  0.05691187083721161
Valid Acc:  0.0
LOSS:  tensor(0.0546, device='cuda:0', grad_fn=<MulBackward0>) alpha^2/2N:  torch.Size([512]) torch.Size([512, 1]) tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0702, device='cuda:0') tensor([[-9.8547e-02],
        [-1.5495e-01],
        [-2.2396e-01],
        [ 1.0922e-01],
        [ 4.8215e-02],
        [-2.8265e-01],
        [ 5.2699e-02],
        [-3.9560e-02],
        [ 1.6843e-01],
        [-6.6058e-02],
        [ 8.4438e-02],
        [-1.0274e-01],
        [-1.4329e-02],
        [ 2.1230e-01],
        [ 1.3506e-01],
        [-1.4319e-02],
        [-2.3046e-01],
        [ 1.0954e-01],
        [ 1.9453e-01],
        [ 3.2196e-02],
        [-1.5811e-01],
        [-1.6750e-01],
        [ 1.2426e-01],
        [-7.1416e-02],
        [-1.6192e-02],
        [-2.3939e-01],
        [-1.0182e-02],
        [ 1.7215e-02],
        [ 1.0831e-01],
        [ 1.9836e-01],
        [-2.0159e-01],
        [ 1.5881e-01],
        [-1.0066e-01],
        [-5.7693e-02],
        [-1.7994e-02],
        [-6.5612e-02],
        [ 1.6054e-01],
        [-4.7818e-02],
        [ 2.0589e-01],
        [ 3.0469e-02],
        [ 2.7202e-02],
        [ 7.8995e-02],
        [ 4.7440e-03],
        [ 8.6353e-02],
        [-5.0531e-02],
        [-1.8116e-01],
        [-7.1657e-02],
        [-1.9311e-01],
        [ 1.1848e-01],
        [ 1.2123e-01],
        [-2.2786e-01],
        [-2.3554e-01],
        [-1.6624e-01],
        [-1.1796e-01],
        [-1.8340e-01],
        [ 4.8783e-02],
        [ 4.5240e-04],
        [ 1.9547e-01],
        [-2.9418e-01],
        [-4.8836e-02],
        [-4.6133e-02],
        [-1.4235e-02],
        [-2.4138e-01],
        [-2.0820e-01],
        [ 2.3605e-01],
        [ 1.5186e-03],
        [ 1.7080e-02],
        [ 2.9190e-03],
        [-3.3423e-02],
        [ 3.2143e-03],
        [-4.0717e-03],
        [-9.7715e-03],
        [ 4.5244e-02],
        [-1.1176e-01],
        [-5.1377e-02],
        [-1.1353e-01],
        [-4.9858e-02],
        [-4.5814e-03],
        [-3.2553e-03],
        [-2.7570e-01],
        [-8.1264e-02],
        [-2.6370e-02],
        [-7.3874e-02],
        [-7.3054e-02],
        [ 2.3297e-01],
        [-2.0680e-01],
        [-4.1899e-02],
        [-2.2586e-01],
        [ 1.7307e-01],
        [ 7.6790e-02],
        [ 6.3954e-02],
        [-9.4269e-02],
        [-3.3607e-02],
        [ 2.3896e-01],
        [ 1.9176e-01],
        [ 2.7423e-01],
        [ 2.5264e-01],
        [ 1.2180e-01],
        [-2.1221e-01],
        [ 1.5517e-01],
        [ 2.5869e-01],
        [ 2.7733e-01],
        [ 2.4435e-02],
        [-2.6102e-01],
        [ 1.2638e-01],
        [ 1.5523e-01],
        [-9.5767e-03],
        [ 1.0109e-03],
        [ 1.5517e-01],
        [ 2.3934e-01],
        [ 1.1547e-01],
        [ 2.2727e-02],
        [-5.4610e-02],
        [ 7.9171e-02],
        [-4.9132e-02],
        [-2.7847e-01],
        [ 1.5049e-01],
        [-1.1708e-02],
        [ 8.1609e-02],
        [ 1.1433e-01],
        [ 1.3054e-01],
        [-8.3588e-02],
        [-8.3333e-02],
        [-1.2266e-01],
        [-1.1233e-01],
        [ 8.2948e-02],
        [-1.8935e-01],
        [-2.8153e-01],
        [ 2.7469e-01],
        [-3.7057e-02],
        [-2.4184e-02],
        [ 9.1407e-02],
        [ 1.7510e-01],
        [ 2.8243e-01],
        [-1.8015e-01],
        [-4.4190e-02],
        [ 8.1325e-03],
        [-1.1761e-01],
        [ 1.8857e-01],
        [ 1.5483e-01],
        [-2.6880e-01],
        [ 8.7675e-02],
        [-7.0275e-02],
        [-3.2877e-02],
        [ 2.8682e-01],
        [-2.7083e-02],
        [ 2.2024e-01],
        [ 1.0903e-01],
        [-7.0722e-02],
        [ 2.1025e-01],
        [ 2.3171e-02],
        [-1.0904e-01],
        [-2.5440e-01],
        [-6.5285e-03],
        [-2.1109e-01],
        [-5.5677e-04],
        [ 1.3714e-01],
        [-2.8264e-02],
        [ 3.1501e-02],
        [-1.5483e-01],
        [-8.2610e-02],
        [ 1.0845e-01],
        [ 1.4478e-03],
        [-1.6376e-01],
        [-1.5521e-02],
        [-8.1623e-03],
        [ 1.7087e-01],
        [-8.3207e-02],
        [-6.2783e-02],
        [-2.0449e-01],
        [ 1.3626e-01],
        [ 1.0467e-02],
        [-2.2599e-01],
        [-8.8669e-02],
        [ 9.9014e-02],
        [ 6.2751e-03],
        [-1.0223e-01],
        [-1.2754e-01],
        [ 2.7628e-02],
        [-1.7432e-01],
        [ 6.8232e-02],
        [-1.0221e-01],
        [ 8.3715e-03],
        [ 2.3436e-01],
        [ 6.2860e-02],
        [ 1.8050e-01],
        [-2.0854e-01],
        [-1.6046e-01],
        [-5.3600e-02],
        [ 1.2902e-02],
        [ 7.7997e-02],
        [-1.0963e-01],
        [-2.3431e-01],
        [-1.4199e-01],
        [ 5.1482e-02],
        [-2.2947e-01],
        [-2.4800e-01],
        [ 1.5632e-01],
        [ 1.1991e-02],
        [ 5.0305e-03],
        [ 2.0770e-02],
        [ 1.9601e-02],
        [-4.5686e-02],
        [-9.1692e-02],
        [-2.4870e-01],
        [ 4.1260e-03],
        [ 1.3526e-01],
        [ 7.8101e-02],
        [-1.1996e-01],
        [-9.5239e-03],
        [-3.6481e-04],
        [ 1.2979e-01],
        [-2.3738e-02],
        [-1.2511e-01],
        [ 3.5982e-02],
        [ 3.6031e-02],
        [-8.2441e-03],
        [ 9.3314e-02],
        [ 1.0627e-01],
        [-1.2334e-01],
        [-2.8634e-02],
        [ 6.3687e-03],
        [-2.3271e-01],
        [-2.5239e-01],
        [-1.7566e-01],
        [ 2.6208e-01],
        [ 1.4050e-02],
        [ 7.1753e-03],
        [-5.1549e-02],
        [ 1.4499e-01],
        [-9.2653e-02],
        [-1.0586e-01],
        [ 1.3995e-01],
        [ 2.8672e-02],
        [-1.5053e-01],
        [-2.2221e-01],
        [ 2.2691e-02],
        [ 2.1384e-01],
        [-1.3879e-01],
        [ 1.6185e-02],
        [ 2.5313e-01],
        [ 3.6767e-02],
        [ 2.7227e-01],
        [ 1.5875e-02],
        [ 9.3182e-02],
        [-3.1677e-02],
        [-7.1539e-02],
        [ 8.6766e-02],
        [ 9.2092e-02],
        [-2.3349e-01],
        [ 9.8140e-02],
        [-2.4583e-01],
        [ 0.0000e+00],
        [ 8.0512e-02],
        [ 2.0133e-01],
        [ 8.0494e-02],
        [ 7.5638e-02],
        [-2.8689e-02],
        [-1.4269e-01],
        [-4.9443e-02],
        [ 8.2847e-02],
        [ 2.1652e-01],
        [-1.6473e-01],
        [-1.5513e-01],
        [ 6.1290e-03],
        [-2.7664e-01],
        [ 5.3055e-02],
        [ 2.1360e-03],
        [ 2.4987e-01],
        [-5.2545e-02],
        [-2.0058e-01],
        [-2.3630e-01],
        [-1.3790e-01],
        [ 1.7941e-01],
        [-1.9260e-01],
        [-2.5397e-01],
        [ 2.0409e-01],
        [ 1.1244e-01],
        [-2.3487e-01],
        [-1.9583e-02],
        [ 1.1350e-01],
        [-4.0448e-02],
        [ 4.2973e-03],
        [ 1.7843e-02],
        [-1.4157e-01],
        [-8.7973e-02],
        [ 1.9908e-01],
        [-6.3253e-02],
        [ 1.0792e-01],
        [-2.3144e-01],
        [-1.2919e-03],
        [ 4.1660e-02],
        [ 1.5430e-01],
        [ 5.0952e-02],
        [ 1.7542e-01],
        [-2.0897e-02],
        [-7.3385e-02],
        [-1.0798e-02],
        [ 1.8288e-02],
        [-2.6945e-02],
        [ 1.6610e-03],
        [-1.1076e-03],
        [-1.1324e-01],
        [-1.2780e-01],
        [-5.7391e-02],
        [ 1.7151e-01],
        [ 1.0252e-02],
        [ 6.6029e-02],
        [ 2.8051e-01],
        [ 4.3485e-03],
        [-1.0282e-02],
        [ 8.6976e-03],
        [-5.4800e-02],
        [-9.0169e-02],
        [-1.6431e-01],
        [ 1.6586e-01],
        [ 2.1416e-01],
        [ 2.1827e-01],
        [ 2.0055e-01],
        [ 2.0638e-01],
        [-4.3315e-02],
        [ 1.4722e-01],
        [-2.5828e-02],
        [ 1.2404e-01],
        [-1.7739e-01],
        [-1.5192e-01],
        [ 2.5704e-02],
        [-4.3143e-02],
        [-2.3581e-01],
        [ 7.3911e-02],
        [-2.0584e-01],
        [ 1.7139e-01],
        [-1.1298e-01],
        [ 1.9632e-01],
        [ 7.8448e-02],
        [ 2.6058e-01],
        [-1.3587e-01],
        [ 1.0749e-01],
        [ 2.5974e-02],
        [-2.1592e-02],
        [-1.8628e-01],
        [-1.3540e-01],
        [ 1.5748e-01],
        [ 7.9912e-02],
        [ 1.5183e-01],
        [-1.6635e-01],
        [-2.8557e-01],
        [ 2.7455e-02],
        [-9.5649e-02],
        [-9.6349e-02],
        [ 7.9806e-03],
        [-2.6264e-01],
        [ 2.5279e-01],
        [ 1.8254e-01],
        [ 2.1853e-01],
        [-2.5442e-01],
        [-8.6257e-02],
        [ 2.6549e-01],
        [ 2.6515e-01],
        [ 9.6293e-02],
        [ 5.5211e-02],
        [-2.2129e-01],
        [-5.3984e-02],
        [ 2.3089e-02],
        [-2.1744e-01],
        [ 4.8821e-02],
        [ 1.6290e-01],
        [ 1.3162e-01],
        [-3.1058e-03],
        [ 8.1649e-02],
        [ 1.8362e-02],
        [ 1.7592e-01],
        [ 8.4588e-02],
        [-4.2499e-02],
        [-7.8959e-03],
        [-2.5399e-02],
        [ 4.8998e-03],
        [ 1.3531e-01],
        [ 8.2431e-02],
        [ 2.4697e-01],
        [-1.3244e-01],
        [ 2.5666e-01],
        [-7.8476e-02],
        [-3.8365e-02],
        [-2.6829e-01],
        [-8.7670e-03],
        [ 2.0512e-01],
        [ 1.2879e-01],
        [-1.4761e-01],
        [ 2.3691e-01],
        [-4.8003e-02],
        [-2.3316e-01],
        [ 9.7141e-02],
        [-7.5823e-02],
        [-6.2622e-02],
        [ 2.5941e-01],
        [ 3.9201e-02],
        [-3.3601e-02],
        [ 1.6064e-01],
        [-6.0582e-02],
        [ 4.4400e-03],
        [ 1.7584e-01],
        [-6.8244e-03],
        [-1.3787e-01],
        [ 1.1561e-01],
        [ 1.1154e-01],
        [ 2.7968e-01],
        [-1.3480e-01],
        [ 1.3857e-01],
        [-3.6016e-02],
        [-2.7509e-02],
        [-9.3208e-02],
        [ 1.3300e-01],
        [ 2.4531e-03],
        [-9.9991e-02],
        [ 1.0147e-01],
        [-5.4453e-02],
        [ 2.8236e-01],
        [-1.0985e-01],
        [-6.7864e-02],
        [-1.7225e-01],
        [ 2.2750e-01],
        [-6.5884e-03],
        [ 4.8691e-02],
        [-1.8331e-01],
        [ 2.0753e-04],
        [ 7.1666e-02],
        [-1.7514e-01],
        [-1.8258e-01],
        [-2.4540e-01],
        [ 6.6715e-02],
        [-1.2591e-01],
        [ 3.0574e-02],
        [-1.5273e-01],
        [-1.0912e-01],
        [ 2.6641e-01],
        [-2.8203e-01],
        [ 4.2713e-03],
        [-6.2700e-02],
        [ 5.3705e-02],
        [ 1.2111e-01],
        [ 9.4633e-02],
        [ 2.4053e-01],
        [ 1.5012e-01],
        [-9.1834e-03],
        [-9.3533e-02],
        [ 2.8334e-02],
        [-1.2563e-01],
        [ 6.9178e-02],
        [ 6.4434e-02],
        [ 3.4624e-03],
        [ 2.4689e-01],
        [ 5.5635e-03],
        [-5.9980e-02],
        [ 8.3504e-02],
        [ 1.0892e-01],
        [-3.4532e-03],
        [ 1.3487e-01],
        [ 2.5907e-01],
        [ 2.6931e-01],
        [ 1.4310e-01],
        [ 1.2310e-01],
        [-1.6290e-03],
        [-1.4665e-01],
        [-2.3596e-01],
        [-1.3749e-02],
        [ 4.8266e-03],
        [-1.2748e-02],
        [-4.8328e-02],
        [-2.3703e-01],
        [-2.4418e-01],
        [-2.1650e-02],
        [ 3.2728e-02],
        [-2.2751e-02],
        [ 2.1606e-01],
        [ 1.8376e-01],
        [ 2.0694e-02],
        [-5.1907e-02],
        [ 6.5181e-02],
        [ 2.3394e-02],
        [ 1.2099e-01],
        [-2.5605e-01],
        [-1.8884e-01],
        [-7.3468e-03],
        [-1.3185e-01],
        [ 1.2943e-01],
        [ 1.7761e-02],
        [-5.3580e-02],
        [-1.3930e-02],
        [-1.1675e-01],
        [-2.5000e-01],
        [ 3.7577e-02],
        [-1.4312e-01],
        [ 7.0129e-02],
        [ 2.6625e-01],
        [ 1.4973e-01],
        [-3.3498e-02],
        [-1.1461e-01],
        [ 8.4707e-02],
        [ 3.0068e-01],
        [ 8.5623e-02],
        [-7.8030e-03],
        [ 5.4140e-02],
        [-1.5015e-01],
        [-2.9335e-01],
        [ 9.0831e-02],
        [-1.8657e-01],
        [ 1.0767e-01],
        [-6.7313e-02],
        [ 1.4511e-01],
        [-8.4021e-02],
        [-4.6503e-02]], device='cuda:0', grad_fn=<SubBackward0>)
max of grad d_p:  tensor(0.0106, device='cuda:0')
min of grad d_p:  tensor(-0.0819, device='cuda:0')
J_L:  tensor([[-4.8385e-05],
        [ 1.2540e-03],
        [ 3.3021e-05],
        ...,
        [ 2.5367e-03],
        [ 1.9587e-03],
        [-8.1886e-02]], device='cuda:0') 
Jta:  tensor([-8.0972e-06,  4.8762e-03, -1.2125e-05,  ...,  6.6531e-03,
         6.5654e-03, -8.1886e-02], device='cuda:0')
max|min: (J_L, Jta/N)  (0.01059434562921524, 0.05017261207103729, ratio: 4.7357916831970215)|(-0.08188621699810028, -0.08188624680042267)

 check Jacobi res:  torch.Size([532609]) max:  tensor(0.0594, device='cuda:0') mean:  tensor(-0.0002, device='cuda:0') min:  tensor(-0.0396, device='cuda:0') norm:  tensor(1.2835, device='cuda:0') MSE:  tensor(2.4099e-06, device='cuda:0')
BAD Jacobian OCCURS!

 check NTK dimension reduction res:  torch.Size([532609, 1]) max:  tensor(0.0003, device='cuda:0') mean:  tensor(3.6665e-06, device='cuda:0') min:  tensor(0., device='cuda:0') norm:  tensor(0.0061, device='cuda:0') MSE:  tensor(1.1442e-08, device='cuda:0')
Shape check:  torch.Size([532609, 1])
max of d_p_list:  tensor(0.0094, device='cuda:0')
min of d_p_list:  tensor(-0.0047, device='cuda:0')
Epoch:  1  
Training Loss: 0.054250793167739175
Test Loss:  0.06109918653964996
Test Acc:  0.0
Valid Loss:  0.05642188340425491
Valid Acc:  0.0
local minima detector shape:  (0,)
Preserved_eigens number check:  512
100%|██████████| 1/1 [00:02<00:00,  2.49s/it]100%|██████████| 1/1 [00:02<00:00,  2.49s/it]
