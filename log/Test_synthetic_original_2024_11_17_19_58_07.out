/nishome/yui/ModifiedNGD/utils/modified_fisher_inverse.py:168: SyntaxWarning: invalid escape sequence '\s'
  '''
/nishome/yui/ModifiedNGD/utils/readData.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)
Train info: 
 train data shape: torch.Size([512, 1]), 
 train lable shape: torch.Size([512, 1]), 
 positive / negative: 0.021206805482506752 / 0.978793203830719, 
 discrepancy norm error: 5.045955617788422e-07
Test info: 
 test data shape: torch.Size([128, 1]), 
 test lable shape: torch.Size([128, 1]), , 
 positive / negative: -0.030759211629629135 / 1.0307592153549194, 
 discrepancy norm error: 2.750667249529215e-07
Valid info: 
 valid data shape: torch.Size([128, 1]), valid lable shape: torch.Size([128, 1]), 
 positive / negative: 0.015859205275774002 / 0.9841408133506775, 
 discrepancy norm error: 2.3576444618811365e-07
torch.Size([512, 1]) torch.Size([512])
seed is  2191
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/1 [00:00<?, ?it/s]/nishome/yui/anaconda3/envs/ng/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971214/work/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch:   1
LOSS BY ALPHA:  tensor(6.1254, device='cuda:0')
max of Lambda2 tensor(540.3308, device='cuda:0')
min of Lambda2 tensor(0.0100, device='cuda:0')
eigenvalues preserved:  512
Test train Loss:  0.14700695872306824
Test train Acc:  0.0
Test Loss:  0.1684994399547577
Test Acc:  0.0
Valid Loss:  0.14777939021587372
Valid Acc:  0.0
LOSS:  tensor(0.1470, device='cuda:0', grad_fn=<MulBackward0>)
max of grad d_p:  tensor(0.4087, device='cuda:0')
min of grad d_p:  tensor(-0.0670, device='cuda:0')
J_L:  tensor([[-3.6956e-04],
        [ 5.9764e-03],
        [-4.2803e-05],
        [-8.6730e-04],
        [-1.9192e-05],
        [ 1.2872e-03],
        [-2.8445e-05],
        [-1.1580e-04],
        [ 6.3527e-05],
        [-1.4054e-03],
        [ 9.2421e-06],
        [ 1.3501e-04],
        [-2.8522e-04],
        [ 2.4842e-02],
        [-5.8786e-05],
        [-1.6543e-03],
        [-8.5565e-04],
        [ 1.0934e-02],
        [-2.7735e-03],
        [ 3.2603e-03],
        [-7.7075e-04],
        [ 6.3816e-04],
        [ 3.9007e-02],
        [ 1.0467e-02],
        [-5.1574e-02],
        [-6.7022e-02],
        [ 4.0870e-01]], device='cuda:0') 
Jta:  tensor([-1.2487e-04,  4.2916e-03, -7.6699e-05, -4.8971e-04,  4.2268e-04,
         1.0959e-03, -8.8089e-05,  1.0575e-05,  3.0604e-05, -1.2433e-03,
         3.6264e-05,  1.4960e-04, -8.5643e-05,  1.7974e-02, -2.0220e-04,
        -1.4489e-03, -9.8324e-04,  9.5773e-03, -2.9444e-03,  2.1016e-03,
        -9.4274e-04,  4.0860e-04,  3.7244e-02,  1.1402e-02, -5.2989e-02,
        -6.6610e-02,  4.0870e-01], device='cuda:0')
max|min: (J_L, Jta/N)  (0.4087013304233551, 0.4087013006210327, ratio: 0.9999999403953552)|(-0.06702195107936859, -0.06660963594913483)

 check Jacobi res:  torch.Size([27]) max:  tensor(0.0069, device='cuda:0') mean:  tensor(0.0005, device='cuda:0') min:  tensor(-0.0009, device='cuda:0') norm:  tensor(0.0078, device='cuda:0') MSE:  tensor(0.0003, device='cuda:0')
BAD Jacobian OCCURS!

 check NTK dimension reduction res:  torch.Size([27, 1]) max:  tensor(0.0002, device='cuda:0') mean:  tensor(6.1935e-05, device='cuda:0') min:  tensor(2.8339e-07, device='cuda:0') norm:  tensor(0.0004, device='cuda:0') MSE:  tensor(1.6371e-05, device='cuda:0')
Shape check:  torch.Size([27, 1])
max of d_p_list:  tensor(0.1081, device='cuda:0')
min of d_p_list:  tensor(-0.2106, device='cuda:0')
Epoch:  1  
Training Loss: 0.1466680541052483
Test Loss:  0.16795691847801208
Test Acc:  0.0
Valid Loss:  0.14719629287719727
Valid Acc:  0.0
local minima detector shape:  (0,)
Preserved_eigens number check:  512
100%|██████████| 1/1 [00:02<00:00,  2.53s/it]100%|██████████| 1/1 [00:02<00:00,  2.53s/it]
