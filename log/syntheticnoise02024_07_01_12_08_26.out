/nishome/yui/ModifiedNGD/utils/readData.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)
/nishome/yui/anaconda3/envs/ng/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971214/work/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Train info: 
 train data shape: torch.Size([256, 2]), 
 train lable shape: torch.Size([256, 1]), 
 positive / negative: 0.3337317407131195 / 0.6662682294845581
Test info: 
 test data shape: torch.Size([64, 2]), 
 test lable shape: torch.Size([64, 1]), , 
 positive / negative: 0.3785669207572937 / 0.6214330792427063
Valid info: 
 valid data shape: torch.Size([64, 2]), valid lable shape: torch.Size([64, 1]), 
 positive / negative: 0.345711886882782 / 0.654288113117218
seed is  1
---------------------------------------- MNGD ----------------------------------------
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:06<31:17,  6.28s/it]  1%|          | 2/300 [00:12<32:22,  6.52s/it]/nishome/yui/ModifiedNGD/utils/modified_fisher_inverse.py:80: UserWarning: torch.linalg.svd: During SVD computation with the selected cusolver driver, batches 0 failed to converge. A more accurate method will be used to compute the SVD as a fallback. Check doc at https://pytorch.org/docs/stable/generated/torch.linalg.svd.html (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971214/work/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp:697.)
  U ,Lambda, Vh = torch.linalg.svd(J)
  1%|          | 2/300 [00:17<43:00,  8.66s/it]
Epoch:  1  	Training Loss: 1.886734473053366
correct 0 vs. total 64
Test Acc:  0.0
Valid Acc:  0.0
local minima detector shape:  (0,)
Epoch:  2  	Training Loss: 9154.972303152084
correct 0 vs. total 64
Test Acc:  0.0
Valid Acc:  0.0
local minima detector shape:  (1,)
Epoch:  3  	Training Loss: inf
correct 0 vs. total 64
Test Acc:  0.0
Valid Acc:  0.0
Traceback (most recent call last):
  File "/nishome/yui/ModifiedNGD/train.py", line 334, in <module>
    train(model,mode, lr_decay=True)
  File "/nishome/yui/ModifiedNGD/train.py", line 181, in train
    F_inverse_modified, preserved_eigens = modified_Fisher_inverse(model=model, 
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nishome/yui/ModifiedNGD/utils/modified_fisher_inverse.py", line 80, in modified_Fisher_inverse
    U ,Lambda, Vh = torch.linalg.svd(J)
                    ^^^^^^^^^^^^^^^^^^^
torch._C._LinAlgError: linalg.svd: The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated singular values (error code: 255).
