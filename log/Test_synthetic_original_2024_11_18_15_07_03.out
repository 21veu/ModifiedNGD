/nishome/yui/ModifiedNGD/utils/readData.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)
Train info: 
 train data shape: torch.Size([512, 1]), 
 train lable shape: torch.Size([512, 1]), 
 positive / negative: 0.021206805482506752 / 0.978793203830719, 
 discrepancy norm error: 5.045955617788422e-07
Test info: 
 test data shape: torch.Size([128, 1]), 
 test lable shape: torch.Size([128, 1]), , 
 positive / negative: -0.030759211629629135 / 1.0307592153549194, 
 discrepancy norm error: 2.750667249529215e-07
Valid info: 
 valid data shape: torch.Size([128, 1]), valid lable shape: torch.Size([128, 1]), 
 positive / negative: 0.015859205275774002 / 0.9841408133506775, 
 discrepancy norm error: 2.3576444618811365e-07
torch.Size([512, 1]) torch.Size([512])
seed is  2191
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/1 [00:00<?, ?it/s]/nishome/yui/anaconda3/envs/ng/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971214/work/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch:   1
train data shape:  torch.Size([512, 1]) torch.Size([512, 1]) torch.Size([512])
LOSS BY ALPHA:  tensor(0.0546, device='cuda:0') 512
max of Lambda2 tensor(1000., device='cuda:0')
min of Lambda2 tensor(0.0100, device='cuda:0')
eigenvalues preserved:  512
Test train Loss:  0.05464249104261398
Test train Acc:  0.0
Test Loss:  0.0616748183965683
Test Acc:  0.0
Valid Loss:  0.05691187083721161
Valid Acc:  0.0
LOSS:  tensor(0.0546, device='cuda:0', grad_fn=<MulBackward0>) alpha^2/2N:  torch.Size([512]) torch.Size([512, 1]) tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0546, device='cuda:0') tensor([[ 3.9913e-02],
        [ 6.3887e-02],
        [-1.9398e-01],
        [ 1.0512e-01],
        [ 1.0406e-01],
        [ 3.2945e-03],
        [-3.2100e-02],
        [-4.7138e-02],
        [ 2.5690e-01],
        [-5.4896e-03],
        [ 1.6205e-01],
        [ 2.6902e-02],
        [-5.3027e-03],
        [ 1.2842e-01],
        [-1.3890e-01],
        [-2.7247e-03],
        [-1.0713e-01],
        [ 1.1235e-01],
        [ 2.1629e-01],
        [ 1.1119e-01],
        [ 6.6932e-04],
        [ 4.1521e-02],
        [ 6.9401e-02],
        [-4.6387e-04],
        [-1.0781e-01],
        [-2.7844e-01],
        [-1.2616e-01],
        [ 1.6068e-02],
        [-4.6048e-02],
        [ 1.5702e-01],
        [ 4.5038e-03],
        [ 4.8573e-02],
        [-8.9414e-02],
        [-2.6897e-01],
        [-1.9000e-01],
        [-3.3660e-02],
        [ 2.9516e-02],
        [-1.2180e-01],
        [ 2.0745e-01],
        [-1.5667e-01],
        [-2.1975e-01],
        [ 6.7082e-02],
        [ 1.2023e-02],
        [ 7.3127e-02],
        [ 1.5978e-01],
        [ 5.3861e-02],
        [ 2.4582e-02],
        [-2.2592e-01],
        [ 1.2453e-01],
        [-2.7690e-02],
        [ 6.9917e-03],
        [-1.3783e-01],
        [-2.6520e-01],
        [ 2.9846e-03],
        [ 8.1134e-02],
        [-2.4747e-02],
        [ 3.2555e-03],
        [ 1.0219e-01],
        [-7.8182e-02],
        [-6.5499e-02],
        [-3.4501e-02],
        [ 2.1838e-01],
        [-1.5325e-01],
        [-1.5967e-01],
        [ 1.8580e-01],
        [-2.3076e-01],
        [-2.7491e-01],
        [-2.7996e-01],
        [-4.2158e-02],
        [ 3.7256e-03],
        [-4.5891e-03],
        [-5.4140e-02],
        [ 5.4856e-02],
        [-1.2042e-02],
        [ 2.1966e-01],
        [-2.8252e-01],
        [-2.2970e-01],
        [ 2.9093e-01],
        [-8.4760e-02],
        [ 2.9647e-04],
        [ 9.4197e-02],
        [ 4.8400e-02],
        [ 1.4266e-02],
        [-2.6228e-02],
        [ 9.3691e-02],
        [ 3.1835e-02],
        [ 1.6017e-01],
        [-1.0617e-01],
        [ 1.6723e-01],
        [ 2.6327e-01],
        [ 1.8406e-01],
        [ 6.0539e-04],
        [ 2.7944e-03],
        [-1.2537e-02],
        [ 2.3102e-01],
        [ 1.8091e-01],
        [ 1.2558e-01],
        [ 8.8871e-02],
        [-1.3040e-02],
        [-1.0496e-01],
        [ 1.9005e-01],
        [ 2.7086e-01],
        [-5.0944e-02],
        [-1.7810e-01],
        [ 2.0083e-01],
        [ 2.3176e-01],
        [-2.6403e-01],
        [ 2.1248e-01],
        [ 1.4583e-01],
        [ 2.5524e-01],
        [ 7.5315e-02],
        [ 1.7916e-01],
        [-7.4469e-02],
        [ 2.3347e-01],
        [ 3.1036e-02],
        [-1.2581e-01],
        [-5.4718e-02],
        [-1.2827e-01],
        [-1.4404e-01],
        [-1.6390e-01],
        [ 3.8663e-02],
        [-1.8828e-01],
        [-2.6860e-01],
        [-2.6355e-01],
        [ 8.6559e-03],
        [ 9.9664e-02],
        [-1.1936e-01],
        [ 1.0588e-04],
        [ 2.4865e-01],
        [-4.2017e-02],
        [-7.4031e-02],
        [ 1.9994e-01],
        [-5.5413e-02],
        [ 2.0244e-01],
        [-1.1108e-01],
        [ 1.2241e-02],
        [-1.4207e-01],
        [ 1.7327e-01],
        [ 6.2013e-02],
        [ 2.9212e-01],
        [ 5.7203e-03],
        [ 8.6640e-02],
        [ 8.0052e-02],
        [ 2.4151e-01],
        [ 1.7859e-01],
        [-2.1529e-01],
        [ 2.1854e-01],
        [ 1.0852e-01],
        [ 9.1724e-02],
        [-2.4772e-02],
        [ 1.4038e-02],
        [ 4.9025e-02],
        [-1.3950e-01],
        [ 2.5833e-01],
        [-3.0814e-03],
        [-1.5354e-01],
        [ 2.2497e-01],
        [-2.5277e-01],
        [ 2.5587e-01],
        [ 4.3476e-02],
        [-1.1917e-01],
        [ 2.2998e-01],
        [-6.5484e-02],
        [-9.6885e-02],
        [-6.4710e-02],
        [-8.1182e-02],
        [ 7.2217e-03],
        [-2.1063e-01],
        [ 2.2384e-01],
        [-4.9421e-02],
        [ 8.0119e-02],
        [ 2.0809e-01],
        [ 1.1765e-02],
        [ 2.3344e-03],
        [ 1.2037e-01],
        [-2.4462e-01],
        [ 1.2874e-01],
        [ 1.3880e-01],
        [ 1.4924e-01],
        [-8.4245e-02],
        [ 6.6540e-02],
        [-5.7971e-02],
        [-1.0871e-02],
        [ 5.3446e-02],
        [-1.2417e-01],
        [ 7.2687e-02],
        [-1.2950e-01],
        [-2.0374e-01],
        [ 1.7294e-01],
        [-1.3927e-01],
        [ 1.3617e-01],
        [-2.9341e-01],
        [-1.5201e-01],
        [ 6.8823e-02],
        [-1.0433e-01],
        [ 5.5399e-02],
        [-2.2615e-03],
        [ 1.6711e-01],
        [ 2.6656e-02],
        [-5.9280e-02],
        [ 6.2629e-02],
        [-1.1406e-01],
        [ 6.4428e-02],
        [-1.3298e-01],
        [-1.1278e-02],
        [ 5.4100e-03],
        [ 2.2941e-01],
        [ 1.1686e-02],
        [ 1.3415e-01],
        [-1.6588e-01],
        [-3.0914e-02],
        [-1.0398e-01],
        [-1.2127e-01],
        [-1.4236e-01],
        [ 1.7209e-01],
        [-1.1746e-01],
        [ 2.0419e-01],
        [-2.6373e-02],
        [ 1.8213e-01],
        [ 2.1083e-02],
        [-2.4191e-01],
        [-3.4488e-02],
        [-1.0272e-01],
        [-7.3937e-02],
        [-2.7688e-01],
        [ 2.4135e-01],
        [-5.1116e-02],
        [-9.6272e-02],
        [-2.9067e-01],
        [ 5.7646e-02],
        [-1.7525e-01],
        [-9.5874e-02],
        [ 3.4995e-02],
        [-1.4624e-01],
        [ 3.4132e-02],
        [ 4.9973e-02],
        [-1.9642e-01],
        [ 3.9930e-02],
        [-1.4401e-01],
        [ 6.1034e-02],
        [-2.0833e-02],
        [ 4.5467e-02],
        [ 2.6873e-01],
        [ 1.3991e-01],
        [ 8.0742e-02],
        [ 2.1563e-01],
        [-4.2067e-02],
        [ 2.1511e-01],
        [-9.5691e-02],
        [-2.3838e-01],
        [ 1.3975e-01],
        [-1.1875e-01],
        [-8.0826e-03],
        [ 7.4018e-02],
        [ 1.5883e-01],
        [ 8.0640e-02],
        [ 4.3995e-02],
        [ 2.6694e-01],
        [-9.2603e-03],
        [-2.8291e-01],
        [ 2.1082e-01],
        [ 6.4646e-02],
        [ 5.9850e-02],
        [-2.4680e-01],
        [-9.8438e-03],
        [-2.2796e-01],
        [ 3.0406e-02],
        [ 1.3472e-01],
        [ 2.4685e-01],
        [-2.0053e-01],
        [-4.8608e-02],
        [-1.3453e-01],
        [-4.9245e-04],
        [-4.2034e-02],
        [ 1.0250e-02],
        [-2.4896e-01],
        [ 9.4193e-02],
        [ 1.0382e-01],
        [ 3.5853e-04],
        [ 2.4254e-01],
        [-1.7363e-01],
        [-1.4636e-02],
        [-9.9718e-02],
        [-2.1576e-01],
        [-2.3335e-01],
        [ 2.8174e-03],
        [ 2.0127e-01],
        [-2.1977e-01],
        [ 1.6505e-02],
        [-2.6913e-01],
        [ 1.0150e-01],
        [ 9.3580e-02],
        [-8.5718e-03],
        [ 3.5614e-02],
        [-8.6686e-02],
        [-1.3116e-01],
        [-3.4253e-02],
        [-1.5381e-01],
        [-1.2491e-01],
        [-9.3223e-02],
        [-5.8316e-02],
        [-1.7671e-01],
        [ 2.8103e-02],
        [-2.4736e-01],
        [ 9.3909e-02],
        [ 7.0268e-02],
        [ 8.1524e-02],
        [-1.3881e-02],
        [ 4.7368e-02],
        [-2.0103e-01],
        [-1.4719e-01],
        [-6.5182e-02],
        [-1.2939e-01],
        [-2.6385e-01],
        [ 1.0165e-01],
        [-8.3182e-02],
        [ 1.2004e-01],
        [ 1.6831e-02],
        [-2.5636e-02],
        [ 6.9180e-02],
        [-2.1854e-01],
        [ 1.4800e-01],
        [-2.4017e-01],
        [-3.4207e-02],
        [-6.3873e-03],
        [-1.4542e-01],
        [ 1.4164e-02],
        [ 1.4324e-01],
        [-1.5586e-01],
        [ 6.7882e-02],
        [-2.0584e-01],
        [ 7.4170e-02],
        [ 1.5456e-01],
        [ 2.0449e-01],
        [-4.0132e-02],
        [ 3.4277e-02],
        [-1.3587e-01],
        [ 5.3330e-02],
        [-9.4556e-02],
        [ 1.1754e-02],
        [-5.0686e-02],
        [-2.0717e-01],
        [ 2.7578e-01],
        [ 1.2102e-01],
        [ 4.7162e-02],
        [ 8.2494e-03],
        [-2.2659e-01],
        [-2.1671e-01],
        [-2.5507e-01],
        [ 5.5959e-02],
        [ 4.9982e-03],
        [-7.9077e-02],
        [ 2.0225e-01],
        [ 1.7755e-01],
        [ 1.4834e-01],
        [ 1.7271e-03],
        [-5.5451e-02],
        [ 1.4029e-01],
        [ 2.6222e-01],
        [-5.4836e-02],
        [ 2.3247e-01],
        [ 1.8658e-02],
        [-8.6009e-02],
        [ 2.6409e-01],
        [-1.7656e-01],
        [ 4.3220e-02],
        [-5.4268e-02],
        [ 1.4255e-01],
        [-2.7746e-01],
        [ 3.6982e-02],
        [-1.2452e-01],
        [-1.1797e-01],
        [-2.0760e-01],
        [-9.5968e-04],
        [-2.2752e-03],
        [-4.8781e-02],
        [-9.2681e-02],
        [-7.4479e-03],
        [-4.7219e-02],
        [-1.9828e-02],
        [ 8.9035e-02],
        [ 9.0495e-02],
        [ 2.4676e-02],
        [-3.6118e-02],
        [ 2.4392e-02],
        [ 2.2921e-01],
        [ 2.0406e-01],
        [-1.4978e-01],
        [-1.7243e-01],
        [ 1.4087e-01],
        [ 1.9750e-03],
        [-2.6159e-01],
        [ 2.2389e-01],
        [-1.5100e-01],
        [-4.7291e-02],
        [ 2.1962e-01],
        [ 3.5632e-02],
        [ 9.3240e-02],
        [-4.3098e-03],
        [ 1.8073e-01],
        [-1.4000e-02],
        [ 1.6070e-01],
        [-2.0468e-02],
        [-2.1838e-01],
        [ 3.2346e-02],
        [-5.4811e-02],
        [ 2.9698e-01],
        [-1.9106e-01],
        [-3.6611e-02],
        [ 2.0715e-02],
        [-2.9301e-01],
        [ 5.1463e-02],
        [ 1.6708e-02],
        [-1.2817e-01],
        [-1.4492e-01],
        [-1.0649e-01],
        [ 1.5381e-01],
        [ 1.7729e-01],
        [-1.0528e-01],
        [ 1.5550e-01],
        [-6.5514e-02],
        [-5.5963e-02],
        [-1.5526e-01],
        [ 2.2707e-01],
        [-1.9423e-02],
        [ 8.2926e-03],
        [ 1.2672e-01],
        [-1.6239e-01],
        [-1.5622e-01],
        [-2.5991e-01],
        [ 1.4998e-01],
        [ 4.1401e-02],
        [-5.6951e-02],
        [-4.2546e-03],
        [ 2.9837e-02],
        [ 1.4181e-01],
        [-2.2745e-01],
        [-4.1626e-03],
        [ 6.6231e-02],
        [-3.2921e-02],
        [ 1.1151e-01],
        [ 2.7872e-01],
        [ 1.2480e-01],
        [ 1.2618e-01],
        [-1.5267e-02],
        [-5.0912e-02],
        [ 1.4460e-01],
        [ 7.2670e-02],
        [ 5.8981e-02],
        [ 2.5890e-01],
        [-3.6610e-02],
        [ 9.3672e-02],
        [ 5.5607e-03],
        [-1.6150e-01],
        [ 1.8767e-01],
        [ 8.9281e-02],
        [-2.3364e-01],
        [-3.9106e-02],
        [ 2.7167e-01],
        [ 1.9569e-01],
        [ 1.9069e-01],
        [ 1.7087e-01],
        [-2.2577e-02],
        [-6.4724e-02],
        [-6.1542e-02],
        [-2.7476e-01],
        [-1.5528e-01],
        [-1.4002e-02],
        [ 2.1863e-01],
        [-2.5424e-01],
        [-3.1033e-03],
        [-2.6379e-01],
        [-1.5807e-01],
        [-1.1592e-01],
        [ 2.8133e-01],
        [-3.3926e-02],
        [ 1.2947e-01],
        [ 2.0275e-01],
        [-8.5856e-02],
        [ 1.0611e-01],
        [-1.1723e-01],
        [-1.3807e-01],
        [-1.0206e-01],
        [-2.1471e-01],
        [-2.1644e-01],
        [ 3.5983e-02],
        [ 2.6989e-01],
        [ 1.5124e-01],
        [-7.5651e-02],
        [-7.3382e-02],
        [ 3.2925e-02],
        [-9.5633e-02],
        [-2.2127e-01],
        [ 3.6321e-02],
        [ 2.6985e-01],
        [ 1.9952e-01],
        [-1.0380e-01],
        [-1.3782e-02],
        [ 8.1858e-02],
        [ 2.5905e-01],
        [ 8.2243e-02],
        [ 1.4842e-02],
        [-1.7920e-01],
        [ 4.2081e-02],
        [-1.1869e-02],
        [-1.1216e-01],
        [ 1.0386e-01],
        [ 2.4005e-02],
        [-3.3232e-02],
        [-5.2828e-03],
        [ 9.8497e-02],
        [-2.0588e-01]], device='cuda:0', grad_fn=<SubBackward0>)
max of grad d_p:  tensor(0.0106, device='cuda:0')
min of grad d_p:  tensor(-0.0819, device='cuda:0')
J_L:  tensor([[-4.8385e-05],
        [ 1.2540e-03],
        [ 3.3021e-05],
        ...,
        [ 2.5367e-03],
        [ 1.9587e-03],
        [-8.1886e-02]], device='cuda:0') 
Jta:  tensor([-4.8385e-05,  1.2540e-03,  3.3021e-05,  ...,  2.5367e-03,
         1.9587e-03, -8.1886e-02], device='cuda:0')
max|min: (J_L, Jta/N)  (0.010594343766570091, 0.01059434562921524, ratio: 1.0000001192092896)|(-0.08188621699810028, -0.08188624680042267)

 check Jacobi res:  torch.Size([532609]) max:  tensor(2.9802e-08, device='cuda:0') mean:  tensor(-4.2317e-13, device='cuda:0') min:  tensor(-4.6566e-09, device='cuda:0') norm:  tensor(1.2011e-07, device='cuda:0') MSE:  tensor(2.2552e-13, device='cuda:0')

 check NTK dimension reduction res:  torch.Size([532609, 1]) max:  tensor(0.0003, device='cuda:0') mean:  tensor(3.8576e-06, device='cuda:0') min:  tensor(2.8422e-13, device='cuda:0') norm:  tensor(0.0064, device='cuda:0') MSE:  tensor(1.1976e-08, device='cuda:0')
Shape check:  torch.Size([532609, 1])
max of d_p_list:  tensor(0.0094, device='cuda:0')
min of d_p_list:  tensor(-0.0046, device='cuda:0')
Epoch:  1  
Training Loss: 0.05424960107484367
Test Loss:  0.06109817326068878
Test Acc:  0.0
Valid Loss:  0.05642057955265045
Valid Acc:  0.0
local minima detector shape:  (0,)
Preserved_eigens number check:  512
100%|██████████| 1/1 [00:02<00:00,  2.46s/it]100%|██████████| 1/1 [00:02<00:00,  2.46s/it]
