/nishome/yui/ModifiedNGD/utils/readData.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)
Train info: 
 train data shape: torch.Size([512, 1]), 
 train lable shape: torch.Size([512, 1]), 
 positive / negative: 0.021206805482506752 / 0.978793203830719, 
 discrepancy norm error: 5.045955617788422e-07
Test info: 
 test data shape: torch.Size([128, 1]), 
 test lable shape: torch.Size([128, 1]), , 
 positive / negative: -0.030759211629629135 / 1.0307592153549194, 
 discrepancy norm error: 2.750667249529215e-07
Valid info: 
 valid data shape: torch.Size([128, 1]), valid lable shape: torch.Size([128, 1]), 
 positive / negative: 0.015859205275774002 / 0.9841408133506775, 
 discrepancy norm error: 2.3576444618811365e-07
torch.Size([512, 1]) torch.Size([512])
seed is  2191
---------------------------------------- NGD ----------------------------------------
  0%|          | 0/1 [00:00<?, ?it/s]/nishome/yui/anaconda3/envs/ng/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971214/work/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch:   1
train data shape:  torch.Size([512, 1]) torch.Size([512, 1]) torch.Size([512])
LOSS BY ALPHA:  tensor(0.0702, device='cuda:0') 512
max of Lambda2 tensor(1000., device='cuda:0')
min of Lambda2 tensor(0.0100, device='cuda:0')
eigenvalues preserved:  512
Test train Loss:  0.05464249104261398
Test train Acc:  0.0
Test Loss:  0.0616748183965683
Test Acc:  0.0
Valid Loss:  0.05691187083721161
Valid Acc:  0.0
LOSS:  tensor(0.0546, device='cuda:0', grad_fn=<MulBackward0>) alpha^2/2N:  torch.Size([512]) torch.Size([512, 1]) tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0702, device='cuda:0') tensor([[-7.1513e-01],
        [ 4.8443e-02],
        [ 3.3087e-01],
        [-2.8997e-02],
        [ 8.2992e-01],
        [ 4.1761e-01],
        [-7.9621e-01],
        [ 6.2065e-01],
        [ 4.9886e-01],
        [ 1.6743e-01],
        [ 2.1021e-01],
        [-8.0636e-02],
        [ 2.1400e-02],
        [ 3.6115e-01],
        [ 1.4713e-01],
        [ 3.3599e-03],
        [-2.4621e-01],
        [-6.3523e-01],
        [ 6.2943e-01],
        [ 2.9411e-01],
        [-8.7837e-01],
        [-1.9713e-01],
        [-2.5220e-01],
        [ 2.1599e-01],
        [ 5.1858e-01],
        [-2.3523e-01],
        [-9.7823e-02],
        [-1.6477e-01],
        [-5.2546e-01],
        [ 1.9891e-01],
        [ 4.4704e-01],
        [ 1.3629e-01],
        [-1.0312e-01],
        [ 7.8309e-01],
        [ 8.8632e-01],
        [ 5.5141e-01],
        [ 2.1953e-01],
        [-5.7310e-01],
        [ 1.7337e-01],
        [-6.0449e-01],
        [-6.9228e-01],
        [ 5.0333e-01],
        [ 1.1470e-02],
        [ 1.7895e-01],
        [ 7.4403e-01],
        [ 1.3576e-01],
        [-1.7158e-01],
        [-1.8720e-01],
        [-1.4086e-01],
        [-5.7646e-01],
        [-2.2628e-01],
        [ 5.7073e-01],
        [-5.6729e-01],
        [-1.5987e-01],
        [-3.8045e-01],
        [ 4.5564e-01],
        [-8.9603e-01],
        [ 6.9046e-01],
        [-5.8089e-01],
        [-6.3298e-01],
        [-6.2821e-01],
        [ 1.3910e-01],
        [ 5.4105e-01],
        [ 6.5747e-02],
        [-4.5395e-01],
        [ 5.6318e-02],
        [-3.5836e-02],
        [-8.6648e-01],
        [ 5.8254e-02],
        [ 2.3728e-01],
        [-9.0949e-01],
        [ 2.7865e-01],
        [ 7.2738e-01],
        [-6.6687e-01],
        [ 4.5332e-01],
        [-2.9827e-01],
        [ 9.4225e-01],
        [-3.5111e-01],
        [-8.4120e-01],
        [ 3.3440e-01],
        [-2.4809e-01],
        [-2.9153e-01],
        [-4.8685e-02],
        [ 3.5520e-01],
        [-6.6093e-02],
        [ 6.8277e-01],
        [-3.3836e-02],
        [-1.6425e-01],
        [-5.6367e-01],
        [-1.2159e-01],
        [ 5.6256e-02],
        [-6.8215e-01],
        [-1.4005e-01],
        [-5.5102e-01],
        [ 1.0028e+00],
        [-4.8472e-01],
        [-2.6369e-02],
        [ 1.1184e+00],
        [ 5.3183e-01],
        [ 6.1346e-02],
        [-5.8707e-01],
        [-6.7950e-01],
        [ 6.6652e-02],
        [ 4.1807e-01],
        [ 1.9032e-01],
        [ 3.8369e-01],
        [-2.3626e-01],
        [-2.0603e-01],
        [-6.6740e-01],
        [ 9.3833e-01],
        [-3.9035e-02],
        [ 7.9876e-01],
        [-1.3958e-01],
        [ 5.8589e-01],
        [-2.1661e-01],
        [ 7.0730e-04],
        [ 2.5505e-01],
        [-8.9411e-01],
        [ 1.3363e-01],
        [-8.6609e-02],
        [-8.2876e-01],
        [-1.6291e-01],
        [-4.4736e-02],
        [-5.2724e-01],
        [-8.3049e-01],
        [-7.2620e-02],
        [-6.8128e-01],
        [ 3.6339e-01],
        [ 2.3178e-01],
        [ 7.8972e-02],
        [-9.2674e-01],
        [-8.6721e-02],
        [-3.5162e-01],
        [ 3.3288e-01],
        [ 7.6380e-02],
        [-5.6863e-02],
        [ 2.1188e-01],
        [-9.9002e-02],
        [-7.4941e-01],
        [-8.3009e-01],
        [-7.0735e-01],
        [-4.0813e-01],
        [-5.1097e-01],
        [ 3.7279e-01],
        [ 4.3573e-01],
        [ 7.0983e-01],
        [-1.2142e-01],
        [ 8.6299e-02],
        [-4.3545e-01],
        [ 2.0158e-01],
        [ 1.3508e-01],
        [-7.7686e-01],
        [ 6.8873e-01],
        [ 6.8356e-01],
        [-4.6274e-01],
        [ 6.4730e-02],
        [-6.0489e-01],
        [ 9.5626e-01],
        [ 9.3711e-01],
        [ 8.2673e-01],
        [-5.6475e-01],
        [-1.1015e-01],
        [-2.8661e-04],
        [-4.4205e-01],
        [ 2.0977e-01],
        [ 7.1545e-01],
        [ 2.9983e-02],
        [ 3.5801e-01],
        [-6.2200e-02],
        [-8.1789e-02],
        [-2.9432e-01],
        [-3.6536e-01],
        [-1.9299e-01],
        [ 1.3305e-01],
        [-1.6726e-03],
        [ 2.7939e-01],
        [ 3.2921e-01],
        [-5.6574e-02],
        [ 3.7399e-01],
        [ 6.5429e-02],
        [-7.1116e-01],
        [-6.3874e-02],
        [-8.1547e-01],
        [-7.3267e-01],
        [ 6.9071e-01],
        [ 9.1660e-02],
        [ 2.1898e-01],
        [-1.0212e-01],
        [-2.9970e-02],
        [-5.8521e-01],
        [ 1.4307e-01],
        [-1.9597e-02],
        [-3.3073e-01],
        [ 4.4004e-01],
        [-5.3804e-01],
        [ 2.6047e-01],
        [ 6.1736e-01],
        [-7.3939e-01],
        [ 1.9748e-01],
        [ 4.3974e-01],
        [-9.0469e-01],
        [ 2.3514e-01],
        [-2.7680e-01],
        [-1.3742e-01],
        [-4.2466e-02],
        [-1.3827e-01],
        [ 4.8273e-01],
        [ 1.1655e-01],
        [ 8.6254e-01],
        [ 2.2120e-01],
        [-9.2579e-01],
        [-8.2174e-01],
        [ 3.6807e-01],
        [ 5.4282e-01],
        [ 7.7771e-01],
        [-3.1754e-02],
        [ 2.3851e-01],
        [-8.9241e-01],
        [-4.2078e-01],
        [-4.2719e-01],
        [ 8.9910e-02],
        [-2.1477e-01],
        [-4.2512e-02],
        [ 6.9114e-01],
        [-8.1998e-01],
        [ 9.8553e-02],
        [ 3.7834e-02],
        [ 5.7258e-02],
        [ 6.9367e-01],
        [ 1.2837e-01],
        [-5.9631e-01],
        [-1.4422e-01],
        [-1.2044e-01],
        [-1.2394e-01],
        [-6.5281e-01],
        [ 3.9805e-01],
        [-4.5020e-01],
        [-6.7125e-01],
        [-7.6310e-01],
        [ 3.3434e-03],
        [ 7.9300e-01],
        [-7.7982e-02],
        [-6.1099e-01],
        [ 5.8295e-02],
        [ 3.3919e-01],
        [ 6.9331e-01],
        [-2.0177e-01],
        [-4.7751e-01],
        [ 3.6739e-01],
        [ 5.6744e-01],
        [-3.6591e-01],
        [ 7.0211e-01],
        [-4.2414e-01],
        [ 6.0813e-01],
        [ 2.3773e-01],
        [-5.0828e-01],
        [ 5.3523e-02],
        [ 5.5284e-01],
        [-9.8910e-01],
        [ 8.1253e-01],
        [-8.9062e-01],
        [ 9.7925e-01],
        [-1.3420e-01],
        [-2.1545e-01],
        [-3.0965e-01],
        [ 9.9370e-02],
        [-5.6643e-02],
        [-5.6776e-01],
        [ 1.7225e-01],
        [ 6.0054e-01],
        [-4.5621e-02],
        [ 3.6253e-01],
        [-1.2735e-01],
        [ 6.4610e-01],
        [-4.7042e-01],
        [-1.0103e-01],
        [-5.1527e-01],
        [ 5.2706e-01],
        [ 6.0409e-01],
        [ 6.3053e-01],
        [-6.4852e-01],
        [ 4.8767e-01],
        [ 1.2174e-02],
        [-9.7996e-01],
        [-1.3739e-01],
        [-7.5226e-02],
        [ 1.6638e-01],
        [ 1.8554e-01],
        [-7.9520e-01],
        [ 7.4634e-01],
        [-7.4299e-01],
        [ 4.1948e-02],
        [ 3.2504e-01],
        [ 4.7256e-01],
        [ 3.1255e-01],
        [ 6.1922e-01],
        [-2.8481e-01],
        [ 2.6798e-01],
        [-7.4960e-01],
        [ 7.3967e-01],
        [-1.9979e-01],
        [-4.4557e-01],
        [ 3.5094e-01],
        [-8.7579e-01],
        [-1.9733e-01],
        [ 6.0775e-01],
        [ 7.1410e-02],
        [-2.1049e-01],
        [ 6.4716e-01],
        [-3.7580e-01],
        [-2.1040e-01],
        [-2.0882e-01],
        [ 6.9587e-01],
        [-3.4458e-02],
        [-1.6135e-01],
        [-6.3643e-01],
        [ 8.5317e-01],
        [ 2.8775e-02],
        [ 4.5704e-01],
        [ 1.8572e-01],
        [ 7.6487e-01],
        [-5.0049e-01],
        [ 2.7942e-01],
        [-2.1069e-01],
        [ 7.0403e-02],
        [ 5.6170e-01],
        [ 3.1987e-01],
        [-5.4235e-01],
        [ 1.9422e-01],
        [ 4.4029e-02],
        [ 2.0856e-01],
        [ 3.9610e-01],
        [ 6.1075e-01],
        [ 2.4315e-01],
        [-3.9200e-01],
        [ 5.8283e-01],
        [-2.0952e-01],
        [-7.9223e-01],
        [-3.6262e-01],
        [ 1.4882e-01],
        [ 6.2548e-01],
        [-1.0983e+00],
        [-6.5478e-01],
        [ 1.6459e-01],
        [ 1.5104e-01],
        [-6.0966e-01],
        [ 1.4526e-01],
        [ 7.7825e-01],
        [-4.6461e-02],
        [-8.1484e-01],
        [-7.3881e-02],
        [ 6.5873e-01],
        [-1.0007e-01],
        [-2.2849e-02],
        [ 1.0358e-01],
        [ 4.2180e-01],
        [ 6.0506e-01],
        [-8.0223e-02],
        [ 1.9736e-01],
        [-7.2575e-01],
        [-1.3561e-01],
        [ 5.4764e-01],
        [ 6.5537e-01],
        [ 7.6215e-01],
        [-3.5352e-01],
        [ 3.9368e-01],
        [ 2.4783e-01],
        [-7.4981e-01],
        [-2.8124e-02],
        [ 5.1546e-01],
        [-6.5931e-02],
        [ 1.3114e-01],
        [-4.4038e-01],
        [ 4.3551e-01],
        [-7.3755e-01],
        [-4.2189e-02],
        [-6.5867e-01],
        [-4.5684e-01],
        [-7.7673e-01],
        [-6.9020e-01],
        [ 6.0984e-01],
        [-3.2703e-01],
        [ 1.3398e-01],
        [-8.6412e-01],
        [-1.5937e-01],
        [ 4.2376e-01],
        [ 9.0637e-01],
        [ 2.1686e-02],
        [-2.6651e-02],
        [ 1.1210e+00],
        [ 6.1240e-01],
        [ 2.1989e-01],
        [-8.8302e-01],
        [-6.3188e-02],
        [ 4.2842e-01],
        [ 4.3557e-01],
        [ 2.2162e-02],
        [-7.1324e-01],
        [ 8.2397e-02],
        [ 6.8394e-01],
        [-5.5508e-01],
        [-3.2440e-01],
        [-7.3875e-01],
        [-1.1170e+00],
        [ 5.0619e-02],
        [ 6.1222e-01],
        [ 2.5156e-01],
        [ 1.6550e-01],
        [ 1.2717e-01],
        [ 6.9768e-01],
        [ 5.6585e-01],
        [-5.7551e-01],
        [-1.7511e-01],
        [ 1.7908e-01],
        [ 6.1018e-01],
        [-5.0394e-01],
        [ 2.1605e-02],
        [-5.7043e-01],
        [-2.7713e-01],
        [-1.6609e-01],
        [-4.5838e-01],
        [ 1.8357e-01],
        [-7.4479e-01],
        [ 7.8589e-01],
        [ 4.0281e-01],
        [-5.1151e-01],
        [ 5.7917e-02],
        [ 7.9440e-01],
        [-5.6092e-02],
        [-1.7467e-02],
        [ 6.8670e-01],
        [-6.1758e-01],
        [-6.9255e-01],
        [-1.3100e-01],
        [ 3.7192e-01],
        [ 3.0870e-01],
        [-5.3921e-01],
        [-9.0928e-01],
        [ 4.4943e-01],
        [-7.4854e-01],
        [ 4.9454e-01],
        [ 4.7564e-01],
        [-3.8857e-01],
        [-6.5259e-01],
        [-4.1935e-01],
        [-1.2818e-01],
        [-8.6497e-01],
        [ 1.9399e-01],
        [-3.3432e-01],
        [ 5.9080e-01],
        [ 7.5653e-02],
        [-5.8810e-01],
        [-2.3762e-01],
        [ 1.4196e-01],
        [-3.4142e-01],
        [ 5.2874e-02],
        [-3.2124e-01],
        [ 1.8937e-01],
        [ 3.5779e-02],
        [ 3.9650e-01],
        [ 5.6111e-01],
        [ 6.2619e-02],
        [-7.5411e-01],
        [-5.4015e-02],
        [-2.4153e-01],
        [ 3.5378e-01],
        [-3.8922e-01],
        [ 2.0882e-01],
        [-4.0642e-01],
        [ 6.1523e-02],
        [ 7.1165e-01],
        [-1.3927e-01],
        [ 3.0908e-01],
        [-7.9603e-01],
        [ 2.5989e-01],
        [ 4.4189e-01],
        [ 1.5655e-01],
        [ 5.4438e-01],
        [-1.6491e-01],
        [ 9.0657e-01],
        [ 1.4673e-01],
        [ 7.0978e-01],
        [ 4.3711e-01],
        [ 1.7550e-01],
        [-1.0454e+00],
        [ 6.2865e-01],
        [ 7.5349e-01],
        [ 5.6880e-01],
        [-4.2364e-01],
        [-8.7574e-01],
        [ 3.3064e-01],
        [ 3.2303e-02],
        [-1.3132e-01],
        [ 9.0371e-01],
        [ 1.1548e+00],
        [-2.5446e-01],
        [ 6.1117e-01],
        [ 2.6155e-01],
        [-7.6096e-02],
        [-1.7248e-01],
        [-6.7018e-01],
        [-3.4550e-02],
        [-2.8401e-01],
        [ 5.0192e-01],
        [-6.4820e-01],
        [ 7.1222e-01],
        [ 5.4709e-02],
        [ 3.1072e-01],
        [ 7.0548e-01],
        [ 6.5570e-01],
        [ 3.7591e-01],
        [-3.2898e-01]], device='cuda:0', grad_fn=<SubBackward0>)
max of grad d_p:  tensor(0.0106, device='cuda:0')
min of grad d_p:  tensor(-0.0819, device='cuda:0')
J_L:  tensor([[-4.8385e-05],
        [ 1.2540e-03],
        [ 3.3021e-05],
        ...,
        [ 2.5367e-03],
        [ 1.9587e-03],
        [-8.1886e-02]], device='cuda:0') 
Jta:  tensor([-8.0972e-06,  4.8762e-03, -1.2125e-05,  ...,  6.6531e-03,
         6.5654e-03, -8.1886e-02], device='cuda:0')
max|min: (J_L, Jta/N)  (0.01059434562921524, 0.05017261207103729, ratio: 4.7357916831970215)|(-0.08188621699810028, -0.08188624680042267)

 check Jacobi res:  torch.Size([532609]) max:  tensor(0.0594, device='cuda:0') mean:  tensor(-0.0002, device='cuda:0') min:  tensor(-0.0396, device='cuda:0') norm:  tensor(1.2835, device='cuda:0') MSE:  tensor(2.4099e-06, device='cuda:0')
BAD Jacobian OCCURS!

 check NTK dimension reduction res:  torch.Size([532609, 1]) max:  tensor(0.0003, device='cuda:0') mean:  tensor(3.6665e-06, device='cuda:0') min:  tensor(0., device='cuda:0') norm:  tensor(0.0061, device='cuda:0') MSE:  tensor(1.1442e-08, device='cuda:0')
Shape check:  torch.Size([532609, 1])
max of d_p_list:  tensor(0.0094, device='cuda:0')
min of d_p_list:  tensor(-0.0047, device='cuda:0')
Epoch:  1  
Training Loss: 0.054250793167739175
Test Loss:  0.06109918653964996
Test Acc:  0.0
Valid Loss:  0.05642188340425491
Valid Acc:  0.0
local minima detector shape:  (0,)
Preserved_eigens number check:  512
100%|██████████| 1/1 [00:02<00:00,  2.47s/it]100%|██████████| 1/1 [00:02<00:00,  2.47s/it]
